{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duvidas\n",
    "\n",
    "1. Qual é o threshold da distribuiçao para podermos considerar um valor como \"long tail\" e podermos agrupar com um valor de \"other\"? \n",
    "    - no caso de geo_cities testou-se com percentil 60 e passámos a ter 100 e poucos valores distintos. \n",
    "    - ja na page_path_level_3 ao percentil 75 ainda sao perto de 200 valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "filepath=r'data/google_merch_store_raw_merge_america.csv'\n",
    "\n",
    "\n",
    "\n",
    "file_tag = \"ga4_merch_store\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSLabs functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"scripts/dslabs_functions.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_functions lodaded\n"
     ]
    }
   ],
   "source": [
    "%run \"scripts/data_functions.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# north america filtering df creation\n",
    "\n",
    "- use if you only have full df version and not northern america filtered file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.read_csv(r'google_merch_store_raw_merge.csv')\n",
    "\n",
    "# data=data[data['geo_sub_continent']=='Northern America']\n",
    "\n",
    "# data.to_csv('google_merch_store_raw_merge_america.csv',index=False)\n",
    "\n",
    "# data.to_csv('google_merch_store_raw_merge_america_compressed.csv',index=False, compression='gzip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 405866 entries, 0 to 405865\n",
      "Data columns (total 48 columns):\n",
      " #   Column                           Non-Null Count   Dtype              \n",
      "---  ------                           --------------   -----              \n",
      " 0   event_date                       405866 non-null  datetime64[ns]     \n",
      " 1   session_id                       405866 non-null  int64              \n",
      " 2   user_pseudo_id                   405866 non-null  float64            \n",
      " 3   event_name                       405866 non-null  object             \n",
      " 4   event_timestamp                  405866 non-null  datetime64[ns, UTC]\n",
      " 5   page_location                    405866 non-null  object             \n",
      " 6   page_title                       404796 non-null  object             \n",
      " 7   device_category                  405866 non-null  object             \n",
      " 8   device_mobile_brand_name         405866 non-null  object             \n",
      " 9   device_mobile_model_name         405866 non-null  object             \n",
      " 10  device_mobile_marketing_name     405866 non-null  object             \n",
      " 11  device_operating_system          405866 non-null  object             \n",
      " 12  device_operating_system_version  405866 non-null  object             \n",
      " 13  device_language                  222198 non-null  object             \n",
      " 14  device_is_limited_ad_tracking    405866 non-null  object             \n",
      " 15  device_web_info_browser          405866 non-null  object             \n",
      " 16  device_web_info_browser_version  405866 non-null  object             \n",
      " 17  geo_continent                    405866 non-null  object             \n",
      " 18  geo_country                      405866 non-null  object             \n",
      " 19  geo_region                       405866 non-null  object             \n",
      " 20  geo_city                         405866 non-null  object             \n",
      " 21  geo_sub_continent                405866 non-null  object             \n",
      " 22  geo_metro                        405866 non-null  object             \n",
      " 23  traffic_source_name              405866 non-null  object             \n",
      " 24  traffic_source_medium            405866 non-null  object             \n",
      " 25  traffic_source_source            405866 non-null  object             \n",
      " 26  page_referrer                    141793 non-null  object             \n",
      " 27  entrances                        17233 non-null   float64            \n",
      " 28  debug_mode                       374693 non-null  float64            \n",
      " 29  ga_session_number                405866 non-null  int64              \n",
      " 30  session_engaged                  374720 non-null  float64            \n",
      " 31  engagement_time_msec             308422 non-null  float64            \n",
      " 32  ecommerce_total_item_quantity    39797 non-null   float64            \n",
      " 33  ecommerce_purchase_revenue       826 non-null     float64            \n",
      " 34  ecommerce_shipping_value         0 non-null       float64            \n",
      " 35  ecommerce_tax_value              826 non-null     float64            \n",
      " 36  ecommerce_unique_items           209453 non-null  float64            \n",
      " 37  ecommerce_transaction_id         220389 non-null  object             \n",
      " 38  item_id                          209453 non-null  object             \n",
      " 39  item_name                        209453 non-null  object             \n",
      " 40  item_brand                       209333 non-null  object             \n",
      " 41  item_variant                     208157 non-null  object             \n",
      " 42  item_category                    208583 non-null  object             \n",
      " 43  price                            201879 non-null  float64            \n",
      " 44  quantity                         7910 non-null    float64            \n",
      " 45  item_revenue                     209453 non-null  float64            \n",
      " 46  item_list_index                  204286 non-null  float64            \n",
      " 47  promotion_name                   208586 non-null  object             \n",
      "dtypes: datetime64[ns, UTC](1), datetime64[ns](1), float64(14), int64(2), object(30)\n",
      "memory usage: 148.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# test_data=True\n",
    "test_data=False\n",
    "\n",
    "\n",
    "# Define a function to sample 10% from each group\n",
    "def sample_per_day(group, fraction=0.1):\n",
    "    return group.sample(frac=fraction)\n",
    "\n",
    "\n",
    "if test_data==True:\n",
    "\n",
    "    data=pd.read_csv(filepath)\n",
    "    \n",
    "\n",
    "    data['event_timestamp'] = pd.to_datetime(data['event_timestamp'], unit='us', utc=True)\n",
    "    data['event_date'] = pd.to_datetime(data['event_date'], infer_datetime_format=True)\n",
    " \n",
    "\n",
    "    # Apply the sampling to each group (grouped by event_date) 1%\n",
    "    data = data.groupby('event_date').apply(lambda x: sample_per_day(x, 0.01)).reset_index(drop=True)\n",
    "\n",
    "   \n",
    "\n",
    "else:\n",
    "    data=pd.read_csv(filepath)\n",
    "    \n",
    "\n",
    "    data['event_timestamp'] = pd.to_datetime(data['event_timestamp'], unit='us', utc=True)\n",
    "    data['event_date'] = pd.to_datetime(data['event_date'], infer_datetime_format=True)\n",
    "\n",
    "    # 10% sample\n",
    "    data = data.groupby('event_date').apply(lambda x: sample_per_day(x, 0.1)).reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(405866, 48)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class target column creation\n",
    "\n",
    "\n",
    "- we want to classify if that hit is from a returning or new user.\n",
    "- Due to web analytics tracking particularities like cookie acceptance we prefer to consider returning users as users that are on their 3rd or larger session number\n",
    "- In this case, new user (ga_session_number <=2) will be 0 and returning user will be more than 2 (ga_session_number > 2)\n",
    "- session number column shall be removed afterwards as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['returning_user'] = data['ga_session_number'].apply(lambda x: 0 if x <= 2 else 1)\n",
    "\n",
    "\n",
    "data=data.drop(['ga_session_number'],axis=1) # now we do not need it anymore. remove it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returning_user\n",
      "0    0.754062\n",
      "1    0.245938\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "target = \"returning_user\"\n",
    "\n",
    "values = data[target].value_counts(normalize=True) \n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unbalance dataset\n",
    "\n",
    "let us umbalance to have 10% as returning users\n",
    "UPDATE: NOT NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate the majority (0) and minority (1) classes\n",
    "# df_majority = data[data['returning_user'] == 0]\n",
    "# df_minority = data[data['returning_user'] == 1]\n",
    "\n",
    "# # Calculate the number of minority rows needed to make a 90/10 split\n",
    "# # Let total_rows be the total number of rows after undersampling\n",
    "# total_rows = len(df_majority) / 0.9  # 90% majority, 10% minority\n",
    "# desired_minority_count = int(total_rows * 0.1)  # 10% of the total should be minority\n",
    "\n",
    "# # Downsample the minority class to the desired number of rows\n",
    "# df_minority_downsampled = df_minority.sample(n=desired_minority_count, random_state=42)\n",
    "\n",
    "# # Combine the majority class with the downsampled minority class\n",
    "# df_imbalanced = pd.concat([df_majority, df_minority_downsampled])\n",
    "\n",
    "# # Shuffle the combined dataset\n",
    "# data = df_imbalanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # Check the new class distribution to verify the 90/10 split\n",
    "# print(data['returning_user'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# column drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### low value or high null count columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['debug_mode','device_is_limited_ad_tracking','device_mobile_marketing_name','geo_metro','traffic_source_name','page_referrer','entrances'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ecommerce specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop([\n",
    " 'ecommerce_total_item_quantity', \n",
    " 'ecommerce_purchase_revenue',            \n",
    " 'ecommerce_shipping_value',              \n",
    " 'ecommerce_tax_value',                   \n",
    " 'ecommerce_unique_items',               \n",
    " 'ecommerce_transaction_id',              \n",
    " 'item_id',                               \n",
    " 'item_name',                             \n",
    " 'item_brand',                            \n",
    " 'item_variant',                          \n",
    " 'item_category',                         \n",
    " 'price',                                 \n",
    " 'quantity',                              \n",
    " 'item_revenue',                          \n",
    " 'item_list_index',                       \n",
    " 'promotion_name',\n",
    " 'geo_continent',\n",
    " 'geo_sub_continent'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_date</th>\n",
       "      <th>session_id</th>\n",
       "      <th>user_pseudo_id</th>\n",
       "      <th>event_name</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>page_location</th>\n",
       "      <th>page_title</th>\n",
       "      <th>device_category</th>\n",
       "      <th>device_mobile_brand_name</th>\n",
       "      <th>device_mobile_model_name</th>\n",
       "      <th>...</th>\n",
       "      <th>device_web_info_browser</th>\n",
       "      <th>device_web_info_browser_version</th>\n",
       "      <th>geo_country</th>\n",
       "      <th>geo_region</th>\n",
       "      <th>geo_city</th>\n",
       "      <th>traffic_source_medium</th>\n",
       "      <th>traffic_source_source</th>\n",
       "      <th>session_engaged</th>\n",
       "      <th>engagement_time_msec</th>\n",
       "      <th>returning_user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>405866</td>\n",
       "      <td>4.058660e+05</td>\n",
       "      <td>4.058660e+05</td>\n",
       "      <td>405866</td>\n",
       "      <td>405866</td>\n",
       "      <td>405866</td>\n",
       "      <td>404796</td>\n",
       "      <td>405866</td>\n",
       "      <td>405866</td>\n",
       "      <td>405866</td>\n",
       "      <td>...</td>\n",
       "      <td>405866</td>\n",
       "      <td>405866</td>\n",
       "      <td>405866</td>\n",
       "      <td>405866</td>\n",
       "      <td>405866</td>\n",
       "      <td>405866</td>\n",
       "      <td>405866</td>\n",
       "      <td>374720.000000</td>\n",
       "      <td>3.084220e+05</td>\n",
       "      <td>405866.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1167</td>\n",
       "      <td>478</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>325</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>view_item</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/Google...</td>\n",
       "      <td>Home</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>...</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>87.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>organic</td>\n",
       "      <td>google</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>152252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37865</td>\n",
       "      <td>65112</td>\n",
       "      <td>236426</td>\n",
       "      <td>171155</td>\n",
       "      <td>112417</td>\n",
       "      <td>...</td>\n",
       "      <td>276716</td>\n",
       "      <td>150445</td>\n",
       "      <td>346605</td>\n",
       "      <td>72941</td>\n",
       "      <td>174703</td>\n",
       "      <td>134311</td>\n",
       "      <td>139476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2020-12-14 04:58:52.854685952</td>\n",
       "      <td>4.997784e+09</td>\n",
       "      <td>2.693552e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-12-14 17:00:29.753952512+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.914101</td>\n",
       "      <td>1.207260e+04</td>\n",
       "      <td>0.245938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-11-01 00:00:00</td>\n",
       "      <td>1.762600e+04</td>\n",
       "      <td>1.000985e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-01 00:00:27.483279+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2020-11-26 00:00:00</td>\n",
       "      <td>2.520310e+09</td>\n",
       "      <td>5.778555e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-26 07:58:01.326646016+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.726000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2020-12-11 00:00:00</td>\n",
       "      <td>4.997597e+09</td>\n",
       "      <td>2.230185e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-12-11 13:44:16.358777344+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.502000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2021-01-04 00:00:00</td>\n",
       "      <td>7.515868e+09</td>\n",
       "      <td>6.108386e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-04 11:53:44.003582464+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.266900e+04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2021-01-31 00:00:00</td>\n",
       "      <td>9.999964e+09</td>\n",
       "      <td>9.985207e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-31 23:57:36.930837+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.789041e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.883035e+09</td>\n",
       "      <td>1.207909e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.280215</td>\n",
       "      <td>2.823020e+04</td>\n",
       "      <td>0.430643</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           event_date    session_id  user_pseudo_id  \\\n",
       "count                          405866  4.058660e+05    4.058660e+05   \n",
       "unique                            NaN           NaN             NaN   \n",
       "top                               NaN           NaN             NaN   \n",
       "freq                              NaN           NaN             NaN   \n",
       "mean    2020-12-14 04:58:52.854685952  4.997784e+09    2.693552e+08   \n",
       "min               2020-11-01 00:00:00  1.762600e+04    1.000985e+06   \n",
       "25%               2020-11-26 00:00:00  2.520310e+09    5.778555e+06   \n",
       "50%               2020-12-11 00:00:00  4.997597e+09    2.230185e+07   \n",
       "75%               2021-01-04 00:00:00  7.515868e+09    6.108386e+07   \n",
       "max               2021-01-31 00:00:00  9.999964e+09    9.985207e+09   \n",
       "std                               NaN  2.883035e+09    1.207909e+09   \n",
       "\n",
       "       event_name                      event_timestamp  \\\n",
       "count      405866                               405866   \n",
       "unique         17                                  NaN   \n",
       "top     view_item                                  NaN   \n",
       "freq       152252                                  NaN   \n",
       "mean          NaN  2020-12-14 17:00:29.753952512+00:00   \n",
       "min           NaN     2020-11-01 00:00:27.483279+00:00   \n",
       "25%           NaN  2020-11-26 07:58:01.326646016+00:00   \n",
       "50%           NaN  2020-12-11 13:44:16.358777344+00:00   \n",
       "75%           NaN  2021-01-04 11:53:44.003582464+00:00   \n",
       "max           NaN     2021-01-31 23:57:36.930837+00:00   \n",
       "std           NaN                                  NaN   \n",
       "\n",
       "                                            page_location page_title  \\\n",
       "count                                              405866     404796   \n",
       "unique                                               1167        478   \n",
       "top     https://shop.googlemerchandisestore.com/Google...       Home   \n",
       "freq                                                37865      65112   \n",
       "mean                                                  NaN        NaN   \n",
       "min                                                   NaN        NaN   \n",
       "25%                                                   NaN        NaN   \n",
       "50%                                                   NaN        NaN   \n",
       "75%                                                   NaN        NaN   \n",
       "max                                                   NaN        NaN   \n",
       "std                                                   NaN        NaN   \n",
       "\n",
       "       device_category device_mobile_brand_name device_mobile_model_name  ...  \\\n",
       "count           405866                   405866                   405866  ...   \n",
       "unique               3                        8                       10  ...   \n",
       "top            desktop                    Apple                   Chrome  ...   \n",
       "freq            236426                   171155                   112417  ...   \n",
       "mean               NaN                      NaN                      NaN  ...   \n",
       "min                NaN                      NaN                      NaN  ...   \n",
       "25%                NaN                      NaN                      NaN  ...   \n",
       "50%                NaN                      NaN                      NaN  ...   \n",
       "75%                NaN                      NaN                      NaN  ...   \n",
       "max                NaN                      NaN                      NaN  ...   \n",
       "std                NaN                      NaN                      NaN  ...   \n",
       "\n",
       "       device_web_info_browser device_web_info_browser_version    geo_country  \\\n",
       "count                   405866                          405866         405866   \n",
       "unique                       6                              14              2   \n",
       "top                     Chrome                            87.0  United States   \n",
       "freq                    276716                          150445         346605   \n",
       "mean                       NaN                             NaN            NaN   \n",
       "min                        NaN                             NaN            NaN   \n",
       "25%                        NaN                             NaN            NaN   \n",
       "50%                        NaN                             NaN            NaN   \n",
       "75%                        NaN                             NaN            NaN   \n",
       "max                        NaN                             NaN            NaN   \n",
       "std                        NaN                             NaN            NaN   \n",
       "\n",
       "        geo_region   geo_city traffic_source_medium traffic_source_source  \\\n",
       "count       405866     405866                405866                405866   \n",
       "unique          62        325                     6                     5   \n",
       "top     California  (not set)               organic                google   \n",
       "freq         72941     174703                134311                139476   \n",
       "mean           NaN        NaN                   NaN                   NaN   \n",
       "min            NaN        NaN                   NaN                   NaN   \n",
       "25%            NaN        NaN                   NaN                   NaN   \n",
       "50%            NaN        NaN                   NaN                   NaN   \n",
       "75%            NaN        NaN                   NaN                   NaN   \n",
       "max            NaN        NaN                   NaN                   NaN   \n",
       "std            NaN        NaN                   NaN                   NaN   \n",
       "\n",
       "       session_engaged engagement_time_msec returning_user  \n",
       "count    374720.000000         3.084220e+05  405866.000000  \n",
       "unique             NaN                  NaN            NaN  \n",
       "top                NaN                  NaN            NaN  \n",
       "freq               NaN                  NaN            NaN  \n",
       "mean          0.914101         1.207260e+04       0.245938  \n",
       "min           0.000000         1.000000e+00       0.000000  \n",
       "25%           1.000000         1.726000e+03       0.000000  \n",
       "50%           1.000000         5.502000e+03       0.000000  \n",
       "75%           1.000000         1.266900e+04       0.000000  \n",
       "max           1.000000         1.789041e+06       1.000000  \n",
       "std           0.280215         2.823020e+04       0.430643  \n",
       "\n",
       "[11 rows x 23 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary5 = data.describe(include=\"all\")\n",
    "\n",
    "summary5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drop specific event_names\n",
    "\n",
    "We want to classify a user by its interactions with the website so we want to exclude some actions that may also be biased by incorrect ga4 tracking namely:\n",
    "- session_start\n",
    "- first_visit\n",
    "- click (low event count)\n",
    "- view_item_list (may not be triggered by user interaction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data==True:\n",
    "    data['event_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values to drop\n",
    "events_to_drop = ['session_start', 'first_visit','click','view_item_list']\n",
    "\n",
    "# drop events from list\n",
    "data = data[~data['event_name'].isin(events_to_drop)]\n",
    "\n",
    "if test_data==True:\n",
    "    data['event_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replace (not set) with null\n",
    "\n",
    "we will handle these later but these are actually null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace('(not set)', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engagement time msec\n",
    "- https://support.google.com/analytics/answer/11109416?hl=en\n",
    "- is it null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['engagement_time_msec'] = data['engagement_time_msec'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geo columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## geo_region and geo_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['California',\n",
       " 'Florida',\n",
       " 'Texas',\n",
       " 'New Jersey',\n",
       " 'Pennsylvania',\n",
       " 'New York',\n",
       " 'Michigan',\n",
       " 'Ohio',\n",
       " 'Ontario',\n",
       " 'Illinois',\n",
       " 'Georgia',\n",
       " 'Massachusetts',\n",
       " 'North Carolina',\n",
       " 'Virginia',\n",
       " 'Quebec',\n",
       " 'Maryland',\n",
       " 'Connecticut',\n",
       " 'Wisconsin',\n",
       " 'Indiana',\n",
       " 'Missouri',\n",
       " 'Colorado',\n",
       " 'Washington',\n",
       " 'British Columbia',\n",
       " 'Utah',\n",
       " 'South Carolina',\n",
       " 'Minnesota',\n",
       " 'Alberta',\n",
       " 'Tennessee',\n",
       " 'Alabama',\n",
       " 'Oregon',\n",
       " 'Louisiana',\n",
       " 'Iowa',\n",
       " 'Kentucky',\n",
       " 'Arkansas',\n",
       " 'Mississippi',\n",
       " 'Arizona',\n",
       " 'Oklahoma',\n",
       " 'Kansas',\n",
       " 'New Hampshire',\n",
       " 'Delaware',\n",
       " 'West Virginia',\n",
       " 'Idaho',\n",
       " 'Saskatchewan',\n",
       " 'Manitoba',\n",
       " 'Nova Scotia',\n",
       " 'North Dakota',\n",
       " 'Montana',\n",
       " 'New Mexico',\n",
       " 'Maine',\n",
       " 'Rhode Island',\n",
       " 'Alaska',\n",
       " 'Nebraska',\n",
       " 'New Brunswick',\n",
       " 'Vermont',\n",
       " 'Prince Edward Island',\n",
       " 'Nevada',\n",
       " 'Hawaii',\n",
       " 'Newfoundland and Labrador',\n",
       " 'Wyoming',\n",
       " 'South Dakota']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_region_cities_df=data.groupby(['geo_region']).agg(\n",
    "    null_count=('geo_city', lambda x: x.isna().sum())\n",
    ").sort_values('null_count',ascending=False).reset_index()\n",
    "\n",
    "# will retrieve the most populated cities of these regions to use as fill method\n",
    "region_cities_with_nulls = null_region_cities_df[null_region_cities_df['null_count'] > 0]['geo_region'].tolist()\n",
    "\n",
    "\n",
    "region_cities_with_nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## geo mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_populated_cities = {\n",
    "    'California': 'Los Angeles',\n",
    "    'Florida': 'Jacksonville',\n",
    "    'Texas': 'Houston',\n",
    "    'New Jersey': 'Newark',\n",
    "    'New York': 'New York City',\n",
    "    'Pennsylvania': 'Philadelphia',\n",
    "    'Michigan': 'Detroit',\n",
    "    'Ohio': 'Columbus',\n",
    "    'Illinois': 'Chicago',\n",
    "    'Georgia': 'Atlanta',\n",
    "    'Massachusetts': 'Boston',\n",
    "    'North Carolina': 'Charlotte',\n",
    "    'Virginia': 'Virginia Beach',\n",
    "    'Maryland': 'Baltimore',\n",
    "    'Connecticut': 'Bridgeport',\n",
    "    'Indiana': 'Indianapolis',\n",
    "    'Wisconsin': 'Milwaukee',\n",
    "    'Missouri': 'Kansas City',\n",
    "    'Washington': 'Seattle',\n",
    "    'Colorado': 'Denver',\n",
    "    'Minnesota': 'Minneapolis',\n",
    "    'South Carolina': 'Charleston',\n",
    "    'Utah': 'Salt Lake City',\n",
    "    'Tennessee': 'Nashville',\n",
    "    'Alabama': 'Birmingham',\n",
    "    'Louisiana': 'New Orleans',\n",
    "    'Oregon': 'Portland',\n",
    "    'Kentucky': 'Louisville',\n",
    "    'Mississippi': 'Jackson',\n",
    "    'Iowa': 'Des Moines',\n",
    "    'Arkansas': 'Little Rock',\n",
    "    'Arizona': 'Phoenix',\n",
    "    'Oklahoma': 'Oklahoma City',\n",
    "    'Kansas': 'Wichita',\n",
    "    'Delaware': 'Wilmington',\n",
    "    'West Virginia': 'Charleston',\n",
    "    'New Hampshire': 'Manchester',\n",
    "    'Idaho': 'Boise',\n",
    "    'Montana': 'Billings',\n",
    "    'Maine': 'Portland',\n",
    "    'New Mexico': 'Albuquerque',\n",
    "    'North Dakota': 'Fargo',\n",
    "    'Rhode Island': 'Providence',\n",
    "    'Nebraska': 'Omaha',\n",
    "    'Alaska': 'Anchorage',\n",
    "    'Vermont': 'Burlington',\n",
    "    'Hawaii': 'Honolulu',\n",
    "    'Nevada': 'Las Vegas',\n",
    "    'Wyoming': 'Cheyenne',\n",
    "    'South Dakota': 'Sioux Falls'\n",
    "}\n",
    "\n",
    "data['geo_city'] = data['geo_city'].fillna(data['geo_region'].map(most_populated_cities))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## geo city long tail conversion\n",
    "\n",
    "- no conversion applied after discussion. encode everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Aggregate events by 'page_location' and 'page_path_level_3'\n",
    "# df_cities_agg = data.groupby(['geo_city']).agg(\n",
    "#     total_events=('event_timestamp', 'count'),\n",
    "# ).sort_values('total_events', ascending=False).reset_index()\n",
    "\n",
    "# # Step 2: Calculate the desired percentile of total events to consider long tail\n",
    "# event_count_percentile = df_cities_agg['total_events'].quantile(0.65)\n",
    "\n",
    "# # Step 3: Create a mask for long tail pages\n",
    "# long_tail_mask = df_cities_agg['total_events'] < event_count_percentile\n",
    "\n",
    "# # Step 4: Create a dictionary to map original page_path_level_3 to \"other\" (long tail)\n",
    "# long_tail_mapping = dict(zip(df_cities_agg['geo_city'], \n",
    "#                              df_cities_agg['geo_city'].where(~long_tail_mask, \"Other\")))\n",
    "\n",
    "\n",
    "# # Step 5: Replace the values directly in the original DataFrame\n",
    "# data['geo_city'] = data.apply(\n",
    "#     lambda row: long_tail_mapping.get(row['geo_city'], row['geo_city']),\n",
    "#     axis=1\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# event timestamp treatment\n",
    "\n",
    "- need to convert to local time from different us timezones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timezone from regions and create local_event_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['British Columbia', 'New Hampshire', 'California', 'Minnesota', 'District of Columbia', 'Saskatchewan', 'Michigan', 'Arizona', 'Tennessee', 'Prince Edward Island', 'Alabama', 'Mississippi', 'Illinois', 'West Virginia', 'Maine', 'Texas', 'Delaware', 'South Dakota', 'New Jersey', 'Indiana', 'South Carolina', 'Arkansas', 'Wyoming', 'Utah', 'Colorado', 'New Mexico', 'New Brunswick', 'Florida', 'Missouri', 'Quebec', 'Kentucky', 'Iowa', 'Virginia', 'Pennsylvania', 'Massachusetts', 'Oklahoma', nan, 'Maryland', 'Alberta', 'Washington', 'New York', 'North Dakota', 'Idaho', 'Manitoba', 'Alaska', 'Nova Scotia', 'Nevada', 'Newfoundland and Labrador', 'Montana', 'Vermont', 'Nebraska', 'Oregon', 'North Carolina', 'Ohio', 'Hawaii', 'Ontario', 'Kansas', 'Rhode Island', 'Connecticut', 'Georgia', 'Wisconsin', 'Louisiana']\n"
     ]
    }
   ],
   "source": [
    "geo_region_list = data['geo_region'].tolist()\n",
    "distinct_geo_regions = list(set(geo_region_list))\n",
    "\n",
    "print(distinct_geo_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### timezone mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "\n",
    "\n",
    "city_timezone_mapping = {\n",
    "    'Kingston': 'America/Toronto',  # Canada (Eastern Time)\n",
    "    'Calgary': 'America/Edmonton',  # Canada (Mountain Time)\n",
    "    'Bethesda': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Nashville': 'America/Chicago',  # USA (Central Time)\n",
    "    'Saint Paul': 'America/Chicago',  # USA (Central Time)\n",
    "    'Berkeley': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Little Rock': 'America/Chicago',  # USA (Central Time)\n",
    "    'Sunnyvale': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Salinas': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Ann Arbor': 'America/Detroit',  # USA (Eastern Time)\n",
    "    'Auburn': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Germantown': 'America/Chicago',  # USA (Central Time)\n",
    "    'Fairfield': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Charlotte': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Guelph': 'America/Toronto',  # Canada (Eastern Time)\n",
    "    'Vancouver': 'America/Vancouver',  # Canada (Pacific Time)\n",
    "    'Stamford': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Pleasanton': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Cambridge': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Paramus': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Jacksonville': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Livermore': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Eau Claire': 'America/Chicago',  # USA (Central Time)\n",
    "    'Sacramento': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Los Altos': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Dartmouth': 'America/Halifax',  # Canada (Atlantic Time)\n",
    "    'Everett': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Modesto': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Saint John': 'America/Halifax',  # Canada (Atlantic Time)\n",
    "    'Lethbridge': 'America/Edmonton',  # Canada (Mountain Time)\n",
    "    'Kansas City': 'America/Chicago',  # USA (Central Time)\n",
    "    'Madison': 'America/Chicago',  # USA (Central Time)\n",
    "    'Evanston': 'America/Chicago',  # USA (Central Time)\n",
    "    'Fontana': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Hayward': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Franklin': 'America/Chicago',  # USA (Central Time)\n",
    "    'Somerville': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Chantilly': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Halifax': 'America/Halifax',  # Canada (Atlantic Time)\n",
    "    'Laredo': 'America/Chicago',  # USA (Central Time)\n",
    "    'Windsor': 'America/Toronto',  # Canada (Eastern Time)\n",
    "    'Richardson': 'America/Chicago',  # USA (Central Time)\n",
    "    'Oshawa': 'America/Toronto',  # Canada (Eastern Time)\n",
    "    'Denver': 'America/Denver',  # USA (Mountain Time)\n",
    "    'Cherry Hill': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Fredericton': 'America/Moncton',  # Canada (Atlantic Time)\n",
    "    'Houston': 'America/Chicago',  # USA (Central Time)\n",
    "    'Regina': 'America/Regina',  # Canada (Central Time)\n",
    "    'Buffalo': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Coffeyville': 'America/Chicago',  # USA (Central Time)\n",
    "    'Sugar Land': 'America/Chicago',  # USA (Central Time)\n",
    "    'Johns Creek': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Winston-Salem': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Fargo': 'America/Chicago',  # USA (Central Time)\n",
    "    'Granby': 'America/Toronto',  # Canada (Eastern Time)\n",
    "    'New Orleans': 'America/Chicago',  # USA (Central Time)\n",
    "    'Hartford': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Bridgeport': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Reno': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Raleigh': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Miami': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Chicago': 'America/Chicago',  # USA (Central Time)\n",
    "    'San Francisco': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Los Angeles': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Philadelphia': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Anchorage': 'America/Anchorage',  # USA (Alaska Time)\n",
    "    'Minneapolis': 'America/Chicago',  # USA (Central Time)\n",
    "    'New York': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Boston': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Dallas': 'America/Chicago',  # USA (Central Time)\n",
    "    'Boulder': 'America/Denver',  # USA (Mountain Time)\n",
    "    'Seattle': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Montgomery': 'America/Chicago',  # USA (Central Time)\n",
    "    'Phoenix': 'America/Phoenix',  # USA (Mountain Standard Time, no DST)\n",
    "    'Las Vegas': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "}\n",
    "\n",
    "\n",
    "region_timezone_dict = {\n",
    "    'Texas': 'America/Chicago',           # Central Time\n",
    "    'Nova Scotia': 'America/Halifax',     # Atlantic Time\n",
    "    'Wisconsin': 'America/Chicago',       # Central Time\n",
    "    'Massachusetts': 'America/New_York',  # Eastern Time\n",
    "    'Nebraska': 'America/Chicago',        # Central Time\n",
    "    'Wyoming': 'America/Denver',          # Mountain Time\n",
    "    'Kentucky': 'America/New_York',       # Eastern Time\n",
    "    'Maryland': 'America/New_York',       # Eastern Time\n",
    "    'Arkansas': 'America/Chicago',        # Central Time\n",
    "    'Saskatchewan': 'America/Regina',     # Central Time (no DST)\n",
    "    'South Dakota': 'America/Chicago',    # Central Time\n",
    "    'Ohio': 'America/New_York',           # Eastern Time\n",
    "    'Nevada': 'America/Los_Angeles',      # Pacific Time\n",
    "    'Kansas': 'America/Chicago',          # Central Time\n",
    "    'Virginia': 'America/New_York',       # Eastern Time\n",
    "    'Minnesota': 'America/Chicago',       # Central Time\n",
    "    'Washington': 'America/Los_Angeles',  # Pacific Time\n",
    "    'Maine': 'America/New_York',          # Eastern Time\n",
    "    'Vermont': 'America/New_York',        # Eastern Time\n",
    "    'Alaska': 'America/Anchorage',        # Alaska Time\n",
    "    'Manitoba': 'America/Winnipeg',       # Central Time\n",
    "    'Alabama': 'America/Chicago',         # Central Time\n",
    "    'Iowa': 'America/Chicago',            # Central Time\n",
    "    'Rhode Island': 'America/New_York',   # Eastern Time\n",
    "    'Missouri': 'America/Chicago',        # Central Time\n",
    "    'Hawaii': 'Pacific/Honolulu',         # Hawaii-Aleutian Time (no DST)\n",
    "    'Florida': 'America/New_York',        # Eastern Time (some parts Central)\n",
    "    'Michigan': 'America/Detroit',        # Eastern Time\n",
    "    'Tennessee': 'America/Chicago',       # Central Time (some parts Eastern)\n",
    "    'Pennsylvania': 'America/New_York',   # Eastern Time\n",
    "    'Delaware': 'America/New_York',       # Eastern Time\n",
    "    'Prince Edward Island': 'America/Halifax',  # Atlantic Time\n",
    "    'District of Columbia': 'America/New_York', # Eastern Time\n",
    "    'North Dakota': 'America/Chicago',    # Central Time\n",
    "    'Oklahoma': 'America/Chicago',        # Central Time\n",
    "    'New Brunswick': 'America/Halifax',   # Atlantic Time\n",
    "    'Ontario': 'America/Toronto',         # Eastern Time\n",
    "    'Quebec': 'America/Toronto',          # Eastern Time\n",
    "    'Idaho': 'America/Boise',             # Mountain Time\n",
    "    'Indiana': 'America/Indiana/Indianapolis',  # Eastern Time\n",
    "    'New York': 'America/New_York',       # Eastern Time\n",
    "    'Mississippi': 'America/Chicago',     # Central Time\n",
    "    'Georgia': 'America/New_York',        # Eastern Time\n",
    "    'Illinois': 'America/Chicago',        # Central Time\n",
    "    'Louisiana': 'America/Chicago',       # Central Time\n",
    "    'New Jersey': 'America/New_York',     # Eastern Time\n",
    "    'West Virginia': 'America/New_York',  # Eastern Time\n",
    "    'South Carolina': 'America/New_York', # Eastern Time\n",
    "    'Alberta': 'America/Edmonton',        # Mountain Time\n",
    "    'Arizona': 'America/Phoenix',         # Mountain Time (no DST)\n",
    "    'North Carolina': 'America/New_York', # Eastern Time\n",
    "    'Newfoundland and Labrador': 'America/St_Johns',  # Newfoundland Time\n",
    "    'California': 'America/Los_Angeles',  # Pacific Time\n",
    "    'Utah': 'America/Denver',             # Mountain Time\n",
    "    'New Hampshire': 'America/New_York',  # Eastern Time\n",
    "    'Connecticut': 'America/New_York',    # Eastern Time\n",
    "    'Montana': 'America/Denver',          # Mountain Time\n",
    "    'British Columbia': 'America/Vancouver',  # Pacific Time\n",
    "    'Colorado': 'America/Denver',         # Mountain Time\n",
    "    'New Mexico': 'America/Denver',       # Mountain Time\n",
    "    'Oregon': 'America/Los_Angeles'       # Pacific Time\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timezone conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to convert the UTC datetime to local time and extract date, hour, and minute\n",
    "def extract_local_date_hour_minute(row, region_timezone_dict):\n",
    "    # Check if the row contains a valid timestamp\n",
    "    if pd.isnull(row['event_timestamp']):\n",
    "        return pd.NaT, pd.NaT, pd.NaT  # Return NaT for all if the timestamp is null\n",
    "\n",
    "    # Get the region and corresponding timezone\n",
    "    region = row['geo_region']\n",
    "    tz = region_timezone_dict.get(region, None)  # Fetch timezone, fallback to None\n",
    "    \n",
    "    # If a valid timezone is found, apply timezone conversion\n",
    "    if tz:\n",
    "        try:\n",
    "            local_time = row['event_timestamp'].tz_convert(tz)  # Convert to local time\n",
    "            \n",
    "            local_date = local_time.date()  # Extract local date\n",
    "            local_hour = local_time.hour  # Extract local hour\n",
    "            local_minute = local_time.minute  # Extract local minute\n",
    "            \n",
    "            return local_date, local_hour, local_minute\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting to timezone {tz}: {e}\")\n",
    "            return pd.NaT, pd.NaT, pd.NaT\n",
    "    else:\n",
    "        # Fallback to UTC timestamp\n",
    "        return row['event_timestamp'].date(), row['event_timestamp'].hour, row['event_timestamp'].minute\n",
    "\n",
    "# Apply the conversion function to the DataFrame\n",
    "data[['local_date', 'local_hour', 'local_minute']] = data.apply(\n",
    "    lambda row: extract_local_date_hour_minute(row, region_timezone_dict), axis=1, result_type='expand'\n",
    ")\n",
    "\n",
    "# Convert local_date to datetime format\n",
    "data['local_date'] = pd.to_datetime(data['local_date'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional date columns creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hour_minute_fraction'] = round(data['local_hour'] + data['local_minute'] / 60,2)  # Hour + fraction of minute\n",
    "\n",
    "# Categorize the time of day\n",
    "def categorize_time_of_day(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 22:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "data['time_of_day'] = data['local_hour'].apply(categorize_time_of_day)\n",
    "\n",
    "\n",
    "\n",
    "# create year, quarter, month, day number of week, weekend/weekday based on event_date column\n",
    "\n",
    "# Create new columns\n",
    "data['year'] = data['local_date'].dt.year\n",
    "data['quarter'] = data['local_date'].dt.quarter\n",
    "data['month'] = data['local_date'].dt.month\n",
    "data['day'] = data['local_date'].dt.day\n",
    "\n",
    "# data['day_of_week'] = data['local_date'].dt.day_name()  \n",
    "\n",
    "data['day_of_year'] = data['local_date'].dt.dayofyear  # Day of the year\n",
    "data['week_number'] = data['local_date'].dt.isocalendar().week  # ISO week number\n",
    "\n",
    "# Assuming 'local_date' is in datetime format, otherwise you can parse it using pd.to_datetime\n",
    "def week_of_month(dt):\n",
    "    first_day = dt.replace(day=1)\n",
    "    # Calculate the week of the month by comparing the current date to the first day of the month\n",
    "    return (dt.day + first_day.weekday()) // 7 + 1\n",
    "\n",
    "# Apply this function to your 'local_date' column\n",
    "data['week_of_month'] = data['local_date'].apply(week_of_month)\n",
    "\n",
    "\n",
    "\n",
    "data['day_of_week_nr'] = data['local_date'].dt.weekday  # Monday=0, Sunday=6\n",
    "data['is_weekend'] = data['day_of_week_nr'].apply(lambda x: 1 if x >= 5 else 0)  # 1 for weekend, 0 for weekday\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# device columns\n",
    "\n",
    "- for many cases we assumed devices, brands and os versions of 2021 as top devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device mobile brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device_mobile_brand_name\n",
       "Apple         157963\n",
       "PC            120144\n",
       "Google         29512\n",
       "Samsung        28847\n",
       "Smartphone     23443\n",
       "Xiaomi          8949\n",
       "Huawei          4649\n",
       "<Other>          711\n",
       "Mozilla          362\n",
       "Microsoft         40\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Fill 'device_mobile_brand_name' with 'PC' where the conditions are met\n",
    "data.loc[(data['device_operating_system'] == 'Windows') & (data['device_category'] == 'desktop'), 'device_mobile_brand_name'] = 'PC'\n",
    "data.loc[(data['device_operating_system'] == 'Web') & (data['device_category'] == 'desktop') & ((data['device_mobile_model_name'].isin(['Chrome','Edge','Firefox']))), 'device_mobile_brand_name'] = 'PC'\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Microsoft') & (data['device_category'] == 'desktop'), 'device_mobile_brand_name'] = 'PC'\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == '<Other>') & (data['device_mobile_brand_name'] == 'Edge'), 'device_mobile_brand_name'] = 'Microsoft'\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == '<Other>') & (data['device_mobile_brand_name'] == '<Other>') & (data['device_category'] == 'desktop'), 'device_mobile_brand_name'] = 'PC'\n",
    "data.loc[(data['device_mobile_brand_name'] == '<Other>') & (data['device_mobile_brand_name'] == '<Other>') & (data['device_category'] == 'mobile'), 'device_mobile_brand_name'] = 'Smartphone'\n",
    "\n",
    "\n",
    "data['device_mobile_brand_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_mobile_model_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Samsung'), 'device_mobile_model_name'] = 'Galaxy S21'\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Xiaomi'), 'device_mobile_model_name'] = 'Mi 11'\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Huawei') & (data['device_category'] == 'mobile'), 'device_mobile_model_name'] = 'P50'\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Apple') & (data['device_category'] == 'desktop'), 'device_mobile_model_name'] = 'Macintosh'\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Apple') & (data['device_category'] == 'mobile'), 'device_mobile_model_name'] = 'iPhone'\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Apple') & (data['device_category'] == 'tablet'), 'device_mobile_model_name'] = 'iPad'\n",
    "\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'PC')& (data['device_mobile_model_name'] == 'Chrome'), 'device_mobile_model_name'] = 'PC'\n",
    "\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Microsoft') & (data['device_mobile_brand_name'] == '<Other>'), 'device_mobile_model_name'] = 'Edge'\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Mozilla') & (data['device_mobile_brand_name'] == '<Other>'), 'device_mobile_model_name'] = 'Firefox'\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Google') & (data['device_mobile_brand_name'] == '<Other>'), 'device_mobile_model_name'] = 'Chrome'\n",
    "\n",
    "\n",
    "\n",
    "data['device_mobile_model_name'] = (\n",
    "    data['device_mobile_brand_name'] + \" - \" +\n",
    "    data['device_mobile_model_name'].astype(str).where(pd.notna(data['device_mobile_model_name']), '')\n",
    ")\n",
    "\n",
    "\n",
    "if test_data==True:\n",
    "    data['device_mobile_model_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_operating_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chrome OS - ensure 'desktop' is correctly spelled\n",
    "data.loc[(data['device_mobile_model_name'] == 'Google - ChromeBook') & (data['device_category'] == 'desktop'), 'device_operating_system'] = 'ChromeOS'\n",
    "\n",
    "# iOS - for iPhone and iPad\n",
    "data.loc[data['device_mobile_model_name'].isin(['Apple - iPhone', 'Apple - iPad']) | (data['device_mobile_brand_name'] == 'Apple') | ((data['device_mobile_model_name'] == 'Apple') & (data['device_category'].isin(['mobile','tablet']))), 'device_operating_system'] = 'iOS'\n",
    "\n",
    "# Android - for specified brands\n",
    "android_brands = ['Xiaomi', 'Huawei', 'Samsung']\n",
    "data.loc[data['device_mobile_brand_name'].isin(android_brands), 'device_operating_system'] = 'Android'\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Google') & (data['device_category'].isin(['mobile','tablet'])), 'device_operating_system'] = 'Android'\n",
    "\n",
    "# macOS\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Apple') & (data['device_category'] == 'desktop'), 'device_operating_system'] = 'MacOS'\n",
    "\n",
    "# Windows\n",
    "data.loc[(data['device_operating_system'] == 'Web') & (data['device_category'] == 'desktop') & ((data['device_mobile_brand_name'] == 'PC')), 'device_operating_system'] = 'Windows'\n",
    "data.loc[(data['device_operating_system'] == 'Web') & (data['device_category'] == 'desktop') & ((data['device_mobile_brand_name'] == 'Mozilla')), 'device_operating_system'] = 'Windows'\n",
    "data.loc[(data['device_category'] == 'desktop') & ((data['device_mobile_brand_name'] == 'Microsoft')), 'device_operating_system'] = 'Windows'\n",
    "\n",
    "if test_data==True:\n",
    "    data.groupby(['device_category','device_operating_system', 'device_mobile_brand_name']).agg(\n",
    "        unique_event_count=('event_timestamp', 'nunique')\n",
    "    ).sort_values('unique_event_count',ascending=False).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_operating_system_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all string characters and keep float values\n",
    "data['device_operating_system_version'] = data['device_operating_system_version'].str.extract(r'(\\d+\\.\\d+|\\d+)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chrome os consider same browser version\n",
    "# https://chromereleases.googleblog.com/2021/\n",
    "data.loc[(data['device_operating_system'] == 'ChromeOS') & (data['device_operating_system_version'].isnull()), 'device_operating_system_version'] = data['device_web_info_browser_version']\n",
    "data.loc[(data['device_operating_system'] == 'ChromeOS') & (data['device_operating_system_version']=='<Other>'), 'device_operating_system_version'] = 87.0\n",
    "\n",
    "\n",
    "data['device_operating_system_version'] = (\n",
    "    data['device_operating_system_version']\n",
    "    .where(pd.notna(data['device_operating_system_version']), None)  # Retain None/NaN for missing values\n",
    ")\n",
    "\n",
    "# Only perform concatenation where 'device_operating_system_version' is not null\n",
    "data['device_operating_system_version'] = (\n",
    "    data.apply(lambda row: f\"{row['device_operating_system']} - {row['device_operating_system_version']}\"\n",
    "                if pd.notna(row['device_operating_system_version']) else None, axis=1)\n",
    ")\n",
    "\n",
    "if test_data==True:\n",
    "    data['device_operating_system_version'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data==True:\n",
    "    data['device_operating_system_version'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if test_data==True:\n",
    "    data.groupby(['geo_country','device_language']).agg(\n",
    "        unique_event_count=('event_timestamp', 'nunique')\n",
    "    ).sort_values('unique_event_count',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data==True:\n",
    "    data.groupby(['geo_country']).agg(\n",
    "        null_device_language_count=('device_language', lambda x: x.isna().sum())\n",
    "    ).sort_values('null_device_language_count',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data==True:\n",
    "    data['device_language'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_web_info_browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data['device_web_info_browser'] == 'Android Webview'), 'device_web_info_browser'] = \"Chrome\"\n",
    "\n",
    "if test_data==True:\n",
    "    data['device_web_info_browser'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_web_info_browser_version\n",
    "\n",
    "let us concatenate so that the values make sense and not mixed between browser versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device_web_info_browser_version\n",
       "Chrome - 87.0        182320\n",
       "Chrome - 86.0         77887\n",
       "Safari - 14.0         69242\n",
       "<Other> - <Other>     10839\n",
       "Chrome - 14.0          9317\n",
       "Safari - 13.1          8375\n",
       "Edge - 87.0            6388\n",
       "Firefox - 84.0         3759\n",
       "Edge - 86.0            2022\n",
       "Firefox - 83.0         1948\n",
       "Safari - 13.0          1552\n",
       "Firefox - 82.0          941\n",
       "Safari - 14.1            30\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "data.loc[(data['device_web_info_browser'].isin(['Chrome','Edge'])) & (data['device_web_info_browser_version']==\"<Other>\"), 'device_web_info_browser_version'] = 87.0\n",
    "data.loc[(data['device_web_info_browser'] == 'Firefox') & (data['device_web_info_browser_version']==\"<Other>\"), 'device_web_info_browser_version'] = 84.0\n",
    "\n",
    "data.loc[(data['device_web_info_browser'] == 'Safari') & (data['device_web_info_browser_version']==\"<Other>\"), 'device_web_info_browser_version'] = 14.0\n",
    "\n",
    "\n",
    "data['device_web_info_browser_version'] = data['device_web_info_browser_version'].astype(str).str.strip()\n",
    "data['device_web_info_browser_version'] = data['device_web_info_browser_version'].replace({86: 86.0, 87: 87.0})\n",
    "\n",
    "\n",
    "data['device_web_info_browser_version'] = (\n",
    "    data['device_web_info_browser'] + ' - ' +\n",
    "    data['device_web_info_browser_version'].astype(str).where(pd.notna(data['device_web_info_browser_version']), '')\n",
    ")\n",
    "\n",
    "data['device_web_info_browser_version'] = data['device_web_info_browser_version'].replace({\"Chrome - 86\": \"Chrome - 86.0\", \"Chrome - 87\": \"Chrome - 87.0\"})\n",
    "data['device_web_info_browser_version'] = data['device_web_info_browser_version'].replace({\"Safari - 604\": \"Chrome - 14.0\"})\n",
    "\n",
    "# if test_data==True:\n",
    "data['device_web_info_browser_version'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## session counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data==True:\n",
    "    data.groupby(['traffic_source_medium','traffic_source_source']).agg(\n",
    "    unique_session_count=('session_id', 'nunique')\n",
    "    ).sort_values('unique_session_count',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove parenthesis ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['traffic_source_source'] = data['traffic_source_source'].str.replace(r'\\(|\\)', '', regex=True).str.strip()\n",
    "data['traffic_source_medium'] = data['traffic_source_medium'].str.replace(r'\\(|\\)', '', regex=True).str.strip()\n",
    "\n",
    "if test_data==True:\n",
    "    data.groupby(['traffic_source_medium','traffic_source_source']).agg(\n",
    "        unique_session_count=('session_id', 'nunique')\n",
    "    ).sort_values('unique_session_count',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace values\n",
    "\n",
    "- bad referral naming from shop.googlemerchandisestore.com means badly tracked and we should consider direct traffic instead\n",
    "- medium =none is referring to direct traffic and we will use the same name to not confuse with null values\n",
    "- data deleted is most likely paid campaign by google to avoid confidential data exposure so we will replace with cpc / google as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source is merchandisestore.com then direct (bad parameter)\n",
    "data.loc[(data['traffic_source_source'] == 'shop.googlemerchandisestore.com') & (data['traffic_source_medium'] == 'referral'), ['traffic_source_medium', 'traffic_source_source']] = ['direct','direct']\n",
    "\n",
    "\n",
    "# referral traffic\n",
    "data.loc[(data['traffic_source_medium'] == 'referral'), ['traffic_source_medium', 'traffic_source_source']] = ['referral','referral_link']\n",
    "\n",
    "# google organic\n",
    "data.loc[(data['traffic_source_source'].isnull()) & (data['traffic_source_medium']== 'organic'),  ['traffic_source_medium', 'traffic_source_source']] = ['organic','google']\n",
    "\n",
    "\n",
    "# when we have direct traffic it is direct traffic\n",
    "data.loc[(data['traffic_source_source'] == 'direct'), 'traffic_source_medium'] = 'direct'\n",
    "\n",
    "# data deleted is paid campaign cpc by google\n",
    "data.loc[(data['traffic_source_source'] == 'data deleted') | (data['traffic_source_medium'] == 'data deleted'), ['traffic_source_medium', 'traffic_source_source']] = ['cpc', 'google']\n",
    "\n",
    "\n",
    "# full null values are direct\n",
    "data.loc[(data['traffic_source_source'].isnull()) & (data['traffic_source_medium'].isnull()),  ['traffic_source_medium', 'traffic_source_source']] = ['direct','direct']\n",
    "# data.loc[(data['traffic_source_source'].isnull()) & (data['traffic_source_medium'].isnull()), 'traffic_source_medium'] = 'direct'\n",
    "\n",
    "if test_data == True:\n",
    "    data.groupby(['traffic_source_source','traffic_source_medium']).agg(\n",
    "        unique_session_count=('session_id', 'nunique')\n",
    "    ).sort_values('unique_session_count',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pagelocation string work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove page unavailable\n",
    "data = data.loc[data['page_title'] != 'Page Unavailable']\n",
    "\n",
    "\n",
    "# lowercase everything to guarantee string match\n",
    "data['page_location'] = data['page_location'].str.lower()\n",
    "data['page_title'] = data['page_title'].str.lower()\n",
    "\n",
    "\n",
    "# domain readability\n",
    "data['page_location'] = data['page_location'].str.replace(r'shop.googlemerchandisestore.com/store.html', 'shop.googlemerchandisestore.com/').str.strip()\n",
    "data['page_location'] = data['page_location'].str.replace(r'+', ' ').str.strip()\n",
    "data['page_location'] = data['page_location'].str.replace(r'https://', '').str.strip()\n",
    "data['page_location'] = data['page_location'].str.replace(r'http://', '').str.strip()\n",
    "data['page_location'] = data['page_location'].str.replace(r'www.', '').str.strip()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove low percentile page record count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low percentile of event count: 9.0\n"
     ]
    }
   ],
   "source": [
    "df_pages_agg=data.groupby(['page_location']).agg(\n",
    "    total_events=('event_timestamp', 'count'),\n",
    ").sort_values('total_events',ascending=False).reset_index()\n",
    "\n",
    "\n",
    "# Calculate the percentile of event counts\n",
    "event_count_percentile = df_pages_agg['total_events'].quantile(0.30)\n",
    "\n",
    "print(f\"low percentile of event count: {event_count_percentile}\")\n",
    "\n",
    "# # Filter out page paths below the 10th percentile\n",
    "df_pages_agg_filtered = df_pages_agg[df_pages_agg['total_events'] >= event_count_percentile]\n",
    "\n",
    "\n",
    "\n",
    "data=data.merge(df_pages_agg_filtered[['page_location']],\n",
    "                on=['page_location'],\n",
    "                how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split categories in page paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split 'page_location' into 4 parts (max)\n",
    "split_columns = data['page_location'].str.split('/', n=4, expand=True)\n",
    "\n",
    "# Step 2: Assign the first three parts to new columns (ignore the first empty part if there is a leading '/')\n",
    "data['domain'] = split_columns[0] # url_domain \n",
    "data['page_path_level_1'] = split_columns[1].replace('', pd.NA)\n",
    "data['page_path_level_2'] = split_columns[2].replace('', pd.NA)\n",
    "data['page_path_level_3'] = split_columns[3].replace('', pd.NA)\n",
    "\n",
    "# Count the number of non-null levels in the path\n",
    "data['path_length'] = data[['page_path_level_1', 'page_path_level_2', 'page_path_level_3']].notna().sum(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_pages_total=data[['page_title','page_location','page_path_level_1','page_path_level_2','page_path_level_3','path_length']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill length page path with page title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['page_path_level_1'] = data.apply(\n",
    "    lambda row: row['page_title'] if pd.isna(row['page_path_level_2']) else row['page_path_level_1'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_pages_total=data[['page_title','page_location','page_path_level_1','page_path_level_2','page_path_level_3']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## page location long tail conversion to \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Step 1: Aggregate events by 'page_location' and 'page_path_level_3'\n",
    "# df_pages_agg = data.groupby(['page_location', 'page_path_level_3']).agg(\n",
    "#     total_events=('event_timestamp', 'count'),\n",
    "# ).sort_values('total_events', ascending=False).reset_index()\n",
    "\n",
    "# # Step 2: Calculate the desired percentile of total events to consider long tail\n",
    "# event_count_percentile = df_pages_agg['total_events'].quantile(0.75)\n",
    "\n",
    "# # Step 3: Create a mask for long tail pages\n",
    "# long_tail_mask = df_pages_agg['total_events'] < event_count_percentile\n",
    "\n",
    "# # Step 4: Create a dictionary to map original page_path_level_3 to \"other\"\n",
    "# long_tail_mapping = dict(zip(df_pages_agg['page_path_level_3'], \n",
    "#                              df_pages_agg['page_path_level_3'].where(~long_tail_mask, \"other\")))\n",
    "\n",
    "\n",
    "# # Step 5: Replace the values directly in the original DataFrame\n",
    "# # Only apply the mapping if path_length is 3\n",
    "# data['page_path_level_3'] = data.apply(\n",
    "#     lambda row: long_tail_mapping.get(row['page_path_level_3'], row['page_path_level_3']) if row['path_length'] == 3 else row['page_path_level_3'],\n",
    "#     axis=1\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill the other page path levels with previous page path column\n",
    "\n",
    "This will allow for hierarchical encoding without sacrificing columns or rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill 'page_path_level_2' by concatenating 'page_path_1' and 'page_path_2' (if 'page_path_2' is null)\n",
    "data['page_path_level_2'] = data.apply(\n",
    "    lambda row: row['page_path_level_1'] if pd.isna(row['page_path_level_2']) \n",
    "    else f\"{row['page_path_level_1']}/{row['page_path_level_2']}\", axis=1\n",
    ")\n",
    "\n",
    "# Fill 'page_path_level_3' by concatenating 'page_path_level_2' and 'page_path_3' (if 'page_path_3' is null)\n",
    "data['page_path_level_3'] = data.apply(\n",
    "    lambda row: row['page_path_level_2'] if pd.isna(row['page_path_level_3']) \n",
    "    else f\"{row['page_path_level_2']}/{row['page_path_level_3']}\", axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final df without relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final=data.drop(['page_title','page_location','session_id','user_pseudo_id','event_timestamp','local_date','event_date'],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(371714, 42)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.to_csv('data/df_merch_pre_proc.csv',index=False)\n",
    "\n",
    "# df_merch_pre_proc.csv\n",
    "# df_merch_profile.csv\n",
    "# df_merch_data_prep.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel File for encoding mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: property 'book' of 'OpenpyxlWriter' object has no setter\n",
      "The file might be corrupt or invalid. Creating a new file.\n",
      "Excel file has been updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "\n",
    "# Function to append distinct combinations of selected columns into sheets in an Excel file\n",
    "def append_columns_to_excel(df, columns_dict, output_file):\n",
    "    \"\"\"\n",
    "    Append distinct combinations of selected columns into separate sheets in an existing Excel file,\n",
    "    with the columns ordered by their names for easier hierarchical encoding and add an empty encoding column.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The DataFrame containing the columns to save.\n",
    "    columns_dict (dict): Dictionary where keys are sheet names, and values are lists of column names to include.\n",
    "    output_file (str): The path of the Excel file to save the sheets.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check if the file exists and is a valid Excel file\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            # Try to load the existing workbook\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "                writer.book = load_workbook(output_file)\n",
    "                \n",
    "                # Loop over each sheet name and corresponding list of columns\n",
    "                for sheet_name, columns in columns_dict.items():\n",
    "                    # Check if all the specified columns exist in the DataFrame\n",
    "                    missing_columns = [col for col in columns if col not in df.columns]\n",
    "                    if missing_columns:\n",
    "                        print(f\"Warning: The following columns are not found in the DataFrame for sheet '{sheet_name}': {missing_columns}\")\n",
    "                        continue\n",
    "\n",
    "                    # Get distinct combinations of the selected columns\n",
    "                    distinct_values = df[columns].drop_duplicates().dropna(how='all')\n",
    "\n",
    "                    # Convert columns to strings temporarily for sorting to avoid float-string comparison errors\n",
    "                    distinct_values = distinct_values.astype(str)\n",
    "\n",
    "                    # Sort distinct values by the specified columns for hierarchical grouping\n",
    "                    distinct_values.sort_values(by=columns, inplace=True)\n",
    "\n",
    "                    # Add an empty encoding column for each column in the DataFrame\n",
    "                    for col in columns:\n",
    "                        distinct_values[f'{col}_enc'] = pd.NA\n",
    "\n",
    "                    # Write distinct values to a new sheet named after the sheet_name\n",
    "                    distinct_values.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"The file might be corrupt or invalid. Creating a new file.\")\n",
    "            # Create a new file if loading fails\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl', mode='w') as writer:\n",
    "                for sheet_name, columns in columns_dict.items():\n",
    "                    missing_columns = [col for col in columns if col not in df.columns]\n",
    "                    if missing_columns:\n",
    "                        print(f\"Warning: The following columns are not found in the DataFrame for sheet '{sheet_name}': {missing_columns}\")\n",
    "                        continue\n",
    "\n",
    "                    # Get distinct combinations of the selected columns\n",
    "                    distinct_values = df[columns].drop_duplicates().dropna(how='all')\n",
    "\n",
    "                    # Convert columns to strings temporarily for sorting\n",
    "                    distinct_values = distinct_values.astype(str)\n",
    "\n",
    "                    # Sort distinct values by the specified columns for hierarchical grouping\n",
    "                    distinct_values.sort_values(by=columns, inplace=True)\n",
    "\n",
    "                    # Add an empty encoding column for each column in the DataFrame\n",
    "                    for col in columns:\n",
    "                        distinct_values[f'{col}_enc'] = pd.NA\n",
    "\n",
    "                    distinct_values.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    else:\n",
    "        # If the file does not exist, create a new one\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl', mode='w') as writer:\n",
    "            for sheet_name, columns in columns_dict.items():\n",
    "                missing_columns = [col for col in columns if col not in df.columns]\n",
    "                if missing_columns:\n",
    "                    print(f\"Warning: The following columns are not found in the DataFrame for sheet '{sheet_name}': {missing_columns}\")\n",
    "                    continue\n",
    "\n",
    "                # Get distinct combinations of the selected columns\n",
    "                distinct_values = df[columns].drop_duplicates().dropna(how='all')\n",
    "\n",
    "                # Convert columns to strings temporarily for sorting\n",
    "                distinct_values = distinct_values.astype(str)\n",
    "\n",
    "                # Sort distinct values by the specified columns for hierarchical grouping\n",
    "                distinct_values.sort_values(by=columns, inplace=True)\n",
    "\n",
    "                # Add an empty encoding column for each column in the DataFrame\n",
    "                for col in columns:\n",
    "                    distinct_values[f'{col}_enc'] = pd.NA\n",
    "\n",
    "                distinct_values.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                \n",
    "\n",
    "# Define the groups of columns for hierarchical encoding, grouped by sheet name\n",
    "columns_to_save = {\n",
    "    \n",
    "    'event_name' : ['event_name'],  \n",
    "    'device_category': ['device_category'],   \n",
    "    'device_mobile_brand_name': ['device_mobile_brand_name'],\n",
    "    'device_mobile_model_name': ['device_mobile_model_name'],\n",
    "    'device_language': ['device_language'],\n",
    "    'device_category': ['device_category'],      \n",
    "    'device_operating_system_version': ['device_operating_system_version'],   \n",
    "    'device_operating_system': ['device_operating_system'],   \n",
    "    'device_web_info_browser': ['device_web_info_browser'],   \n",
    "    'device_web_info_browser_version': ['device_web_info_browser_version'],\n",
    "    'geo_country': ['geo_country'],   \n",
    "    'traffic_source_medium':['traffic_source_medium'],\n",
    "    'traffic_source_source':['traffic_source_source'],\n",
    "    'domain':['domain'],\n",
    "    'page_path_level_1':['page_path_level_1'],\n",
    "    'page_path_level_2':['page_path_level_2'],\n",
    "    'page_path_level_3':['page_path_level_3'],\n",
    "    'year':['year'],\n",
    "    'quarter':['quarter'],\n",
    "    'month':['month'],\n",
    "    'day_of_year':['day_of_year'],\n",
    "    'week_number':['week_number']   \n",
    "\n",
    "}\n",
    "\n",
    "# Save the distinct values combinations of each column group into corresponding sheets\n",
    "append_columns_to_excel(data_final, columns_to_save, f'data/df_merch_values_pre_encoding.xlsx')\n",
    "\n",
    "print(\"Excel file has been updated successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
