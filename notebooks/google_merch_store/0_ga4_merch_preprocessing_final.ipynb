{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duvidas\n",
    "\n",
    "1. Qual é o threshold da distribuiçao para podermos considerar um valor como \"long tail\" e podermos agrupar com um valor de \"other\"? \n",
    "    - no caso de geo_cities testou-se com percentil 60 e passámos a ter 100 e poucos valores distintos. \n",
    "    - ja na page_path_level_3 ao percentil 75 ainda sao perto de 200 valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "filepath=r'data/google_merch_store_raw_merge_america.csv'\n",
    "\n",
    "\n",
    "\n",
    "file_tag = \"ga4_merch_store\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSLabs functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"scripts/dslabs_functions.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_functions lodaded\n"
     ]
    }
   ],
   "source": [
    "%run \"scripts/data_functions.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# north america filtering df creation\n",
    "\n",
    "- use if you only have full df version and not northern america filtered file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.read_csv(r'google_merch_store_raw_merge.csv')\n",
    "\n",
    "# data=data[data['geo_sub_continent']=='Northern America']\n",
    "\n",
    "# data.to_csv('google_merch_store_raw_merge_america.csv',index=False)\n",
    "\n",
    "# data.to_csv('google_merch_store_raw_merge_america_compressed.csv',index=False, compression='gzip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 405866 entries, 0 to 405865\n",
      "Data columns (total 48 columns):\n",
      " #   Column                           Non-Null Count   Dtype              \n",
      "---  ------                           --------------   -----              \n",
      " 0   event_date                       405866 non-null  datetime64[ns]     \n",
      " 1   session_id                       405866 non-null  int64              \n",
      " 2   user_pseudo_id                   405866 non-null  float64            \n",
      " 3   event_name                       405866 non-null  object             \n",
      " 4   event_timestamp                  405866 non-null  datetime64[ns, UTC]\n",
      " 5   page_location                    405866 non-null  object             \n",
      " 6   page_title                       404744 non-null  object             \n",
      " 7   device_category                  405866 non-null  object             \n",
      " 8   device_mobile_brand_name         405866 non-null  object             \n",
      " 9   device_mobile_model_name         405866 non-null  object             \n",
      " 10  device_mobile_marketing_name     405866 non-null  object             \n",
      " 11  device_operating_system          405866 non-null  object             \n",
      " 12  device_operating_system_version  405866 non-null  object             \n",
      " 13  device_language                  222520 non-null  object             \n",
      " 14  device_is_limited_ad_tracking    405866 non-null  object             \n",
      " 15  device_web_info_browser          405866 non-null  object             \n",
      " 16  device_web_info_browser_version  405866 non-null  object             \n",
      " 17  geo_continent                    405866 non-null  object             \n",
      " 18  geo_country                      405866 non-null  object             \n",
      " 19  geo_region                       405866 non-null  object             \n",
      " 20  geo_city                         405866 non-null  object             \n",
      " 21  geo_sub_continent                405866 non-null  object             \n",
      " 22  geo_metro                        405866 non-null  object             \n",
      " 23  traffic_source_name              405866 non-null  object             \n",
      " 24  traffic_source_medium            405866 non-null  object             \n",
      " 25  traffic_source_source            405866 non-null  object             \n",
      " 26  page_referrer                    141547 non-null  object             \n",
      " 27  entrances                        17065 non-null   float64            \n",
      " 28  debug_mode                       374509 non-null  float64            \n",
      " 29  ga_session_number                405866 non-null  int64              \n",
      " 30  session_engaged                  374531 non-null  float64            \n",
      " 31  engagement_time_msec             308338 non-null  float64            \n",
      " 32  ecommerce_total_item_quantity    39471 non-null   float64            \n",
      " 33  ecommerce_purchase_revenue       797 non-null     float64            \n",
      " 34  ecommerce_shipping_value         0 non-null       float64            \n",
      " 35  ecommerce_tax_value              797 non-null     float64            \n",
      " 36  ecommerce_unique_items           208988 non-null  float64            \n",
      " 37  ecommerce_transaction_id         220167 non-null  object             \n",
      " 38  item_id                          208988 non-null  object             \n",
      " 39  item_name                        208988 non-null  object             \n",
      " 40  item_brand                       208843 non-null  object             \n",
      " 41  item_variant                     207614 non-null  object             \n",
      " 42  item_category                    208135 non-null  object             \n",
      " 43  price                            201462 non-null  float64            \n",
      " 44  quantity                         7905 non-null    float64            \n",
      " 45  item_revenue                     208988 non-null  float64            \n",
      " 46  item_list_index                  203888 non-null  float64            \n",
      " 47  promotion_name                   208173 non-null  object             \n",
      "dtypes: datetime64[ns, UTC](1), datetime64[ns](1), float64(14), int64(2), object(30)\n",
      "memory usage: 148.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# test_data=True\n",
    "test_data=False\n",
    "\n",
    "\n",
    "# Define a function to sample 10% from each group\n",
    "def sample_per_day(group, fraction=0.1):\n",
    "    return group.sample(frac=fraction)\n",
    "\n",
    "\n",
    "if test_data==True:\n",
    "\n",
    "    data=pd.read_csv(filepath)\n",
    "    \n",
    "\n",
    "    data['event_timestamp'] = pd.to_datetime(data['event_timestamp'], unit='us', utc=True)\n",
    "    data['event_date'] = pd.to_datetime(data['event_date'], infer_datetime_format=True)\n",
    " \n",
    "\n",
    "    # Apply the sampling to each group (grouped by event_date) 1%\n",
    "    data = data.groupby('event_date').apply(lambda x: sample_per_day(x, 0.01)).reset_index(drop=True)\n",
    "\n",
    "   \n",
    "\n",
    "else:\n",
    "    data=pd.read_csv(filepath)\n",
    "    \n",
    "\n",
    "    data['event_timestamp'] = pd.to_datetime(data['event_timestamp'], unit='us', utc=True)\n",
    "    data['event_date'] = pd.to_datetime(data['event_date'], infer_datetime_format=True)\n",
    "\n",
    "    # 10% sample\n",
    "    data = data.groupby('event_date').apply(lambda x: sample_per_day(x, 0.1)).reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(405866, 48)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class target column creation\n",
    "\n",
    "\n",
    "- we want to classify if that hit is from a returning or new user.\n",
    "- Due to web analytics tracking particularities like cookie acceptance we prefer to consider returning users as users that are on their 3rd or larger session number\n",
    "- In this case, new user (ga_session_number <=2) will be 0 and returning user will be more than 2 (ga_session_number > 2)\n",
    "- session number column shall be removed afterwards as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['returning_user'] = data['ga_session_number'].apply(lambda x: 0 if x <= 2 else 1)\n",
    "\n",
    "\n",
    "data=data.drop(['ga_session_number'],axis=1) # now we do not need it anymore. remove it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returning_user\n",
      "0    0.75484\n",
      "1    0.24516\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "target = \"returning_user\"\n",
    "\n",
    "values = data[target].value_counts(normalize=True) \n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unbalance dataset\n",
    "\n",
    "let us umbalance to have 10% as returning users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returning_user\n",
      "0    90.000118\n",
      "1     9.999882\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Separate the majority (0) and minority (1) classes\n",
    "df_majority = data[data['returning_user'] == 0]\n",
    "df_minority = data[data['returning_user'] == 1]\n",
    "\n",
    "# Calculate the number of minority rows needed to make a 90/10 split\n",
    "# Let total_rows be the total number of rows after undersampling\n",
    "total_rows = len(df_majority) / 0.9  # 90% majority, 10% minority\n",
    "desired_minority_count = int(total_rows * 0.1)  # 10% of the total should be minority\n",
    "\n",
    "# Downsample the minority class to the desired number of rows\n",
    "df_minority_downsampled = df_minority.sample(n=desired_minority_count, random_state=42)\n",
    "\n",
    "# Combine the majority class with the downsampled minority class\n",
    "df_imbalanced = pd.concat([df_majority, df_minority_downsampled])\n",
    "\n",
    "# Shuffle the combined dataset\n",
    "data = df_imbalanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Check the new class distribution to verify the 90/10 split\n",
    "print(data['returning_user'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# column drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### low value or high null count columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['debug_mode','device_is_limited_ad_tracking','device_mobile_marketing_name','geo_metro','traffic_source_name','page_referrer','entrances'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ecommerce specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop([\n",
    " 'ecommerce_total_item_quantity', \n",
    " 'ecommerce_purchase_revenue',            \n",
    " 'ecommerce_shipping_value',              \n",
    " 'ecommerce_tax_value',                   \n",
    " 'ecommerce_unique_items',               \n",
    " 'ecommerce_transaction_id',              \n",
    " 'item_id',                               \n",
    " 'item_name',                             \n",
    " 'item_brand',                            \n",
    " 'item_variant',                          \n",
    " 'item_category',                         \n",
    " 'price',                                 \n",
    " 'quantity',                              \n",
    " 'item_revenue',                          \n",
    " 'item_list_index',                       \n",
    " 'promotion_name',\n",
    " 'geo_continent',\n",
    " 'geo_sub_continent'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_date</th>\n",
       "      <th>session_id</th>\n",
       "      <th>user_pseudo_id</th>\n",
       "      <th>event_name</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>page_location</th>\n",
       "      <th>page_title</th>\n",
       "      <th>device_category</th>\n",
       "      <th>device_mobile_brand_name</th>\n",
       "      <th>device_mobile_model_name</th>\n",
       "      <th>...</th>\n",
       "      <th>device_web_info_browser</th>\n",
       "      <th>device_web_info_browser_version</th>\n",
       "      <th>geo_country</th>\n",
       "      <th>geo_region</th>\n",
       "      <th>geo_city</th>\n",
       "      <th>traffic_source_medium</th>\n",
       "      <th>traffic_source_source</th>\n",
       "      <th>session_engaged</th>\n",
       "      <th>engagement_time_msec</th>\n",
       "      <th>returning_user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>340404</td>\n",
       "      <td>3.404040e+05</td>\n",
       "      <td>3.404040e+05</td>\n",
       "      <td>340404</td>\n",
       "      <td>340404</td>\n",
       "      <td>340404</td>\n",
       "      <td>339384</td>\n",
       "      <td>340404</td>\n",
       "      <td>340404</td>\n",
       "      <td>340404</td>\n",
       "      <td>...</td>\n",
       "      <td>340404</td>\n",
       "      <td>340404</td>\n",
       "      <td>340404</td>\n",
       "      <td>340404</td>\n",
       "      <td>340404</td>\n",
       "      <td>340404</td>\n",
       "      <td>340404</td>\n",
       "      <td>310797.000000</td>\n",
       "      <td>2.538770e+05</td>\n",
       "      <td>340404.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1050</td>\n",
       "      <td>477</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>325</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>view_item</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/</td>\n",
       "      <td>Home</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>...</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>87.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>organic</td>\n",
       "      <td>google</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123713</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31134</td>\n",
       "      <td>55022</td>\n",
       "      <td>198814</td>\n",
       "      <td>144244</td>\n",
       "      <td>94303</td>\n",
       "      <td>...</td>\n",
       "      <td>232176</td>\n",
       "      <td>126080</td>\n",
       "      <td>291358</td>\n",
       "      <td>61222</td>\n",
       "      <td>147408</td>\n",
       "      <td>118375</td>\n",
       "      <td>123467</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2020-12-14 05:40:55.289597184</td>\n",
       "      <td>5.000437e+09</td>\n",
       "      <td>2.675488e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-12-14 17:42:24.196818944+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.907432</td>\n",
       "      <td>1.218254e+04</td>\n",
       "      <td>0.099999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-11-01 00:00:00</td>\n",
       "      <td>1.762600e+04</td>\n",
       "      <td>1.001129e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-01 00:02:32.447440+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2020-11-26 00:00:00</td>\n",
       "      <td>2.532614e+09</td>\n",
       "      <td>5.809797e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-26 05:48:50.694297856+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.675000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2020-12-11 00:00:00</td>\n",
       "      <td>4.999471e+09</td>\n",
       "      <td>2.249628e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-12-11 12:55:26.811782912+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.528000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2021-01-04 00:00:00</td>\n",
       "      <td>7.510019e+09</td>\n",
       "      <td>6.143844e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-04 20:59:29.549153280+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.272500e+04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2021-01-31 00:00:00</td>\n",
       "      <td>9.999964e+09</td>\n",
       "      <td>9.985207e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-31 23:59:54.824131+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.042680e+07</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.879110e+09</td>\n",
       "      <td>1.200010e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.289827</td>\n",
       "      <td>4.991454e+04</td>\n",
       "      <td>0.299999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           event_date    session_id  user_pseudo_id  \\\n",
       "count                          340404  3.404040e+05    3.404040e+05   \n",
       "unique                            NaN           NaN             NaN   \n",
       "top                               NaN           NaN             NaN   \n",
       "freq                              NaN           NaN             NaN   \n",
       "mean    2020-12-14 05:40:55.289597184  5.000437e+09    2.675488e+08   \n",
       "min               2020-11-01 00:00:00  1.762600e+04    1.001129e+06   \n",
       "25%               2020-11-26 00:00:00  2.532614e+09    5.809797e+06   \n",
       "50%               2020-12-11 00:00:00  4.999471e+09    2.249628e+07   \n",
       "75%               2021-01-04 00:00:00  7.510019e+09    6.143844e+07   \n",
       "max               2021-01-31 00:00:00  9.999964e+09    9.985207e+09   \n",
       "std                               NaN  2.879110e+09    1.200010e+09   \n",
       "\n",
       "       event_name                      event_timestamp  \\\n",
       "count      340404                               340404   \n",
       "unique         17                                  NaN   \n",
       "top     view_item                                  NaN   \n",
       "freq       123713                                  NaN   \n",
       "mean          NaN  2020-12-14 17:42:24.196818944+00:00   \n",
       "min           NaN     2020-11-01 00:02:32.447440+00:00   \n",
       "25%           NaN  2020-11-26 05:48:50.694297856+00:00   \n",
       "50%           NaN  2020-12-11 12:55:26.811782912+00:00   \n",
       "75%           NaN  2021-01-04 20:59:29.549153280+00:00   \n",
       "max           NaN     2021-01-31 23:59:54.824131+00:00   \n",
       "std           NaN                                  NaN   \n",
       "\n",
       "                                   page_location page_title device_category  \\\n",
       "count                                     340404     339384          340404   \n",
       "unique                                      1050        477               3   \n",
       "top     https://shop.googlemerchandisestore.com/       Home         desktop   \n",
       "freq                                       31134      55022          198814   \n",
       "mean                                         NaN        NaN             NaN   \n",
       "min                                          NaN        NaN             NaN   \n",
       "25%                                          NaN        NaN             NaN   \n",
       "50%                                          NaN        NaN             NaN   \n",
       "75%                                          NaN        NaN             NaN   \n",
       "max                                          NaN        NaN             NaN   \n",
       "std                                          NaN        NaN             NaN   \n",
       "\n",
       "       device_mobile_brand_name device_mobile_model_name  ...  \\\n",
       "count                    340404                   340404  ...   \n",
       "unique                        8                       10  ...   \n",
       "top                       Apple                   Chrome  ...   \n",
       "freq                     144244                    94303  ...   \n",
       "mean                        NaN                      NaN  ...   \n",
       "min                         NaN                      NaN  ...   \n",
       "25%                         NaN                      NaN  ...   \n",
       "50%                         NaN                      NaN  ...   \n",
       "75%                         NaN                      NaN  ...   \n",
       "max                         NaN                      NaN  ...   \n",
       "std                         NaN                      NaN  ...   \n",
       "\n",
       "       device_web_info_browser device_web_info_browser_version    geo_country  \\\n",
       "count                   340404                          340404         340404   \n",
       "unique                       6                              13              2   \n",
       "top                     Chrome                            87.0  United States   \n",
       "freq                    232176                          126080         291358   \n",
       "mean                       NaN                             NaN            NaN   \n",
       "min                        NaN                             NaN            NaN   \n",
       "25%                        NaN                             NaN            NaN   \n",
       "50%                        NaN                             NaN            NaN   \n",
       "75%                        NaN                             NaN            NaN   \n",
       "max                        NaN                             NaN            NaN   \n",
       "std                        NaN                             NaN            NaN   \n",
       "\n",
       "        geo_region   geo_city traffic_source_medium traffic_source_source  \\\n",
       "count       340404     340404                340404                340404   \n",
       "unique          62        325                     6                     5   \n",
       "top     California  (not set)               organic                google   \n",
       "freq         61222     147408                118375                123467   \n",
       "mean           NaN        NaN                   NaN                   NaN   \n",
       "min            NaN        NaN                   NaN                   NaN   \n",
       "25%            NaN        NaN                   NaN                   NaN   \n",
       "50%            NaN        NaN                   NaN                   NaN   \n",
       "75%            NaN        NaN                   NaN                   NaN   \n",
       "max            NaN        NaN                   NaN                   NaN   \n",
       "std            NaN        NaN                   NaN                   NaN   \n",
       "\n",
       "       session_engaged engagement_time_msec returning_user  \n",
       "count    310797.000000         2.538770e+05  340404.000000  \n",
       "unique             NaN                  NaN            NaN  \n",
       "top                NaN                  NaN            NaN  \n",
       "freq               NaN                  NaN            NaN  \n",
       "mean          0.907432         1.218254e+04       0.099999  \n",
       "min           0.000000         1.000000e+00       0.000000  \n",
       "25%           1.000000         1.675000e+03       0.000000  \n",
       "50%           1.000000         5.528000e+03       0.000000  \n",
       "75%           1.000000         1.272500e+04       0.000000  \n",
       "max           1.000000         2.042680e+07       1.000000  \n",
       "std           0.289827         4.991454e+04       0.299999  \n",
       "\n",
       "[11 rows x 23 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary5 = data.describe(include=\"all\")\n",
    "\n",
    "summary5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drop specific event_names\n",
    "\n",
    "We want to classify a user by its interactions with the website so we want to exclude some actions that may also be biased by incorrect ga4 tracking namely:\n",
    "- session_start\n",
    "- first_visit\n",
    "- click (low event count)\n",
    "- view_item_list (may not be triggered by user interaction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data==True:\n",
    "    data['event_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values to drop\n",
    "events_to_drop = ['session_start', 'first_visit','click','view_item_list']\n",
    "\n",
    "# drop events from list\n",
    "data = data[~data['event_name'].isin(events_to_drop)]\n",
    "\n",
    "if test_data==True:\n",
    "    data['event_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replace (not set) with null\n",
    "\n",
    "we will handle these later but these are actually null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace('(not set)', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engagement time msec\n",
    "- https://support.google.com/analytics/answer/11109416?hl=en\n",
    "- is it null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['engagement_time_msec'] = data['engagement_time_msec'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geo columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## geo_region and geo_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['California',\n",
       " 'Florida',\n",
       " 'Texas',\n",
       " 'New Jersey',\n",
       " 'Pennsylvania',\n",
       " 'New York',\n",
       " 'Michigan',\n",
       " 'Ohio',\n",
       " 'Ontario',\n",
       " 'Illinois',\n",
       " 'Massachusetts',\n",
       " 'Georgia',\n",
       " 'North Carolina',\n",
       " 'Virginia',\n",
       " 'Quebec',\n",
       " 'Maryland',\n",
       " 'Connecticut',\n",
       " 'Indiana',\n",
       " 'Missouri',\n",
       " 'Wisconsin',\n",
       " 'Washington',\n",
       " 'Colorado',\n",
       " 'South Carolina',\n",
       " 'Minnesota',\n",
       " 'British Columbia',\n",
       " 'Utah',\n",
       " 'Tennessee',\n",
       " 'Alberta',\n",
       " 'Alabama',\n",
       " 'Louisiana',\n",
       " 'Oregon',\n",
       " 'Kentucky',\n",
       " 'Iowa',\n",
       " 'Mississippi',\n",
       " 'Arizona',\n",
       " 'Arkansas',\n",
       " 'Oklahoma',\n",
       " 'Kansas',\n",
       " 'Delaware',\n",
       " 'New Hampshire',\n",
       " 'West Virginia',\n",
       " 'Manitoba',\n",
       " 'Idaho',\n",
       " 'Maine',\n",
       " 'North Dakota',\n",
       " 'Saskatchewan',\n",
       " 'Montana',\n",
       " 'Rhode Island',\n",
       " 'New Mexico',\n",
       " 'Nova Scotia',\n",
       " 'Alaska',\n",
       " 'New Brunswick',\n",
       " 'Vermont',\n",
       " 'Nebraska',\n",
       " 'Prince Edward Island',\n",
       " 'Newfoundland and Labrador',\n",
       " 'Nevada',\n",
       " 'Hawaii',\n",
       " 'Wyoming',\n",
       " 'South Dakota']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_region_cities_df=data.groupby(['geo_region']).agg(\n",
    "    null_count=('geo_city', lambda x: x.isna().sum())\n",
    ").sort_values('null_count',ascending=False).reset_index()\n",
    "\n",
    "# will retrieve the most populated cities of these regions to use as fill method\n",
    "region_cities_with_nulls = null_region_cities_df[null_region_cities_df['null_count'] > 0]['geo_region'].tolist()\n",
    "\n",
    "\n",
    "region_cities_with_nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## geo mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_populated_cities = {\n",
    "    'California': 'Los Angeles',\n",
    "    'Florida': 'Jacksonville',\n",
    "    'Texas': 'Houston',\n",
    "    'New Jersey': 'Newark',\n",
    "    'New York': 'New York City',\n",
    "    'Pennsylvania': 'Philadelphia',\n",
    "    'Michigan': 'Detroit',\n",
    "    'Ohio': 'Columbus',\n",
    "    'Illinois': 'Chicago',\n",
    "    'Georgia': 'Atlanta',\n",
    "    'Massachusetts': 'Boston',\n",
    "    'North Carolina': 'Charlotte',\n",
    "    'Virginia': 'Virginia Beach',\n",
    "    'Maryland': 'Baltimore',\n",
    "    'Connecticut': 'Bridgeport',\n",
    "    'Indiana': 'Indianapolis',\n",
    "    'Wisconsin': 'Milwaukee',\n",
    "    'Missouri': 'Kansas City',\n",
    "    'Washington': 'Seattle',\n",
    "    'Colorado': 'Denver',\n",
    "    'Minnesota': 'Minneapolis',\n",
    "    'South Carolina': 'Charleston',\n",
    "    'Utah': 'Salt Lake City',\n",
    "    'Tennessee': 'Nashville',\n",
    "    'Alabama': 'Birmingham',\n",
    "    'Louisiana': 'New Orleans',\n",
    "    'Oregon': 'Portland',\n",
    "    'Kentucky': 'Louisville',\n",
    "    'Mississippi': 'Jackson',\n",
    "    'Iowa': 'Des Moines',\n",
    "    'Arkansas': 'Little Rock',\n",
    "    'Arizona': 'Phoenix',\n",
    "    'Oklahoma': 'Oklahoma City',\n",
    "    'Kansas': 'Wichita',\n",
    "    'Delaware': 'Wilmington',\n",
    "    'West Virginia': 'Charleston',\n",
    "    'New Hampshire': 'Manchester',\n",
    "    'Idaho': 'Boise',\n",
    "    'Montana': 'Billings',\n",
    "    'Maine': 'Portland',\n",
    "    'New Mexico': 'Albuquerque',\n",
    "    'North Dakota': 'Fargo',\n",
    "    'Rhode Island': 'Providence',\n",
    "    'Nebraska': 'Omaha',\n",
    "    'Alaska': 'Anchorage',\n",
    "    'Vermont': 'Burlington',\n",
    "    'Hawaii': 'Honolulu',\n",
    "    'Nevada': 'Las Vegas',\n",
    "    'Wyoming': 'Cheyenne',\n",
    "    'South Dakota': 'Sioux Falls'\n",
    "}\n",
    "\n",
    "data['geo_city'] = data['geo_city'].fillna(data['geo_region'].map(most_populated_cities))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## geo city long tail conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Aggregate events by 'page_location' and 'page_path_level_3'\n",
    "df_cities_agg = data.groupby(['geo_city']).agg(\n",
    "    total_events=('event_timestamp', 'count'),\n",
    ").sort_values('total_events', ascending=False).reset_index()\n",
    "\n",
    "# Step 2: Calculate the desired percentile of total events to consider long tail\n",
    "event_count_percentile = df_cities_agg['total_events'].quantile(0.65)\n",
    "\n",
    "# Step 3: Create a mask for long tail pages\n",
    "long_tail_mask = df_cities_agg['total_events'] < event_count_percentile\n",
    "\n",
    "# Step 4: Create a dictionary to map original page_path_level_3 to \"other\" (long tail)\n",
    "long_tail_mapping = dict(zip(df_cities_agg['geo_city'], \n",
    "                             df_cities_agg['geo_city'].where(~long_tail_mask, \"Other\")))\n",
    "\n",
    "\n",
    "# Step 5: Replace the values directly in the original DataFrame\n",
    "data['geo_city'] = data.apply(\n",
    "    lambda row: long_tail_mapping.get(row['geo_city'], row['geo_city']),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# event timestamp treatment\n",
    "\n",
    "- need to convert to local time from different us timezones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timezone from regions and create local_event_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tennessee', 'New Jersey', 'New York', 'Michigan', 'South Carolina', 'Rhode Island', 'Connecticut', 'California', 'Quebec', 'Wyoming', 'Nova Scotia', 'Idaho', 'Vermont', 'Maine', 'Oklahoma', 'Illinois', 'Mississippi', 'Utah', 'Colorado', 'Arizona', 'West Virginia', 'Alaska', 'Montana', 'District of Columbia', 'North Carolina', 'British Columbia', 'Texas', 'Saskatchewan', 'Alabama', 'Maryland', 'Arkansas', 'Nevada', 'Wisconsin', 'Alberta', nan, 'Kansas', 'Missouri', 'South Dakota', 'Kentucky', 'Iowa', 'New Mexico', 'Pennsylvania', 'Hawaii', 'Manitoba', 'North Dakota', 'Ohio', 'Washington', 'Prince Edward Island', 'Newfoundland and Labrador', 'Louisiana', 'Ontario', 'Oregon', 'New Brunswick', 'Nebraska', 'Georgia', 'Delaware', 'Virginia', 'New Hampshire', 'Florida', 'Massachusetts', 'Indiana', 'Minnesota']\n"
     ]
    }
   ],
   "source": [
    "geo_region_list = data['geo_region'].tolist()\n",
    "distinct_geo_regions = list(set(geo_region_list))\n",
    "\n",
    "print(distinct_geo_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### timezone mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "\n",
    "\n",
    "city_timezone_mapping = {\n",
    "    'Kingston': 'America/Toronto',  # Canada (Eastern Time)\n",
    "    'Calgary': 'America/Edmonton',  # Canada (Mountain Time)\n",
    "    'Bethesda': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Nashville': 'America/Chicago',  # USA (Central Time)\n",
    "    'Saint Paul': 'America/Chicago',  # USA (Central Time)\n",
    "    'Berkeley': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Little Rock': 'America/Chicago',  # USA (Central Time)\n",
    "    'Sunnyvale': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Salinas': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Ann Arbor': 'America/Detroit',  # USA (Eastern Time)\n",
    "    'Auburn': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Germantown': 'America/Chicago',  # USA (Central Time)\n",
    "    'Fairfield': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Charlotte': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Guelph': 'America/Toronto',  # Canada (Eastern Time)\n",
    "    'Vancouver': 'America/Vancouver',  # Canada (Pacific Time)\n",
    "    'Stamford': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Pleasanton': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Cambridge': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Paramus': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Jacksonville': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Livermore': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Eau Claire': 'America/Chicago',  # USA (Central Time)\n",
    "    'Sacramento': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Los Altos': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Dartmouth': 'America/Halifax',  # Canada (Atlantic Time)\n",
    "    'Everett': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Modesto': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Saint John': 'America/Halifax',  # Canada (Atlantic Time)\n",
    "    'Lethbridge': 'America/Edmonton',  # Canada (Mountain Time)\n",
    "    'Kansas City': 'America/Chicago',  # USA (Central Time)\n",
    "    'Madison': 'America/Chicago',  # USA (Central Time)\n",
    "    'Evanston': 'America/Chicago',  # USA (Central Time)\n",
    "    'Fontana': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Hayward': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Franklin': 'America/Chicago',  # USA (Central Time)\n",
    "    'Somerville': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Chantilly': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Halifax': 'America/Halifax',  # Canada (Atlantic Time)\n",
    "    'Laredo': 'America/Chicago',  # USA (Central Time)\n",
    "    'Windsor': 'America/Toronto',  # Canada (Eastern Time)\n",
    "    'Richardson': 'America/Chicago',  # USA (Central Time)\n",
    "    'Oshawa': 'America/Toronto',  # Canada (Eastern Time)\n",
    "    'Denver': 'America/Denver',  # USA (Mountain Time)\n",
    "    'Cherry Hill': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Fredericton': 'America/Moncton',  # Canada (Atlantic Time)\n",
    "    'Houston': 'America/Chicago',  # USA (Central Time)\n",
    "    'Regina': 'America/Regina',  # Canada (Central Time)\n",
    "    'Buffalo': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Coffeyville': 'America/Chicago',  # USA (Central Time)\n",
    "    'Sugar Land': 'America/Chicago',  # USA (Central Time)\n",
    "    'Johns Creek': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Winston-Salem': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Fargo': 'America/Chicago',  # USA (Central Time)\n",
    "    'Granby': 'America/Toronto',  # Canada (Eastern Time)\n",
    "    'New Orleans': 'America/Chicago',  # USA (Central Time)\n",
    "    'Hartford': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Bridgeport': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Reno': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Raleigh': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Miami': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Chicago': 'America/Chicago',  # USA (Central Time)\n",
    "    'San Francisco': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Los Angeles': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Philadelphia': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Anchorage': 'America/Anchorage',  # USA (Alaska Time)\n",
    "    'Minneapolis': 'America/Chicago',  # USA (Central Time)\n",
    "    'New York': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Boston': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Dallas': 'America/Chicago',  # USA (Central Time)\n",
    "    'Boulder': 'America/Denver',  # USA (Mountain Time)\n",
    "    'Seattle': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Montgomery': 'America/Chicago',  # USA (Central Time)\n",
    "    'Phoenix': 'America/Phoenix',  # USA (Mountain Standard Time, no DST)\n",
    "    'Las Vegas': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "}\n",
    "\n",
    "\n",
    "region_timezone_dict = {\n",
    "    'Texas': 'America/Chicago',           # Central Time\n",
    "    'Nova Scotia': 'America/Halifax',     # Atlantic Time\n",
    "    'Wisconsin': 'America/Chicago',       # Central Time\n",
    "    'Massachusetts': 'America/New_York',  # Eastern Time\n",
    "    'Nebraska': 'America/Chicago',        # Central Time\n",
    "    'Wyoming': 'America/Denver',          # Mountain Time\n",
    "    'Kentucky': 'America/New_York',       # Eastern Time\n",
    "    'Maryland': 'America/New_York',       # Eastern Time\n",
    "    'Arkansas': 'America/Chicago',        # Central Time\n",
    "    'Saskatchewan': 'America/Regina',     # Central Time (no DST)\n",
    "    'South Dakota': 'America/Chicago',    # Central Time\n",
    "    'Ohio': 'America/New_York',           # Eastern Time\n",
    "    'Nevada': 'America/Los_Angeles',      # Pacific Time\n",
    "    'Kansas': 'America/Chicago',          # Central Time\n",
    "    'Virginia': 'America/New_York',       # Eastern Time\n",
    "    'Minnesota': 'America/Chicago',       # Central Time\n",
    "    'Washington': 'America/Los_Angeles',  # Pacific Time\n",
    "    'Maine': 'America/New_York',          # Eastern Time\n",
    "    'Vermont': 'America/New_York',        # Eastern Time\n",
    "    'Alaska': 'America/Anchorage',        # Alaska Time\n",
    "    'Manitoba': 'America/Winnipeg',       # Central Time\n",
    "    'Alabama': 'America/Chicago',         # Central Time\n",
    "    'Iowa': 'America/Chicago',            # Central Time\n",
    "    'Rhode Island': 'America/New_York',   # Eastern Time\n",
    "    'Missouri': 'America/Chicago',        # Central Time\n",
    "    'Hawaii': 'Pacific/Honolulu',         # Hawaii-Aleutian Time (no DST)\n",
    "    'Florida': 'America/New_York',        # Eastern Time (some parts Central)\n",
    "    'Michigan': 'America/Detroit',        # Eastern Time\n",
    "    'Tennessee': 'America/Chicago',       # Central Time (some parts Eastern)\n",
    "    'Pennsylvania': 'America/New_York',   # Eastern Time\n",
    "    'Delaware': 'America/New_York',       # Eastern Time\n",
    "    'Prince Edward Island': 'America/Halifax',  # Atlantic Time\n",
    "    'District of Columbia': 'America/New_York', # Eastern Time\n",
    "    'North Dakota': 'America/Chicago',    # Central Time\n",
    "    'Oklahoma': 'America/Chicago',        # Central Time\n",
    "    'New Brunswick': 'America/Halifax',   # Atlantic Time\n",
    "    'Ontario': 'America/Toronto',         # Eastern Time\n",
    "    'Quebec': 'America/Toronto',          # Eastern Time\n",
    "    'Idaho': 'America/Boise',             # Mountain Time\n",
    "    'Indiana': 'America/Indiana/Indianapolis',  # Eastern Time\n",
    "    'New York': 'America/New_York',       # Eastern Time\n",
    "    'Mississippi': 'America/Chicago',     # Central Time\n",
    "    'Georgia': 'America/New_York',        # Eastern Time\n",
    "    'Illinois': 'America/Chicago',        # Central Time\n",
    "    'Louisiana': 'America/Chicago',       # Central Time\n",
    "    'New Jersey': 'America/New_York',     # Eastern Time\n",
    "    'West Virginia': 'America/New_York',  # Eastern Time\n",
    "    'South Carolina': 'America/New_York', # Eastern Time\n",
    "    'Alberta': 'America/Edmonton',        # Mountain Time\n",
    "    'Arizona': 'America/Phoenix',         # Mountain Time (no DST)\n",
    "    'North Carolina': 'America/New_York', # Eastern Time\n",
    "    'Newfoundland and Labrador': 'America/St_Johns',  # Newfoundland Time\n",
    "    'California': 'America/Los_Angeles',  # Pacific Time\n",
    "    'Utah': 'America/Denver',             # Mountain Time\n",
    "    'New Hampshire': 'America/New_York',  # Eastern Time\n",
    "    'Connecticut': 'America/New_York',    # Eastern Time\n",
    "    'Montana': 'America/Denver',          # Mountain Time\n",
    "    'British Columbia': 'America/Vancouver',  # Pacific Time\n",
    "    'Colorado': 'America/Denver',         # Mountain Time\n",
    "    'New Mexico': 'America/Denver',       # Mountain Time\n",
    "    'Oregon': 'America/Los_Angeles'       # Pacific Time\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timezone conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to convert the UTC datetime to local time and extract date, hour, and minute\n",
    "def extract_local_date_hour_minute(row, region_timezone_dict):\n",
    "    # Check if the row contains a valid timestamp\n",
    "    if pd.isnull(row['event_timestamp']):\n",
    "        return pd.NaT, pd.NaT, pd.NaT  # Return NaT for all if the timestamp is null\n",
    "\n",
    "    # Get the region and corresponding timezone\n",
    "    region = row['geo_region']\n",
    "    tz = region_timezone_dict.get(region, None)  # Fetch timezone, fallback to None\n",
    "    \n",
    "    # If a valid timezone is found, apply timezone conversion\n",
    "    if tz:\n",
    "        try:\n",
    "            local_time = row['event_timestamp'].tz_convert(tz)  # Convert to local time\n",
    "            \n",
    "            local_date = local_time.date()  # Extract local date\n",
    "            local_hour = local_time.hour  # Extract local hour\n",
    "            local_minute = local_time.minute  # Extract local minute\n",
    "            \n",
    "            return local_date, local_hour, local_minute\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting to timezone {tz}: {e}\")\n",
    "            return pd.NaT, pd.NaT, pd.NaT\n",
    "    else:\n",
    "        # Fallback to UTC timestamp\n",
    "        return row['event_timestamp'].date(), row['event_timestamp'].hour, row['event_timestamp'].minute\n",
    "\n",
    "# Apply the conversion function to the DataFrame\n",
    "data[['local_date', 'local_hour', 'local_minute']] = data.apply(\n",
    "    lambda row: extract_local_date_hour_minute(row, region_timezone_dict), axis=1, result_type='expand'\n",
    ")\n",
    "\n",
    "# Convert local_date to datetime format\n",
    "data['local_date'] = pd.to_datetime(data['local_date'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional date columns creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_date</th>\n",
       "      <th>session_id</th>\n",
       "      <th>user_pseudo_id</th>\n",
       "      <th>event_name</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>page_location</th>\n",
       "      <th>page_title</th>\n",
       "      <th>device_category</th>\n",
       "      <th>device_mobile_brand_name</th>\n",
       "      <th>device_mobile_model_name</th>\n",
       "      <th>...</th>\n",
       "      <th>hour_minute_fraction</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_number</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-30</td>\n",
       "      <td>6322724350</td>\n",
       "      <td>2.174615e+07</td>\n",
       "      <td>scroll</td>\n",
       "      <td>2020-11-30 12:29:50.348950+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/Google...</td>\n",
       "      <td>YouTube | Shop by Brand | Google Merchandise S...</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Google</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>...</td>\n",
       "      <td>6.48</td>\n",
       "      <td>Morning</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>Monday</td>\n",
       "      <td>335</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-11</td>\n",
       "      <td>648543119</td>\n",
       "      <td>4.950864e+07</td>\n",
       "      <td>view_item</td>\n",
       "      <td>2021-01-11 16:57:45.063731+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/Google...</td>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>mobile</td>\n",
       "      <td>&lt;Other&gt;</td>\n",
       "      <td>&lt;Other&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>10.95</td>\n",
       "      <td>Morning</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>Monday</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-12-14</td>\n",
       "      <td>5154837653</td>\n",
       "      <td>4.570995e+06</td>\n",
       "      <td>add_to_cart</td>\n",
       "      <td>2020-12-14 18:04:34.635924+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/Google...</td>\n",
       "      <td>Men's / Unisex | Apparel | Google Merchandise ...</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>...</td>\n",
       "      <td>10.07</td>\n",
       "      <td>Morning</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>Monday</td>\n",
       "      <td>349</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-12-12</td>\n",
       "      <td>1219740274</td>\n",
       "      <td>2.181566e+06</td>\n",
       "      <td>view_item</td>\n",
       "      <td>2020-12-12 04:05:54.170422+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/Google...</td>\n",
       "      <td>Sale | Google Merchandise Store</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Safari</td>\n",
       "      <td>...</td>\n",
       "      <td>23.08</td>\n",
       "      <td>Night</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>Friday</td>\n",
       "      <td>346</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-21</td>\n",
       "      <td>2963698078</td>\n",
       "      <td>3.214524e+06</td>\n",
       "      <td>view_item</td>\n",
       "      <td>2021-01-21 00:05:34.561945+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/Google...</td>\n",
       "      <td>Bags | Lifestyle | Google Merchandise Store</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Google</td>\n",
       "      <td>ChromeBook</td>\n",
       "      <td>...</td>\n",
       "      <td>0.08</td>\n",
       "      <td>Night</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340398</th>\n",
       "      <td>2020-12-03</td>\n",
       "      <td>4970452831</td>\n",
       "      <td>8.735713e+07</td>\n",
       "      <td>view_item</td>\n",
       "      <td>2020-12-03 01:18:13.972176+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/Google...</td>\n",
       "      <td>Hats | Apparel | Google Merchandise Store</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Apple</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>...</td>\n",
       "      <td>20.30</td>\n",
       "      <td>Evening</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>337</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340399</th>\n",
       "      <td>2020-12-05</td>\n",
       "      <td>2243860569</td>\n",
       "      <td>4.961809e+06</td>\n",
       "      <td>view_item</td>\n",
       "      <td>2020-12-05 07:43:38.422177+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/Google...</td>\n",
       "      <td>Campus Collection | Google Merchandise Store</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Google</td>\n",
       "      <td>ChromeBook</td>\n",
       "      <td>...</td>\n",
       "      <td>2.72</td>\n",
       "      <td>Night</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>340</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340401</th>\n",
       "      <td>2020-12-08</td>\n",
       "      <td>1984964573</td>\n",
       "      <td>4.160617e+07</td>\n",
       "      <td>page_view</td>\n",
       "      <td>2020-12-08 20:42:52.079397+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/Google...</td>\n",
       "      <td>Sale | Google Merchandise Store</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Google</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>...</td>\n",
       "      <td>14.70</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>343</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340402</th>\n",
       "      <td>2020-12-10</td>\n",
       "      <td>6618896043</td>\n",
       "      <td>5.334720e+07</td>\n",
       "      <td>user_engagement</td>\n",
       "      <td>2020-12-10 18:56:46.357472+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/Google...</td>\n",
       "      <td>Apparel | Google Merchandise Store</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>&lt;Other&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>12.93</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>345</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340403</th>\n",
       "      <td>2020-12-05</td>\n",
       "      <td>3196855152</td>\n",
       "      <td>1.392795e+07</td>\n",
       "      <td>view_item</td>\n",
       "      <td>2020-12-05 01:56:57.009396+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/Google...</td>\n",
       "      <td>Apparel | Google Merchandise Store</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Apple</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>...</td>\n",
       "      <td>20.93</td>\n",
       "      <td>Evening</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>Friday</td>\n",
       "      <td>339</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310732 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       event_date  session_id  user_pseudo_id       event_name  \\\n",
       "0      2020-11-30  6322724350    2.174615e+07           scroll   \n",
       "1      2021-01-11   648543119    4.950864e+07        view_item   \n",
       "2      2020-12-14  5154837653    4.570995e+06      add_to_cart   \n",
       "3      2020-12-12  1219740274    2.181566e+06        view_item   \n",
       "4      2021-01-21  2963698078    3.214524e+06        view_item   \n",
       "...           ...         ...             ...              ...   \n",
       "340398 2020-12-03  4970452831    8.735713e+07        view_item   \n",
       "340399 2020-12-05  2243860569    4.961809e+06        view_item   \n",
       "340401 2020-12-08  1984964573    4.160617e+07        page_view   \n",
       "340402 2020-12-10  6618896043    5.334720e+07  user_engagement   \n",
       "340403 2020-12-05  3196855152    1.392795e+07        view_item   \n",
       "\n",
       "                        event_timestamp  \\\n",
       "0      2020-11-30 12:29:50.348950+00:00   \n",
       "1      2021-01-11 16:57:45.063731+00:00   \n",
       "2      2020-12-14 18:04:34.635924+00:00   \n",
       "3      2020-12-12 04:05:54.170422+00:00   \n",
       "4      2021-01-21 00:05:34.561945+00:00   \n",
       "...                                 ...   \n",
       "340398 2020-12-03 01:18:13.972176+00:00   \n",
       "340399 2020-12-05 07:43:38.422177+00:00   \n",
       "340401 2020-12-08 20:42:52.079397+00:00   \n",
       "340402 2020-12-10 18:56:46.357472+00:00   \n",
       "340403 2020-12-05 01:56:57.009396+00:00   \n",
       "\n",
       "                                            page_location  \\\n",
       "0       https://shop.googlemerchandisestore.com/Google...   \n",
       "1       https://shop.googlemerchandisestore.com/Google...   \n",
       "2       https://shop.googlemerchandisestore.com/Google...   \n",
       "3       https://shop.googlemerchandisestore.com/Google...   \n",
       "4       https://shop.googlemerchandisestore.com/Google...   \n",
       "...                                                   ...   \n",
       "340398  https://shop.googlemerchandisestore.com/Google...   \n",
       "340399  https://shop.googlemerchandisestore.com/Google...   \n",
       "340401  https://shop.googlemerchandisestore.com/Google...   \n",
       "340402  https://shop.googlemerchandisestore.com/Google...   \n",
       "340403  https://shop.googlemerchandisestore.com/Google...   \n",
       "\n",
       "                                               page_title device_category  \\\n",
       "0       YouTube | Shop by Brand | Google Merchandise S...         desktop   \n",
       "1                                               Lifestyle          mobile   \n",
       "2       Men's / Unisex | Apparel | Google Merchandise ...         desktop   \n",
       "3                         Sale | Google Merchandise Store         desktop   \n",
       "4             Bags | Lifestyle | Google Merchandise Store         desktop   \n",
       "...                                                   ...             ...   \n",
       "340398          Hats | Apparel | Google Merchandise Store          mobile   \n",
       "340399       Campus Collection | Google Merchandise Store         desktop   \n",
       "340401                    Sale | Google Merchandise Store         desktop   \n",
       "340402                 Apparel | Google Merchandise Store          mobile   \n",
       "340403                 Apparel | Google Merchandise Store          mobile   \n",
       "\n",
       "       device_mobile_brand_name device_mobile_model_name  ...  \\\n",
       "0                        Google                   Chrome  ...   \n",
       "1                       <Other>                  <Other>  ...   \n",
       "2                       Mozilla                  Firefox  ...   \n",
       "3                         Apple                   Safari  ...   \n",
       "4                        Google               ChromeBook  ...   \n",
       "...                         ...                      ...  ...   \n",
       "340398                    Apple                   iPhone  ...   \n",
       "340399                   Google               ChromeBook  ...   \n",
       "340401                   Google                   Chrome  ...   \n",
       "340402                  Samsung                  <Other>  ...   \n",
       "340403                    Apple                   iPhone  ...   \n",
       "\n",
       "       hour_minute_fraction time_of_day  year quarter month day day_of_week  \\\n",
       "0                      6.48     Morning  2020       4    11  30      Monday   \n",
       "1                     10.95     Morning  2021       1     1  11      Monday   \n",
       "2                     10.07     Morning  2020       4    12  14      Monday   \n",
       "3                     23.08       Night  2020       4    12  11      Friday   \n",
       "4                      0.08       Night  2021       1     1  21    Thursday   \n",
       "...                     ...         ...   ...     ...   ...  ..         ...   \n",
       "340398                20.30     Evening  2020       4    12   2   Wednesday   \n",
       "340399                 2.72       Night  2020       4    12   5    Saturday   \n",
       "340401                14.70   Afternoon  2020       4    12   8     Tuesday   \n",
       "340402                12.93   Afternoon  2020       4    12  10    Thursday   \n",
       "340403                20.93     Evening  2020       4    12   4      Friday   \n",
       "\n",
       "       day_of_year week_number is_weekend  \n",
       "0              335          49          0  \n",
       "1               11           2          0  \n",
       "2              349          51          0  \n",
       "3              346          50          0  \n",
       "4               21           3          0  \n",
       "...            ...         ...        ...  \n",
       "340398         337          49          0  \n",
       "340399         340          49          1  \n",
       "340401         343          50          0  \n",
       "340402         345          50          0  \n",
       "340403         339          49          0  \n",
       "\n",
       "[310732 rows x 36 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['hour_minute_fraction'] = round(data['local_hour'] + data['local_minute'] / 60,2)  # Hour + fraction of minute\n",
    "\n",
    "# Categorize the time of day\n",
    "def categorize_time_of_day(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 21:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "data['time_of_day'] = data['local_hour'].apply(categorize_time_of_day)\n",
    "\n",
    "\n",
    "\n",
    "# create year, quarter, month, day number of week, weekend/weekday based on event_date column\n",
    "\n",
    "# Create new columns\n",
    "data['year'] = data['local_date'].dt.year\n",
    "data['quarter'] = data['local_date'].dt.quarter\n",
    "data['month'] = data['local_date'].dt.month\n",
    "data['day'] = data['local_date'].dt.day\n",
    "data['day_of_week'] = data['local_date'].dt.day_name()  \n",
    "data['day_of_year'] = data['local_date'].dt.dayofyear  # Day of the year\n",
    "data['week_number'] = data['local_date'].dt.isocalendar().week  # ISO week number\n",
    "\n",
    "data['day_of_week_nr'] = data['local_date'].dt.weekday  # Monday=0, Sunday=6\n",
    "data['is_weekend'] = data['day_of_week_nr'].apply(lambda x: 1 if x >= 5 else 0)  # 1 for weekend, 0 for weekday\n",
    "data.drop(['day_of_week_nr'],axis=1) # already symbolic. not needed\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# device columns\n",
    "\n",
    "- for many cases we assumed devices, brands and os versions of 2021 as top devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device mobile brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fill 'device_mobile_brand_name' with 'PC' where the conditions are met\n",
    "data.loc[(data['device_operating_system'] == 'Windows') & (data['device_category'] == 'desktop'), 'device_mobile_brand_name'] = 'PC'\n",
    "data.loc[(data['device_operating_system'] == 'Web') & (data['device_category'] == 'desktop') & ((data['device_mobile_model_name'].isin(['Chrome','Edge','Firefox']))), 'device_mobile_brand_name'] = 'PC'\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Microsoft') & (data['device_category'] == 'desktop'), 'device_mobile_brand_name'] = 'PC'\n",
    "\n",
    "\n",
    "# data['device_mobile_brand_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_mobile_model_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Samsung'), 'device_mobile_model_name'] = 'Galaxy S21'\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Xiaomi'), 'device_mobile_model_name'] = 'Mi 11'\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Huawei') & (data['device_category'] == 'mobile'), 'device_mobile_model_name'] = 'P50'\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Apple') & (data['device_category'] == 'desktop'), 'device_mobile_model_name'] = 'Macintosh'\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Apple') & (data['device_category'] == 'mobile'), 'device_mobile_model_name'] = 'iPhone'\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Apple') & (data['device_category'] == 'tablet'), 'device_mobile_model_name'] = 'iPad'\n",
    "\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'PC')& (data['device_mobile_model_name'] == 'Chrome'), 'device_mobile_model_name'] = 'PC'\n",
    "\n",
    "\n",
    "\n",
    "data['device_mobile_model_name'] = (\n",
    "    data['device_mobile_brand_name'] + \" - \" +\n",
    "    data['device_mobile_model_name'].astype(str).where(pd.notna(data['device_mobile_model_name']), '')\n",
    ")\n",
    "\n",
    "\n",
    "if test_data==True:\n",
    "    data['device_mobile_model_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_operating_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chrome OS - ensure 'desktop' is correctly spelled\n",
    "data.loc[(data['device_mobile_model_name'] == 'ChromeBook') & (data['device_category'] == 'desktop'), 'device_operating_system'] = 'ChromeOS'\n",
    "\n",
    "# iOS - for iPhone and iPad\n",
    "data.loc[data['device_mobile_model_name'].isin(['iPhone', 'iPad']) | (data['device_mobile_brand_name'] == 'Apple') | ((data['device_mobile_model_name'] == 'Apple') & (data['device_category'].isin(['mobile','tablet']))), 'device_operating_system'] = 'iOS'\n",
    "\n",
    "# Android - for specified brands\n",
    "android_brands = ['Xiaomi', 'Huawei', 'Samsung']\n",
    "data.loc[data['device_mobile_brand_name'].isin(android_brands), 'device_operating_system'] = 'Android'\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Google') & (data['device_category'].isin(['mobile','tablet'])), 'device_operating_system'] = 'Android'\n",
    "\n",
    "# macOS\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Apple') & (data['device_category'] == 'desktop'), 'device_operating_system'] = 'MacOS'\n",
    "\n",
    "# Windows\n",
    "data.loc[(data['device_operating_system'] == 'Web') & (data['device_category'] == 'desktop') & ((data['device_mobile_brand_name'] == 'PC')), 'device_operating_system'] = 'Windows'\n",
    "data.loc[(data['device_operating_system'] == 'Web') & (data['device_category'] == 'desktop') & ((data['device_mobile_brand_name'] == 'Mozilla')), 'device_operating_system'] = 'Windows'\n",
    "data.loc[(data['device_category'] == 'desktop') & ((data['device_mobile_brand_name'] == 'Microsoft')), 'device_operating_system'] = 'Windows'\n",
    "\n",
    "if test_data==True:\n",
    "    data.groupby(['device_category','device_operating_system', 'device_mobile_brand_name']).agg(\n",
    "        unique_event_count=('event_timestamp', 'nunique')\n",
    "    ).sort_values('unique_event_count',ascending=False).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_operating_system_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all string characters and keep float values\n",
    "data['device_operating_system_version'] = data['device_operating_system_version'].str.extract(r'(\\d+\\.\\d+|\\d+)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chrome os consider same browser version\n",
    "# https://chromereleases.googleblog.com/2021/\n",
    "data.loc[(data['device_operating_system'] == 'ChromeOS') & (data['device_operating_system_version'].isnull()), 'device_operating_system_version'] = data['device_web_info_browser_version']\n",
    "\n",
    "\n",
    "data['device_operating_system_version'] = (\n",
    "    data['device_operating_system'] + \" - \" +\n",
    "    data['device_operating_system_version'].astype(str).where(pd.notna(data['device_operating_system_version']), '')\n",
    ")\n",
    "\n",
    "\n",
    "if test_data==True:\n",
    "    data['device_operating_system_version'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data==True:\n",
    "    data['device_operating_system_version'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if test_data==True:\n",
    "    data.groupby(['geo_country','device_language']).agg(\n",
    "        unique_event_count=('event_timestamp', 'nunique')\n",
    "    ).sort_values('unique_event_count',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data==True:\n",
    "    data.groupby(['geo_country']).agg(\n",
    "        null_device_language_count=('device_language', lambda x: x.isna().sum())\n",
    "    ).sort_values('null_device_language_count',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data==True:\n",
    "    data['device_language'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_web_info_browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data['device_web_info_browser'] == 'Android Webview'), 'device_web_info_browser'] = \"Chrome\"\n",
    "\n",
    "if test_data==True:\n",
    "    data['device_web_info_browser'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_web_info_browser_version\n",
    "\n",
    "let us concatenate so that the values make sense and not mixed between browser versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data['device_web_info_browser_version'] = (\n",
    "    data['device_web_info_browser'] + ' - ' +\n",
    "    data['device_web_info_browser_version'].astype(str).where(pd.notna(data['device_web_info_browser_version']), '')\n",
    ")\n",
    "\n",
    "\n",
    "if test_data==True:\n",
    "    data['device_operating_system_version'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## session counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data==True:\n",
    "    data.groupby(['traffic_source_medium','traffic_source_source']).agg(\n",
    "    unique_session_count=('session_id', 'nunique')\n",
    "    ).sort_values('unique_session_count',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove parenthesis ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['traffic_source_source'] = data['traffic_source_source'].str.replace(r'\\(|\\)', '', regex=True).str.strip()\n",
    "data['traffic_source_medium'] = data['traffic_source_medium'].str.replace(r'\\(|\\)', '', regex=True).str.strip()\n",
    "\n",
    "if test_data==True:\n",
    "    data.groupby(['traffic_source_medium','traffic_source_source']).agg(\n",
    "        unique_session_count=('session_id', 'nunique')\n",
    "    ).sort_values('unique_session_count',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace values\n",
    "\n",
    "- bad referral naming from shop.googlemerchandisestore.com means badly tracked and we should consider direct traffic instead\n",
    "- medium =none is referring to direct traffic and we will use the same name to not confuse with null values\n",
    "- data deleted is most likely paid campaign by google to avoid confidential data exposure so we will replace with cpc / google as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source is merchandisestore.com then direct (bad parameter)\n",
    "data.loc[(data['traffic_source_source'] == 'shop.googlemerchandisestore.com') & (data['traffic_source_medium'] == 'referral'), ['traffic_source_medium', 'traffic_source_source']] = ['direct','direct']\n",
    "\n",
    "\n",
    "# referral traffic\n",
    "data.loc[(data['traffic_source_medium'] == 'referral'), ['traffic_source_medium', 'traffic_source_source']] = ['referral','referral_link']\n",
    "\n",
    "# google organic\n",
    "data.loc[(data['traffic_source_source'].isnull()) & (data['traffic_source_medium']== 'organic'),  ['traffic_source_medium', 'traffic_source_source']] = ['organic','google']\n",
    "\n",
    "\n",
    "# when we have direct traffic it is direct traffic\n",
    "data.loc[(data['traffic_source_source'] == 'direct'), 'traffic_source_medium'] = 'direct'\n",
    "\n",
    "# data deleted is paid campaign cpc by google\n",
    "data.loc[(data['traffic_source_source'] == 'data deleted') | (data['traffic_source_medium'] == 'data deleted'), ['traffic_source_medium', 'traffic_source_source']] = ['cpc', 'google']\n",
    "\n",
    "\n",
    "# full null values are direct\n",
    "data.loc[(data['traffic_source_source'].isnull()) & (data['traffic_source_medium'].isnull()),  ['traffic_source_medium', 'traffic_source_source']] = ['direct','direct']\n",
    "# data.loc[(data['traffic_source_source'].isnull()) & (data['traffic_source_medium'].isnull()), 'traffic_source_medium'] = 'direct'\n",
    "\n",
    "if test_data == True:\n",
    "    data.groupby(['traffic_source_source','traffic_source_medium']).agg(\n",
    "        unique_session_count=('session_id', 'nunique')\n",
    "    ).sort_values('unique_session_count',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pagelocation string work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove page unavailable\n",
    "data = data.loc[data['page_title'] != 'Page Unavailable']\n",
    "\n",
    "\n",
    "# lowercase everything to guarantee string match\n",
    "data['page_location'] = data['page_location'].str.lower()\n",
    "data['page_title'] = data['page_title'].str.lower()\n",
    "\n",
    "\n",
    "# domain readability\n",
    "data['page_location'] = data['page_location'].str.replace(r'shop.googlemerchandisestore.com/store.html', 'shop.googlemerchandisestore.com/').str.strip()\n",
    "data['page_location'] = data['page_location'].str.replace(r'+', ' ').str.strip()\n",
    "data['page_location'] = data['page_location'].str.replace(r'https://', '').str.strip()\n",
    "data['page_location'] = data['page_location'].str.replace(r'http://', '').str.strip()\n",
    "data['page_location'] = data['page_location'].str.replace(r'www.', '').str.strip()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove low percentile page record count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low percentile of event count: 7.0\n"
     ]
    }
   ],
   "source": [
    "df_pages_agg=data.groupby(['page_location']).agg(\n",
    "    total_events=('event_timestamp', 'count'),\n",
    ").sort_values('total_events',ascending=False).reset_index()\n",
    "\n",
    "\n",
    "# Calculate the percentile of event counts\n",
    "event_count_percentile = df_pages_agg['total_events'].quantile(0.30)\n",
    "\n",
    "print(f\"low percentile of event count: {event_count_percentile}\")\n",
    "\n",
    "# # Filter out page paths below the 10th percentile\n",
    "df_pages_agg_filtered = df_pages_agg[df_pages_agg['total_events'] >= event_count_percentile]\n",
    "\n",
    "\n",
    "\n",
    "data=data.merge(df_pages_agg_filtered[['page_location']],\n",
    "                on=['page_location'],\n",
    "                how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split categories in page paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split 'page_location' into 4 parts (max)\n",
    "split_columns = data['page_location'].str.split('/', n=4, expand=True)\n",
    "\n",
    "# Step 2: Assign the first three parts to new columns (ignore the first empty part if there is a leading '/')\n",
    "data['domain'] = split_columns[0] # url_domain \n",
    "data['page_path_level_1'] = split_columns[1].replace('', pd.NA)\n",
    "data['page_path_level_2'] = split_columns[2].replace('', pd.NA)\n",
    "data['page_path_level_3'] = split_columns[3].replace('', pd.NA)\n",
    "\n",
    "# Count the number of non-null levels in the path\n",
    "data['path_length'] = data[['page_path_level_1', 'page_path_level_2', 'page_path_level_3']].notna().sum(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_pages_total=data[['page_title','page_location','page_path_level_1','page_path_level_2','page_path_level_3','path_length']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill length page path with page title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['page_path_level_1'] = data.apply(\n",
    "    lambda row: row['page_title'] if pd.isna(row['page_path_level_2']) else row['page_path_level_1'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_pages_total=data[['page_title','page_location','page_path_level_1','page_path_level_2','page_path_level_3']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## page location long tail conversion to \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Aggregate events by 'page_location' and 'page_path_level_3'\n",
    "df_pages_agg = data.groupby(['page_location', 'page_path_level_3']).agg(\n",
    "    total_events=('event_timestamp', 'count'),\n",
    ").sort_values('total_events', ascending=False).reset_index()\n",
    "\n",
    "# Step 2: Calculate the desired percentile of total events to consider long tail\n",
    "event_count_percentile = df_pages_agg['total_events'].quantile(0.75)\n",
    "\n",
    "# Step 3: Create a mask for long tail pages\n",
    "long_tail_mask = df_pages_agg['total_events'] < event_count_percentile\n",
    "\n",
    "# Step 4: Create a dictionary to map original page_path_level_3 to \"other\"\n",
    "long_tail_mapping = dict(zip(df_pages_agg['page_path_level_3'], \n",
    "                             df_pages_agg['page_path_level_3'].where(~long_tail_mask, \"other\")))\n",
    "\n",
    "\n",
    "# Step 5: Replace the values directly in the original DataFrame\n",
    "# Only apply the mapping if path_length is 3\n",
    "data['page_path_level_3'] = data.apply(\n",
    "    lambda row: long_tail_mapping.get(row['page_path_level_3'], row['page_path_level_3']) if row['path_length'] == 3 else row['page_path_level_3'],\n",
    "    axis=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill the other page path levels with previous page path column\n",
    "\n",
    "This will allow for hierarchical encoding without sacrificing columns or rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill 'page_path_level_2' by concatenating 'page_path_1' and 'page_path_2' (if 'page_path_2' is null)\n",
    "data['page_path_level_2'] = data.apply(\n",
    "    lambda row: row['page_path_level_1'] if pd.isna(row['page_path_level_2']) \n",
    "    else f\"{row['page_path_level_1']}/{row['page_path_level_2']}\", axis=1\n",
    ")\n",
    "\n",
    "# Fill 'page_path_level_3' by concatenating 'page_path_level_2' and 'page_path_3' (if 'page_path_3' is null)\n",
    "data['page_path_level_3'] = data.apply(\n",
    "    lambda row: row['page_path_level_2'] if pd.isna(row['page_path_level_3']) \n",
    "    else f\"{row['page_path_level_2']}/{row['page_path_level_3']}\", axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final df without relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final=data.drop(['page_title','page_location','session_id','user_pseudo_id','event_timestamp','local_date','event_date'],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308430, 42)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.to_csv('data/df_merch_pre_proc.csv',index=False)\n",
    "\n",
    "# df_merch_pre_proc.csv\n",
    "# df_merch_profile.csv\n",
    "# df_merch_data_prep.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel File for encoding mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: property 'book' of 'OpenpyxlWriter' object has no setter\n",
      "The file might be corrupt or invalid. Creating a new file.\n",
      "Excel file has been updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "\n",
    "# Function to append distinct combinations of selected columns into sheets in an Excel file\n",
    "def append_columns_to_excel(df, columns_dict, output_file):\n",
    "    \"\"\"\n",
    "    Append distinct combinations of selected columns into separate sheets in an existing Excel file,\n",
    "    with the columns ordered by their names for easier hierarchical encoding.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The DataFrame containing the columns to save.\n",
    "    columns_dict (dict): Dictionary where keys are sheet names, and values are lists of column names to include.\n",
    "    output_file (str): The path of the Excel file to save the sheets.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check if the file exists and is a valid Excel file\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            # Try to load the existing workbook\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "                writer.book = load_workbook(output_file)\n",
    "                \n",
    "                # Loop over each sheet name and corresponding list of columns\n",
    "                for sheet_name, columns in columns_dict.items():\n",
    "                    # Check if all the specified columns exist in the DataFrame\n",
    "                    missing_columns = [col for col in columns if col not in df.columns]\n",
    "                    if missing_columns:\n",
    "                        print(f\"Warning: The following columns are not found in the DataFrame for sheet '{sheet_name}': {missing_columns}\")\n",
    "                        continue\n",
    "\n",
    "                    # Get distinct combinations of the selected columns\n",
    "                    distinct_values = df[columns].drop_duplicates().dropna(how='all')\n",
    "\n",
    "                    # Sort distinct values by the specified columns for hierarchical grouping\n",
    "                    distinct_values.sort_values(by=columns, inplace=True)\n",
    "\n",
    "                    # Write distinct values to a new sheet named after the sheet_name\n",
    "                    distinct_values.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"The file might be corrupt or invalid. Creating a new file.\")\n",
    "            # Create a new file if loading fails\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl', mode='w') as writer:\n",
    "                for sheet_name, columns in columns_dict.items():\n",
    "                    missing_columns = [col for col in columns if col not in df.columns]\n",
    "                    if missing_columns:\n",
    "                        print(f\"Warning: The following columns are not found in the DataFrame for sheet '{sheet_name}': {missing_columns}\")\n",
    "                        continue\n",
    "                    distinct_values = df[columns].drop_duplicates().dropna(how='all')\n",
    "                    distinct_values.sort_values(by=columns, inplace=True)  # Sort distinct values\n",
    "                    distinct_values.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    else:\n",
    "        # If the file does not exist, create a new one\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl', mode='w') as writer:\n",
    "            for sheet_name, columns in columns_dict.items():\n",
    "                missing_columns = [col for col in columns if col not in df.columns]\n",
    "                if missing_columns:\n",
    "                    print(f\"Warning: The following columns are not found in the DataFrame for sheet '{sheet_name}': {missing_columns}\")\n",
    "                    continue\n",
    "                distinct_values = df[columns].drop_duplicates().dropna(how='all')\n",
    "                distinct_values.sort_values(by=columns, inplace=True)  # Sort distinct values\n",
    "                distinct_values.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                \n",
    "\n",
    "# Define the groups of columns for hierarchical encoding, grouped by sheet name\n",
    "columns_to_save = {\n",
    "    'event_name' : ['event_name'],  \n",
    "    'device_category': ['device_category'],   \n",
    "    'device_mobile_brand_name': ['device_mobile_brand_name'],\n",
    "    'device_mobile_model_name': ['device_mobile_model_name'],\n",
    "    'device_language': ['device_language'],\n",
    "    'device_category': ['device_category'],      \n",
    "    'device_operating_system_version': ['device_operating_system_version'],   \n",
    "    'device_operating_system': ['device_operating_system'],   \n",
    "    'device_web_info_browser': ['device_web_info_browser'],   \n",
    "    'device_web_info_browser_version': ['device_web_info_browser_version'],\n",
    "    'geo_country': ['geo_country'],   \n",
    "    'traffic_source_medium':['traffic_source_medium'],\n",
    "    'traffic_source_source':['traffic_source_source'],\n",
    "    'page_path_level_1':['page_path_level_1'],\n",
    "    'page_path_level_2':['page_path_level_2'],\n",
    "    'page_path_level_3':['page_path_level_3'],\n",
    "    \n",
    "}\n",
    "\n",
    "# Save the distinct values combinations of each column group into corresponding sheets\n",
    "append_columns_to_excel(data_final, columns_to_save, f'df_merch_values_pre_encoding.xlsx')\n",
    "\n",
    "print(\"Excel file has been updated successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
