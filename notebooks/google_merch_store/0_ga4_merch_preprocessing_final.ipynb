{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duvidas\n",
    "\n",
    "1. Qual é o threshold da distribuiçao para podermos considerar um valor como \"long tail\" e podermos agrupar com um valor de \"other\"? \n",
    "    - no caso de geo_cities testou-se com percentil 60 e passámos a ter 100 e poucos valores distintos. \n",
    "    - ja na page_path_level_3 ao percentil 75 ainda sao perto de 200 valores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "filepath=r'data/google_merch_store_raw_merge_america.csv'\n",
    "\n",
    "\n",
    "\n",
    "file_tag = \"ga4_merch_store\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSLabs functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"scripts/dslabs_functions.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_functions lodaded\n"
     ]
    }
   ],
   "source": [
    "%run \"scripts/data_functions.py\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# north america filtering df creation\n",
    "\n",
    "- use if you only have full df version and not northern america filtered file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data=pd.read_csv(r'google_merch_store_raw_merge.csv')\n",
    "\n",
    "# data=data[data['geo_sub_continent']=='Northern America']\n",
    "\n",
    "# data.to_csv('google_merch_store_raw_merge_america.csv',index=False)\n",
    "\n",
    "# data.to_csv('google_merch_store_raw_merge_america_compressed.csv',index=False, compression='gzip')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 405866 entries, 0 to 405865\n",
      "Data columns (total 48 columns):\n",
      " #   Column                           Non-Null Count   Dtype              \n",
      "---  ------                           --------------   -----              \n",
      " 0   event_date                       405866 non-null  datetime64[ns]     \n",
      " 1   session_id                       405866 non-null  int64              \n",
      " 2   user_pseudo_id                   405866 non-null  float64            \n",
      " 3   event_name                       405866 non-null  object             \n",
      " 4   event_timestamp                  405866 non-null  datetime64[ns, UTC]\n",
      " 5   page_location                    405866 non-null  object             \n",
      " 6   page_title                       404763 non-null  object             \n",
      " 7   device_category                  405866 non-null  object             \n",
      " 8   device_mobile_brand_name         405866 non-null  object             \n",
      " 9   device_mobile_model_name         405866 non-null  object             \n",
      " 10  device_mobile_marketing_name     405866 non-null  object             \n",
      " 11  device_operating_system          405866 non-null  object             \n",
      " 12  device_operating_system_version  405866 non-null  object             \n",
      " 13  device_language                  222973 non-null  object             \n",
      " 14  device_is_limited_ad_tracking    405866 non-null  object             \n",
      " 15  device_web_info_browser          405866 non-null  object             \n",
      " 16  device_web_info_browser_version  405866 non-null  object             \n",
      " 17  geo_continent                    405866 non-null  object             \n",
      " 18  geo_country                      405866 non-null  object             \n",
      " 19  geo_region                       405866 non-null  object             \n",
      " 20  geo_city                         405866 non-null  object             \n",
      " 21  geo_sub_continent                405866 non-null  object             \n",
      " 22  geo_metro                        405866 non-null  object             \n",
      " 23  traffic_source_name              405866 non-null  object             \n",
      " 24  traffic_source_medium            405866 non-null  object             \n",
      " 25  traffic_source_source            405866 non-null  object             \n",
      " 26  page_referrer                    141548 non-null  object             \n",
      " 27  entrances                        17014 non-null   float64            \n",
      " 28  debug_mode                       374290 non-null  float64            \n",
      " 29  ga_session_number                405866 non-null  int64              \n",
      " 30  session_engaged                  374303 non-null  float64            \n",
      " 31  engagement_time_msec             308020 non-null  float64            \n",
      " 32  ecommerce_total_item_quantity    39578 non-null   float64            \n",
      " 33  ecommerce_purchase_revenue       850 non-null     float64            \n",
      " 34  ecommerce_shipping_value         0 non-null       float64            \n",
      " 35  ecommerce_tax_value              850 non-null     float64            \n",
      " 36  ecommerce_unique_items           209293 non-null  float64            \n",
      " 37  ecommerce_transaction_id         220365 non-null  object             \n",
      " 38  item_id                          209293 non-null  object             \n",
      " 39  item_name                        209293 non-null  object             \n",
      " 40  item_brand                       209151 non-null  object             \n",
      " 41  item_variant                     207925 non-null  object             \n",
      " 42  item_category                    208458 non-null  object             \n",
      " 43  price                            201544 non-null  float64            \n",
      " 44  quantity                         7901 non-null    float64            \n",
      " 45  item_revenue                     209293 non-null  float64            \n",
      " 46  item_list_index                  204142 non-null  float64            \n",
      " 47  promotion_name                   208431 non-null  object             \n",
      "dtypes: datetime64[ns, UTC](1), datetime64[ns](1), float64(14), int64(2), object(30)\n",
      "memory usage: 148.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# test_data=True\n",
    "test_data=False\n",
    "\n",
    "\n",
    "# Define a function to sample 10% from each group\n",
    "def sample_per_day(group, fraction=0.1):\n",
    "    return group.sample(frac=fraction)\n",
    "\n",
    "\n",
    "if test_data==True:\n",
    "\n",
    "    data=pd.read_csv(filepath)\n",
    "    \n",
    "\n",
    "    data['event_timestamp'] = pd.to_datetime(data['event_timestamp'], unit='us', utc=True)\n",
    "    data['event_date'] = pd.to_datetime(data['event_date'], infer_datetime_format=True)\n",
    " \n",
    "\n",
    "    # Apply the sampling to each group (grouped by event_date) 1%\n",
    "    data = data.groupby('event_date').apply(lambda x: sample_per_day(x, 0.01)).reset_index(drop=True)\n",
    "\n",
    "   \n",
    "\n",
    "else:\n",
    "    data=pd.read_csv(filepath)\n",
    "    \n",
    "\n",
    "    data['event_timestamp'] = pd.to_datetime(data['event_timestamp'], unit='us', utc=True)\n",
    "    data['event_date'] = pd.to_datetime(data['event_date'], infer_datetime_format=True)\n",
    "\n",
    "    # 10% sample\n",
    "    data = data.groupby('event_date').apply(lambda x: sample_per_day(x, 0.1)).reset_index(drop=True)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(405866, 48)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class target column creation\n",
    "\n",
    "\n",
    "- we want to classify if that hit is from a returning or new user.\n",
    "- Due to web analytics tracking particularities like cookie acceptance we prefer to consider returning users as users that are on their 3rd or larger session number\n",
    "- In this case, new user (ga_session_number <=2) will be 0 and returning user will be more than 2 (ga_session_number > 2)\n",
    "- session number column shall be removed afterwards as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data['returning_user'] = data['ga_session_number'].apply(lambda x: 0 if x <= 2 else 1)\n",
    "\n",
    "\n",
    "data=data.drop(['ga_session_number'],axis=1) # now we do not need it anymore. remove it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returning_user\n",
      "0    0.754266\n",
      "1    0.245734\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "target = \"returning_user\"\n",
    "\n",
    "values = data[target].value_counts(normalize=True) \n",
    "print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unbalance dataset\n",
    "\n",
    "let us umbalance to have 10% as returning users\n",
    "UPDATE: NOT NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "returning_user\n",
      "0    90.000147\n",
      "1     9.999853\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# # Separate the majority (0) and minority (1) classes\n",
    "# df_majority = data[data['returning_user'] == 0]\n",
    "# df_minority = data[data['returning_user'] == 1]\n",
    "\n",
    "# # Calculate the number of minority rows needed to make a 90/10 split\n",
    "# # Let total_rows be the total number of rows after undersampling\n",
    "# total_rows = len(df_majority) / 0.9  # 90% majority, 10% minority\n",
    "# desired_minority_count = int(total_rows * 0.1)  # 10% of the total should be minority\n",
    "\n",
    "# # Downsample the minority class to the desired number of rows\n",
    "# df_minority_downsampled = df_minority.sample(n=desired_minority_count, random_state=42)\n",
    "\n",
    "# # Combine the majority class with the downsampled minority class\n",
    "# df_imbalanced = pd.concat([df_majority, df_minority_downsampled])\n",
    "\n",
    "# # Shuffle the combined dataset\n",
    "# data = df_imbalanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# # Check the new class distribution to verify the 90/10 split\n",
    "# print(data['returning_user'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# column drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### low value or high null count columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['debug_mode','device_is_limited_ad_tracking','device_mobile_marketing_name','geo_metro','traffic_source_name','page_referrer','entrances'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ecommerce specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop([\n",
    " 'ecommerce_total_item_quantity', \n",
    " 'ecommerce_purchase_revenue',            \n",
    " 'ecommerce_shipping_value',              \n",
    " 'ecommerce_tax_value',                   \n",
    " 'ecommerce_unique_items',               \n",
    " 'ecommerce_transaction_id',              \n",
    " 'item_id',                               \n",
    " 'item_name',                             \n",
    " 'item_brand',                            \n",
    " 'item_variant',                          \n",
    " 'item_category',                         \n",
    " 'price',                                 \n",
    " 'quantity',                              \n",
    " 'item_revenue',                          \n",
    " 'item_list_index',                       \n",
    " 'promotion_name',\n",
    " 'geo_continent',\n",
    " 'geo_sub_continent'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_date</th>\n",
       "      <th>session_id</th>\n",
       "      <th>user_pseudo_id</th>\n",
       "      <th>event_name</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>page_location</th>\n",
       "      <th>page_title</th>\n",
       "      <th>device_category</th>\n",
       "      <th>device_mobile_brand_name</th>\n",
       "      <th>device_mobile_model_name</th>\n",
       "      <th>...</th>\n",
       "      <th>device_web_info_browser</th>\n",
       "      <th>device_web_info_browser_version</th>\n",
       "      <th>geo_country</th>\n",
       "      <th>geo_region</th>\n",
       "      <th>geo_city</th>\n",
       "      <th>traffic_source_medium</th>\n",
       "      <th>traffic_source_source</th>\n",
       "      <th>session_engaged</th>\n",
       "      <th>engagement_time_msec</th>\n",
       "      <th>returning_user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>340145</td>\n",
       "      <td>3.401450e+05</td>\n",
       "      <td>3.401450e+05</td>\n",
       "      <td>340145</td>\n",
       "      <td>340145</td>\n",
       "      <td>340145</td>\n",
       "      <td>339139</td>\n",
       "      <td>340145</td>\n",
       "      <td>340145</td>\n",
       "      <td>340145</td>\n",
       "      <td>...</td>\n",
       "      <td>340145</td>\n",
       "      <td>340145</td>\n",
       "      <td>340145</td>\n",
       "      <td>340145</td>\n",
       "      <td>340145</td>\n",
       "      <td>340145</td>\n",
       "      <td>340145</td>\n",
       "      <td>310361.000000</td>\n",
       "      <td>2.534880e+05</td>\n",
       "      <td>340145.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1032</td>\n",
       "      <td>478</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>325</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>view_item</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/</td>\n",
       "      <td>Home</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>...</td>\n",
       "      <td>Chrome</td>\n",
       "      <td>87.0</td>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>(not set)</td>\n",
       "      <td>organic</td>\n",
       "      <td>google</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>31278</td>\n",
       "      <td>55271</td>\n",
       "      <td>198100</td>\n",
       "      <td>144389</td>\n",
       "      <td>94121</td>\n",
       "      <td>...</td>\n",
       "      <td>231935</td>\n",
       "      <td>125505</td>\n",
       "      <td>291314</td>\n",
       "      <td>61399</td>\n",
       "      <td>146712</td>\n",
       "      <td>118910</td>\n",
       "      <td>124003</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2020-12-14 05:31:30.707786240</td>\n",
       "      <td>5.003594e+09</td>\n",
       "      <td>2.673831e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-12-14 17:31:52.658177280+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.907276</td>\n",
       "      <td>1.208460e+04</td>\n",
       "      <td>0.099999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2020-11-01 00:00:00</td>\n",
       "      <td>5.455500e+04</td>\n",
       "      <td>1.001303e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-01 00:00:04.579566+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2020-11-26 00:00:00</td>\n",
       "      <td>2.539015e+09</td>\n",
       "      <td>5.815923e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-11-26 05:10:53.949998080+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.690000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2020-12-11 00:00:00</td>\n",
       "      <td>5.004306e+09</td>\n",
       "      <td>2.233245e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-12-11 14:39:56.469096960+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.546000e+03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2021-01-04 00:00:00</td>\n",
       "      <td>7.514050e+09</td>\n",
       "      <td>6.131530e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-04 19:04:46.331791872+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.272500e+04</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2021-01-31 00:00:00</td>\n",
       "      <td>9.999568e+09</td>\n",
       "      <td>9.985207e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021-01-31 23:56:53.265848+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.659287e+06</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2.876847e+09</td>\n",
       "      <td>1.199583e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.290046</td>\n",
       "      <td>2.890860e+04</td>\n",
       "      <td>0.299998</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           event_date    session_id  user_pseudo_id  \\\n",
       "count                          340145  3.401450e+05    3.401450e+05   \n",
       "unique                            NaN           NaN             NaN   \n",
       "top                               NaN           NaN             NaN   \n",
       "freq                              NaN           NaN             NaN   \n",
       "mean    2020-12-14 05:31:30.707786240  5.003594e+09    2.673831e+08   \n",
       "min               2020-11-01 00:00:00  5.455500e+04    1.001303e+06   \n",
       "25%               2020-11-26 00:00:00  2.539015e+09    5.815923e+06   \n",
       "50%               2020-12-11 00:00:00  5.004306e+09    2.233245e+07   \n",
       "75%               2021-01-04 00:00:00  7.514050e+09    6.131530e+07   \n",
       "max               2021-01-31 00:00:00  9.999568e+09    9.985207e+09   \n",
       "std                               NaN  2.876847e+09    1.199583e+09   \n",
       "\n",
       "       event_name                      event_timestamp  \\\n",
       "count      340145                               340145   \n",
       "unique         17                                  NaN   \n",
       "top     view_item                                  NaN   \n",
       "freq       123691                                  NaN   \n",
       "mean          NaN  2020-12-14 17:31:52.658177280+00:00   \n",
       "min           NaN     2020-11-01 00:00:04.579566+00:00   \n",
       "25%           NaN  2020-11-26 05:10:53.949998080+00:00   \n",
       "50%           NaN  2020-12-11 14:39:56.469096960+00:00   \n",
       "75%           NaN  2021-01-04 19:04:46.331791872+00:00   \n",
       "max           NaN     2021-01-31 23:56:53.265848+00:00   \n",
       "std           NaN                                  NaN   \n",
       "\n",
       "                                   page_location page_title device_category  \\\n",
       "count                                     340145     339139          340145   \n",
       "unique                                      1032        478               3   \n",
       "top     https://shop.googlemerchandisestore.com/       Home         desktop   \n",
       "freq                                       31278      55271          198100   \n",
       "mean                                         NaN        NaN             NaN   \n",
       "min                                          NaN        NaN             NaN   \n",
       "25%                                          NaN        NaN             NaN   \n",
       "50%                                          NaN        NaN             NaN   \n",
       "75%                                          NaN        NaN             NaN   \n",
       "max                                          NaN        NaN             NaN   \n",
       "std                                          NaN        NaN             NaN   \n",
       "\n",
       "       device_mobile_brand_name device_mobile_model_name  ...  \\\n",
       "count                    340145                   340145  ...   \n",
       "unique                        8                       10  ...   \n",
       "top                       Apple                   Chrome  ...   \n",
       "freq                     144389                    94121  ...   \n",
       "mean                        NaN                      NaN  ...   \n",
       "min                         NaN                      NaN  ...   \n",
       "25%                         NaN                      NaN  ...   \n",
       "50%                         NaN                      NaN  ...   \n",
       "75%                         NaN                      NaN  ...   \n",
       "max                         NaN                      NaN  ...   \n",
       "std                         NaN                      NaN  ...   \n",
       "\n",
       "       device_web_info_browser device_web_info_browser_version    geo_country  \\\n",
       "count                   340145                          340145         340145   \n",
       "unique                       6                              13              2   \n",
       "top                     Chrome                            87.0  United States   \n",
       "freq                    231935                          125505         291314   \n",
       "mean                       NaN                             NaN            NaN   \n",
       "min                        NaN                             NaN            NaN   \n",
       "25%                        NaN                             NaN            NaN   \n",
       "50%                        NaN                             NaN            NaN   \n",
       "75%                        NaN                             NaN            NaN   \n",
       "max                        NaN                             NaN            NaN   \n",
       "std                        NaN                             NaN            NaN   \n",
       "\n",
       "        geo_region   geo_city traffic_source_medium traffic_source_source  \\\n",
       "count       340145     340145                340145                340145   \n",
       "unique          62        325                     6                     5   \n",
       "top     California  (not set)               organic                google   \n",
       "freq         61399     146712                118910                124003   \n",
       "mean           NaN        NaN                   NaN                   NaN   \n",
       "min            NaN        NaN                   NaN                   NaN   \n",
       "25%            NaN        NaN                   NaN                   NaN   \n",
       "50%            NaN        NaN                   NaN                   NaN   \n",
       "75%            NaN        NaN                   NaN                   NaN   \n",
       "max            NaN        NaN                   NaN                   NaN   \n",
       "std            NaN        NaN                   NaN                   NaN   \n",
       "\n",
       "       session_engaged engagement_time_msec returning_user  \n",
       "count    310361.000000         2.534880e+05  340145.000000  \n",
       "unique             NaN                  NaN            NaN  \n",
       "top                NaN                  NaN            NaN  \n",
       "freq               NaN                  NaN            NaN  \n",
       "mean          0.907276         1.208460e+04       0.099999  \n",
       "min           0.000000         1.000000e+00       0.000000  \n",
       "25%           1.000000         1.690000e+03       0.000000  \n",
       "50%           1.000000         5.546000e+03       0.000000  \n",
       "75%           1.000000         1.272500e+04       0.000000  \n",
       "max           1.000000         3.659287e+06       1.000000  \n",
       "std           0.290046         2.890860e+04       0.299998  \n",
       "\n",
       "[11 rows x 23 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary5 = data.describe(include=\"all\")\n",
    "\n",
    "summary5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# drop specific event_names\n",
    "\n",
    "We want to classify a user by its interactions with the website so we want to exclude some actions that may also be biased by incorrect ga4 tracking namely:\n",
    "- session_start\n",
    "- first_visit\n",
    "- click (low event count)\n",
    "- view_item_list (may not be triggered by user interaction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data==True:\n",
    "    data['event_name'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of values to drop\n",
    "events_to_drop = ['session_start', 'first_visit','click','view_item_list']\n",
    "\n",
    "# drop events from list\n",
    "data = data[~data['event_name'].isin(events_to_drop)]\n",
    "\n",
    "if test_data==True:\n",
    "    data['event_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replace (not set) with null\n",
    "\n",
    "we will handle these later but these are actually null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.replace('(not set)', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engagement time msec\n",
    "- https://support.google.com/analytics/answer/11109416?hl=en\n",
    "- is it null?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['engagement_time_msec'] = data['engagement_time_msec'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# geo columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## geo_region and geo_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['California',\n",
       " 'Florida',\n",
       " 'Texas',\n",
       " 'New Jersey',\n",
       " 'New York',\n",
       " 'Pennsylvania',\n",
       " 'Michigan',\n",
       " 'Ohio',\n",
       " 'Georgia',\n",
       " 'Illinois',\n",
       " 'Ontario',\n",
       " 'Massachusetts',\n",
       " 'North Carolina',\n",
       " 'Virginia',\n",
       " 'Maryland',\n",
       " 'Quebec',\n",
       " 'Connecticut',\n",
       " 'Missouri',\n",
       " 'Wisconsin',\n",
       " 'Washington',\n",
       " 'Indiana',\n",
       " 'Colorado',\n",
       " 'Minnesota',\n",
       " 'South Carolina',\n",
       " 'British Columbia',\n",
       " 'Utah',\n",
       " 'Tennessee',\n",
       " 'Alberta',\n",
       " 'Alabama',\n",
       " 'Louisiana',\n",
       " 'Oregon',\n",
       " 'Kentucky',\n",
       " 'Mississippi',\n",
       " 'Iowa',\n",
       " 'Arizona',\n",
       " 'Arkansas',\n",
       " 'Oklahoma',\n",
       " 'Kansas',\n",
       " 'New Hampshire',\n",
       " 'Delaware',\n",
       " 'West Virginia',\n",
       " 'Idaho',\n",
       " 'Manitoba',\n",
       " 'Rhode Island',\n",
       " 'Montana',\n",
       " 'Nova Scotia',\n",
       " 'Maine',\n",
       " 'Saskatchewan',\n",
       " 'New Mexico',\n",
       " 'North Dakota',\n",
       " 'Alaska',\n",
       " 'New Brunswick',\n",
       " 'Vermont',\n",
       " 'Nebraska',\n",
       " 'Nevada',\n",
       " 'Hawaii',\n",
       " 'Prince Edward Island',\n",
       " 'Newfoundland and Labrador',\n",
       " 'Wyoming',\n",
       " 'South Dakota']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_region_cities_df=data.groupby(['geo_region']).agg(\n",
    "    null_count=('geo_city', lambda x: x.isna().sum())\n",
    ").sort_values('null_count',ascending=False).reset_index()\n",
    "\n",
    "# will retrieve the most populated cities of these regions to use as fill method\n",
    "region_cities_with_nulls = null_region_cities_df[null_region_cities_df['null_count'] > 0]['geo_region'].tolist()\n",
    "\n",
    "\n",
    "region_cities_with_nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## geo mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_populated_cities = {\n",
    "    'California': 'Los Angeles',\n",
    "    'Florida': 'Jacksonville',\n",
    "    'Texas': 'Houston',\n",
    "    'New Jersey': 'Newark',\n",
    "    'New York': 'New York City',\n",
    "    'Pennsylvania': 'Philadelphia',\n",
    "    'Michigan': 'Detroit',\n",
    "    'Ohio': 'Columbus',\n",
    "    'Illinois': 'Chicago',\n",
    "    'Georgia': 'Atlanta',\n",
    "    'Massachusetts': 'Boston',\n",
    "    'North Carolina': 'Charlotte',\n",
    "    'Virginia': 'Virginia Beach',\n",
    "    'Maryland': 'Baltimore',\n",
    "    'Connecticut': 'Bridgeport',\n",
    "    'Indiana': 'Indianapolis',\n",
    "    'Wisconsin': 'Milwaukee',\n",
    "    'Missouri': 'Kansas City',\n",
    "    'Washington': 'Seattle',\n",
    "    'Colorado': 'Denver',\n",
    "    'Minnesota': 'Minneapolis',\n",
    "    'South Carolina': 'Charleston',\n",
    "    'Utah': 'Salt Lake City',\n",
    "    'Tennessee': 'Nashville',\n",
    "    'Alabama': 'Birmingham',\n",
    "    'Louisiana': 'New Orleans',\n",
    "    'Oregon': 'Portland',\n",
    "    'Kentucky': 'Louisville',\n",
    "    'Mississippi': 'Jackson',\n",
    "    'Iowa': 'Des Moines',\n",
    "    'Arkansas': 'Little Rock',\n",
    "    'Arizona': 'Phoenix',\n",
    "    'Oklahoma': 'Oklahoma City',\n",
    "    'Kansas': 'Wichita',\n",
    "    'Delaware': 'Wilmington',\n",
    "    'West Virginia': 'Charleston',\n",
    "    'New Hampshire': 'Manchester',\n",
    "    'Idaho': 'Boise',\n",
    "    'Montana': 'Billings',\n",
    "    'Maine': 'Portland',\n",
    "    'New Mexico': 'Albuquerque',\n",
    "    'North Dakota': 'Fargo',\n",
    "    'Rhode Island': 'Providence',\n",
    "    'Nebraska': 'Omaha',\n",
    "    'Alaska': 'Anchorage',\n",
    "    'Vermont': 'Burlington',\n",
    "    'Hawaii': 'Honolulu',\n",
    "    'Nevada': 'Las Vegas',\n",
    "    'Wyoming': 'Cheyenne',\n",
    "    'South Dakota': 'Sioux Falls'\n",
    "}\n",
    "\n",
    "data['geo_city'] = data['geo_city'].fillna(data['geo_region'].map(most_populated_cities))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## geo city long tail conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Aggregate events by 'page_location' and 'page_path_level_3'\n",
    "df_cities_agg = data.groupby(['geo_city']).agg(\n",
    "    total_events=('event_timestamp', 'count'),\n",
    ").sort_values('total_events', ascending=False).reset_index()\n",
    "\n",
    "# Step 2: Calculate the desired percentile of total events to consider long tail\n",
    "event_count_percentile = df_cities_agg['total_events'].quantile(0.65)\n",
    "\n",
    "# Step 3: Create a mask for long tail pages\n",
    "long_tail_mask = df_cities_agg['total_events'] < event_count_percentile\n",
    "\n",
    "# Step 4: Create a dictionary to map original page_path_level_3 to \"other\" (long tail)\n",
    "long_tail_mapping = dict(zip(df_cities_agg['geo_city'], \n",
    "                             df_cities_agg['geo_city'].where(~long_tail_mask, \"Other\")))\n",
    "\n",
    "\n",
    "# Step 5: Replace the values directly in the original DataFrame\n",
    "data['geo_city'] = data.apply(\n",
    "    lambda row: long_tail_mapping.get(row['geo_city'], row['geo_city']),\n",
    "    axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# event timestamp treatment\n",
    "\n",
    "- need to convert to local time from different us timezones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timezone from regions and create local_event_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tennessee', 'New Jersey', 'New York', 'Michigan', 'South Carolina', 'Rhode Island', 'Vermont', 'California', 'Quebec', 'Nova Scotia', 'Massachusetts', 'Idaho', 'Connecticut', 'Maine', 'Wyoming', 'Oklahoma', 'Illinois', 'Mississippi', 'Utah', 'Colorado', 'West Virginia', 'Arizona', 'Alaska', 'Montana', 'District of Columbia', 'British Columbia', 'Texas', 'Saskatchewan', 'Alabama', 'Maryland', 'Iowa', 'Nevada', 'Wisconsin', 'New Mexico', nan, 'Missouri', 'Alberta', 'South Dakota', 'Kentucky', 'Arkansas', 'Manitoba', 'Pennsylvania', 'Hawaii', 'Kansas', 'North Dakota', 'Ohio', 'Washington', 'Prince Edward Island', 'Newfoundland and Labrador', 'Louisiana', 'Ontario', 'Oregon', 'New Brunswick', 'Nebraska', 'Georgia', 'Delaware', 'Virginia', 'New Hampshire', 'Florida', 'North Carolina', 'Indiana', 'Minnesota']\n"
     ]
    }
   ],
   "source": [
    "geo_region_list = data['geo_region'].tolist()\n",
    "distinct_geo_regions = list(set(geo_region_list))\n",
    "\n",
    "print(distinct_geo_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### timezone mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytz\n",
    "\n",
    "\n",
    "city_timezone_mapping = {\n",
    "    'Kingston': 'America/Toronto',  # Canada (Eastern Time)\n",
    "    'Calgary': 'America/Edmonton',  # Canada (Mountain Time)\n",
    "    'Bethesda': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Nashville': 'America/Chicago',  # USA (Central Time)\n",
    "    'Saint Paul': 'America/Chicago',  # USA (Central Time)\n",
    "    'Berkeley': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Little Rock': 'America/Chicago',  # USA (Central Time)\n",
    "    'Sunnyvale': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Salinas': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Ann Arbor': 'America/Detroit',  # USA (Eastern Time)\n",
    "    'Auburn': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Germantown': 'America/Chicago',  # USA (Central Time)\n",
    "    'Fairfield': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Charlotte': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Guelph': 'America/Toronto',  # Canada (Eastern Time)\n",
    "    'Vancouver': 'America/Vancouver',  # Canada (Pacific Time)\n",
    "    'Stamford': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Pleasanton': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Cambridge': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Paramus': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Jacksonville': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Livermore': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Eau Claire': 'America/Chicago',  # USA (Central Time)\n",
    "    'Sacramento': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Los Altos': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Dartmouth': 'America/Halifax',  # Canada (Atlantic Time)\n",
    "    'Everett': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Modesto': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Saint John': 'America/Halifax',  # Canada (Atlantic Time)\n",
    "    'Lethbridge': 'America/Edmonton',  # Canada (Mountain Time)\n",
    "    'Kansas City': 'America/Chicago',  # USA (Central Time)\n",
    "    'Madison': 'America/Chicago',  # USA (Central Time)\n",
    "    'Evanston': 'America/Chicago',  # USA (Central Time)\n",
    "    'Fontana': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Hayward': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Franklin': 'America/Chicago',  # USA (Central Time)\n",
    "    'Somerville': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Chantilly': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Halifax': 'America/Halifax',  # Canada (Atlantic Time)\n",
    "    'Laredo': 'America/Chicago',  # USA (Central Time)\n",
    "    'Windsor': 'America/Toronto',  # Canada (Eastern Time)\n",
    "    'Richardson': 'America/Chicago',  # USA (Central Time)\n",
    "    'Oshawa': 'America/Toronto',  # Canada (Eastern Time)\n",
    "    'Denver': 'America/Denver',  # USA (Mountain Time)\n",
    "    'Cherry Hill': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Fredericton': 'America/Moncton',  # Canada (Atlantic Time)\n",
    "    'Houston': 'America/Chicago',  # USA (Central Time)\n",
    "    'Regina': 'America/Regina',  # Canada (Central Time)\n",
    "    'Buffalo': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Coffeyville': 'America/Chicago',  # USA (Central Time)\n",
    "    'Sugar Land': 'America/Chicago',  # USA (Central Time)\n",
    "    'Johns Creek': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Winston-Salem': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Fargo': 'America/Chicago',  # USA (Central Time)\n",
    "    'Granby': 'America/Toronto',  # Canada (Eastern Time)\n",
    "    'New Orleans': 'America/Chicago',  # USA (Central Time)\n",
    "    'Hartford': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Bridgeport': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Reno': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Raleigh': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Miami': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Chicago': 'America/Chicago',  # USA (Central Time)\n",
    "    'San Francisco': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Los Angeles': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Philadelphia': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Anchorage': 'America/Anchorage',  # USA (Alaska Time)\n",
    "    'Minneapolis': 'America/Chicago',  # USA (Central Time)\n",
    "    'New York': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Boston': 'America/New_York',  # USA (Eastern Time)\n",
    "    'Dallas': 'America/Chicago',  # USA (Central Time)\n",
    "    'Boulder': 'America/Denver',  # USA (Mountain Time)\n",
    "    'Seattle': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "    'Montgomery': 'America/Chicago',  # USA (Central Time)\n",
    "    'Phoenix': 'America/Phoenix',  # USA (Mountain Standard Time, no DST)\n",
    "    'Las Vegas': 'America/Los_Angeles',  # USA (Pacific Time)\n",
    "}\n",
    "\n",
    "\n",
    "region_timezone_dict = {\n",
    "    'Texas': 'America/Chicago',           # Central Time\n",
    "    'Nova Scotia': 'America/Halifax',     # Atlantic Time\n",
    "    'Wisconsin': 'America/Chicago',       # Central Time\n",
    "    'Massachusetts': 'America/New_York',  # Eastern Time\n",
    "    'Nebraska': 'America/Chicago',        # Central Time\n",
    "    'Wyoming': 'America/Denver',          # Mountain Time\n",
    "    'Kentucky': 'America/New_York',       # Eastern Time\n",
    "    'Maryland': 'America/New_York',       # Eastern Time\n",
    "    'Arkansas': 'America/Chicago',        # Central Time\n",
    "    'Saskatchewan': 'America/Regina',     # Central Time (no DST)\n",
    "    'South Dakota': 'America/Chicago',    # Central Time\n",
    "    'Ohio': 'America/New_York',           # Eastern Time\n",
    "    'Nevada': 'America/Los_Angeles',      # Pacific Time\n",
    "    'Kansas': 'America/Chicago',          # Central Time\n",
    "    'Virginia': 'America/New_York',       # Eastern Time\n",
    "    'Minnesota': 'America/Chicago',       # Central Time\n",
    "    'Washington': 'America/Los_Angeles',  # Pacific Time\n",
    "    'Maine': 'America/New_York',          # Eastern Time\n",
    "    'Vermont': 'America/New_York',        # Eastern Time\n",
    "    'Alaska': 'America/Anchorage',        # Alaska Time\n",
    "    'Manitoba': 'America/Winnipeg',       # Central Time\n",
    "    'Alabama': 'America/Chicago',         # Central Time\n",
    "    'Iowa': 'America/Chicago',            # Central Time\n",
    "    'Rhode Island': 'America/New_York',   # Eastern Time\n",
    "    'Missouri': 'America/Chicago',        # Central Time\n",
    "    'Hawaii': 'Pacific/Honolulu',         # Hawaii-Aleutian Time (no DST)\n",
    "    'Florida': 'America/New_York',        # Eastern Time (some parts Central)\n",
    "    'Michigan': 'America/Detroit',        # Eastern Time\n",
    "    'Tennessee': 'America/Chicago',       # Central Time (some parts Eastern)\n",
    "    'Pennsylvania': 'America/New_York',   # Eastern Time\n",
    "    'Delaware': 'America/New_York',       # Eastern Time\n",
    "    'Prince Edward Island': 'America/Halifax',  # Atlantic Time\n",
    "    'District of Columbia': 'America/New_York', # Eastern Time\n",
    "    'North Dakota': 'America/Chicago',    # Central Time\n",
    "    'Oklahoma': 'America/Chicago',        # Central Time\n",
    "    'New Brunswick': 'America/Halifax',   # Atlantic Time\n",
    "    'Ontario': 'America/Toronto',         # Eastern Time\n",
    "    'Quebec': 'America/Toronto',          # Eastern Time\n",
    "    'Idaho': 'America/Boise',             # Mountain Time\n",
    "    'Indiana': 'America/Indiana/Indianapolis',  # Eastern Time\n",
    "    'New York': 'America/New_York',       # Eastern Time\n",
    "    'Mississippi': 'America/Chicago',     # Central Time\n",
    "    'Georgia': 'America/New_York',        # Eastern Time\n",
    "    'Illinois': 'America/Chicago',        # Central Time\n",
    "    'Louisiana': 'America/Chicago',       # Central Time\n",
    "    'New Jersey': 'America/New_York',     # Eastern Time\n",
    "    'West Virginia': 'America/New_York',  # Eastern Time\n",
    "    'South Carolina': 'America/New_York', # Eastern Time\n",
    "    'Alberta': 'America/Edmonton',        # Mountain Time\n",
    "    'Arizona': 'America/Phoenix',         # Mountain Time (no DST)\n",
    "    'North Carolina': 'America/New_York', # Eastern Time\n",
    "    'Newfoundland and Labrador': 'America/St_Johns',  # Newfoundland Time\n",
    "    'California': 'America/Los_Angeles',  # Pacific Time\n",
    "    'Utah': 'America/Denver',             # Mountain Time\n",
    "    'New Hampshire': 'America/New_York',  # Eastern Time\n",
    "    'Connecticut': 'America/New_York',    # Eastern Time\n",
    "    'Montana': 'America/Denver',          # Mountain Time\n",
    "    'British Columbia': 'America/Vancouver',  # Pacific Time\n",
    "    'Colorado': 'America/Denver',         # Mountain Time\n",
    "    'New Mexico': 'America/Denver',       # Mountain Time\n",
    "    'Oregon': 'America/Los_Angeles'       # Pacific Time\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timezone conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to convert the UTC datetime to local time and extract date, hour, and minute\n",
    "def extract_local_date_hour_minute(row, region_timezone_dict):\n",
    "    # Check if the row contains a valid timestamp\n",
    "    if pd.isnull(row['event_timestamp']):\n",
    "        return pd.NaT, pd.NaT, pd.NaT  # Return NaT for all if the timestamp is null\n",
    "\n",
    "    # Get the region and corresponding timezone\n",
    "    region = row['geo_region']\n",
    "    tz = region_timezone_dict.get(region, None)  # Fetch timezone, fallback to None\n",
    "    \n",
    "    # If a valid timezone is found, apply timezone conversion\n",
    "    if tz:\n",
    "        try:\n",
    "            local_time = row['event_timestamp'].tz_convert(tz)  # Convert to local time\n",
    "            \n",
    "            local_date = local_time.date()  # Extract local date\n",
    "            local_hour = local_time.hour  # Extract local hour\n",
    "            local_minute = local_time.minute  # Extract local minute\n",
    "            \n",
    "            return local_date, local_hour, local_minute\n",
    "        except Exception as e:\n",
    "            print(f\"Error converting to timezone {tz}: {e}\")\n",
    "            return pd.NaT, pd.NaT, pd.NaT\n",
    "    else:\n",
    "        # Fallback to UTC timestamp\n",
    "        return row['event_timestamp'].date(), row['event_timestamp'].hour, row['event_timestamp'].minute\n",
    "\n",
    "# Apply the conversion function to the DataFrame\n",
    "data[['local_date', 'local_hour', 'local_minute']] = data.apply(\n",
    "    lambda row: extract_local_date_hour_minute(row, region_timezone_dict), axis=1, result_type='expand'\n",
    ")\n",
    "\n",
    "# Convert local_date to datetime format\n",
    "data['local_date'] = pd.to_datetime(data['local_date'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### additional date columns creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_date</th>\n",
       "      <th>session_id</th>\n",
       "      <th>user_pseudo_id</th>\n",
       "      <th>event_name</th>\n",
       "      <th>event_timestamp</th>\n",
       "      <th>page_location</th>\n",
       "      <th>page_title</th>\n",
       "      <th>device_category</th>\n",
       "      <th>device_mobile_brand_name</th>\n",
       "      <th>device_mobile_model_name</th>\n",
       "      <th>...</th>\n",
       "      <th>hour_minute_fraction</th>\n",
       "      <th>time_of_day</th>\n",
       "      <th>year</th>\n",
       "      <th>quarter</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>week_number</th>\n",
       "      <th>is_weekend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-11-25</td>\n",
       "      <td>9917649834</td>\n",
       "      <td>5.160882e+07</td>\n",
       "      <td>page_view</td>\n",
       "      <td>2020-11-25 06:46:46.920242+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/Google...</td>\n",
       "      <td>New | Google Merchandise Store</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Apple</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>...</td>\n",
       "      <td>1.77</td>\n",
       "      <td>Night</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>330</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-11-18</td>\n",
       "      <td>4591484845</td>\n",
       "      <td>4.313977e+07</td>\n",
       "      <td>view_item</td>\n",
       "      <td>2020-11-18 17:43:24.803054+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/Google...</td>\n",
       "      <td>Kids | Apparel | Google Merchandise Store</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Apple</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>...</td>\n",
       "      <td>12.72</td>\n",
       "      <td>Afternoon</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>323</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-12-04</td>\n",
       "      <td>1078908601</td>\n",
       "      <td>6.044075e+07</td>\n",
       "      <td>scroll</td>\n",
       "      <td>2020-12-04 02:54:53.967285+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/Google...</td>\n",
       "      <td>Google Men's Softshell Moss</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Mozilla</td>\n",
       "      <td>Firefox</td>\n",
       "      <td>...</td>\n",
       "      <td>21.90</td>\n",
       "      <td>Night</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>Thursday</td>\n",
       "      <td>338</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-12-03</td>\n",
       "      <td>2627211832</td>\n",
       "      <td>6.822684e+06</td>\n",
       "      <td>view_item</td>\n",
       "      <td>2020-12-03 03:15:37.461563+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/Google...</td>\n",
       "      <td>Men's / Unisex | Apparel | Google Merchandise ...</td>\n",
       "      <td>desktop</td>\n",
       "      <td>Google</td>\n",
       "      <td>ChromeBook</td>\n",
       "      <td>...</td>\n",
       "      <td>19.25</td>\n",
       "      <td>Evening</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>337</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-12-15</td>\n",
       "      <td>5511448088</td>\n",
       "      <td>3.736575e+07</td>\n",
       "      <td>add_to_cart</td>\n",
       "      <td>2020-12-15 10:36:09.110030+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/Google...</td>\n",
       "      <td>Small Goods | Lifestyle | Google Merchandise S...</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Samsung</td>\n",
       "      <td>&lt;Other&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>2.60</td>\n",
       "      <td>Night</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>350</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340140</th>\n",
       "      <td>2020-12-05</td>\n",
       "      <td>321756288</td>\n",
       "      <td>2.400522e+07</td>\n",
       "      <td>page_view</td>\n",
       "      <td>2020-12-05 03:33:08.859454+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/Google...</td>\n",
       "      <td>Drinkware | Lifestyle | Google Merchandise Store</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Huawei</td>\n",
       "      <td>&lt;Other&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>22.55</td>\n",
       "      <td>Night</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>Friday</td>\n",
       "      <td>339</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340141</th>\n",
       "      <td>2021-01-15</td>\n",
       "      <td>4306560611</td>\n",
       "      <td>2.997664e+06</td>\n",
       "      <td>page_view</td>\n",
       "      <td>2021-01-15 10:51:49.855156+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/</td>\n",
       "      <td>Home</td>\n",
       "      <td>mobile</td>\n",
       "      <td>&lt;Other&gt;</td>\n",
       "      <td>&lt;Other&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td>5.85</td>\n",
       "      <td>Morning</td>\n",
       "      <td>2021</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>Friday</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340142</th>\n",
       "      <td>2020-12-08</td>\n",
       "      <td>5679840314</td>\n",
       "      <td>6.825749e+06</td>\n",
       "      <td>page_view</td>\n",
       "      <td>2020-12-08 10:50:48.590289+00:00</td>\n",
       "      <td>https://www.googlemerchandisestore.com/</td>\n",
       "      <td>Google Online Store</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Apple</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>...</td>\n",
       "      <td>2.83</td>\n",
       "      <td>Night</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>Tuesday</td>\n",
       "      <td>343</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340143</th>\n",
       "      <td>2020-12-10</td>\n",
       "      <td>3563782843</td>\n",
       "      <td>1.124405e+07</td>\n",
       "      <td>scroll</td>\n",
       "      <td>2020-12-10 03:06:49.905906+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/store....</td>\n",
       "      <td>Home</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Apple</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>...</td>\n",
       "      <td>22.10</td>\n",
       "      <td>Night</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>Wednesday</td>\n",
       "      <td>344</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340144</th>\n",
       "      <td>2020-12-05</td>\n",
       "      <td>6417630759</td>\n",
       "      <td>4.278068e+06</td>\n",
       "      <td>page_view</td>\n",
       "      <td>2020-12-05 12:29:57.966833+00:00</td>\n",
       "      <td>https://shop.googlemerchandisestore.com/</td>\n",
       "      <td>Home</td>\n",
       "      <td>mobile</td>\n",
       "      <td>Apple</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>...</td>\n",
       "      <td>7.48</td>\n",
       "      <td>Morning</td>\n",
       "      <td>2020</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>340</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>310305 rows × 36 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       event_date  session_id  user_pseudo_id   event_name  \\\n",
       "0      2020-11-25  9917649834    5.160882e+07    page_view   \n",
       "1      2020-11-18  4591484845    4.313977e+07    view_item   \n",
       "2      2020-12-04  1078908601    6.044075e+07       scroll   \n",
       "3      2020-12-03  2627211832    6.822684e+06    view_item   \n",
       "4      2020-12-15  5511448088    3.736575e+07  add_to_cart   \n",
       "...           ...         ...             ...          ...   \n",
       "340140 2020-12-05   321756288    2.400522e+07    page_view   \n",
       "340141 2021-01-15  4306560611    2.997664e+06    page_view   \n",
       "340142 2020-12-08  5679840314    6.825749e+06    page_view   \n",
       "340143 2020-12-10  3563782843    1.124405e+07       scroll   \n",
       "340144 2020-12-05  6417630759    4.278068e+06    page_view   \n",
       "\n",
       "                        event_timestamp  \\\n",
       "0      2020-11-25 06:46:46.920242+00:00   \n",
       "1      2020-11-18 17:43:24.803054+00:00   \n",
       "2      2020-12-04 02:54:53.967285+00:00   \n",
       "3      2020-12-03 03:15:37.461563+00:00   \n",
       "4      2020-12-15 10:36:09.110030+00:00   \n",
       "...                                 ...   \n",
       "340140 2020-12-05 03:33:08.859454+00:00   \n",
       "340141 2021-01-15 10:51:49.855156+00:00   \n",
       "340142 2020-12-08 10:50:48.590289+00:00   \n",
       "340143 2020-12-10 03:06:49.905906+00:00   \n",
       "340144 2020-12-05 12:29:57.966833+00:00   \n",
       "\n",
       "                                            page_location  \\\n",
       "0       https://shop.googlemerchandisestore.com/Google...   \n",
       "1       https://shop.googlemerchandisestore.com/Google...   \n",
       "2       https://shop.googlemerchandisestore.com/Google...   \n",
       "3       https://shop.googlemerchandisestore.com/Google...   \n",
       "4       https://shop.googlemerchandisestore.com/Google...   \n",
       "...                                                   ...   \n",
       "340140  https://shop.googlemerchandisestore.com/Google...   \n",
       "340141           https://shop.googlemerchandisestore.com/   \n",
       "340142            https://www.googlemerchandisestore.com/   \n",
       "340143  https://shop.googlemerchandisestore.com/store....   \n",
       "340144           https://shop.googlemerchandisestore.com/   \n",
       "\n",
       "                                               page_title device_category  \\\n",
       "0                          New | Google Merchandise Store          mobile   \n",
       "1               Kids | Apparel | Google Merchandise Store          mobile   \n",
       "2                             Google Men's Softshell Moss         desktop   \n",
       "3       Men's / Unisex | Apparel | Google Merchandise ...         desktop   \n",
       "4       Small Goods | Lifestyle | Google Merchandise S...          mobile   \n",
       "...                                                   ...             ...   \n",
       "340140   Drinkware | Lifestyle | Google Merchandise Store          mobile   \n",
       "340141                                               Home          mobile   \n",
       "340142                                Google Online Store          mobile   \n",
       "340143                                               Home          mobile   \n",
       "340144                                               Home          mobile   \n",
       "\n",
       "       device_mobile_brand_name device_mobile_model_name  ...  \\\n",
       "0                         Apple                   iPhone  ...   \n",
       "1                         Apple                   iPhone  ...   \n",
       "2                       Mozilla                  Firefox  ...   \n",
       "3                        Google               ChromeBook  ...   \n",
       "4                       Samsung                  <Other>  ...   \n",
       "...                         ...                      ...  ...   \n",
       "340140                   Huawei                  <Other>  ...   \n",
       "340141                  <Other>                  <Other>  ...   \n",
       "340142                    Apple                   iPhone  ...   \n",
       "340143                    Apple                   iPhone  ...   \n",
       "340144                    Apple                   iPhone  ...   \n",
       "\n",
       "       hour_minute_fraction time_of_day  year quarter month day day_of_week  \\\n",
       "0                      1.77       Night  2020       4    11  25   Wednesday   \n",
       "1                     12.72   Afternoon  2020       4    11  18   Wednesday   \n",
       "2                     21.90       Night  2020       4    12   3    Thursday   \n",
       "3                     19.25     Evening  2020       4    12   2   Wednesday   \n",
       "4                      2.60       Night  2020       4    12  15     Tuesday   \n",
       "...                     ...         ...   ...     ...   ...  ..         ...   \n",
       "340140                22.55       Night  2020       4    12   4      Friday   \n",
       "340141                 5.85     Morning  2021       1     1  15      Friday   \n",
       "340142                 2.83       Night  2020       4    12   8     Tuesday   \n",
       "340143                22.10       Night  2020       4    12   9   Wednesday   \n",
       "340144                 7.48     Morning  2020       4    12   5    Saturday   \n",
       "\n",
       "       day_of_year week_number is_weekend  \n",
       "0              330          48          0  \n",
       "1              323          47          0  \n",
       "2              338          49          0  \n",
       "3              337          49          0  \n",
       "4              350          51          0  \n",
       "...            ...         ...        ...  \n",
       "340140         339          49          0  \n",
       "340141          15           2          0  \n",
       "340142         343          50          0  \n",
       "340143         344          50          0  \n",
       "340144         340          49          1  \n",
       "\n",
       "[310305 rows x 36 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['hour_minute_fraction'] = round(data['local_hour'] + data['local_minute'] / 60,2)  # Hour + fraction of minute\n",
    "\n",
    "# Categorize the time of day\n",
    "def categorize_time_of_day(hour):\n",
    "    if 5 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 17:\n",
    "        return 'Afternoon'\n",
    "    elif 17 <= hour < 22:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "data['time_of_day'] = data['local_hour'].apply(categorize_time_of_day)\n",
    "\n",
    "\n",
    "\n",
    "# create year, quarter, month, day number of week, weekend/weekday based on event_date column\n",
    "\n",
    "# Create new columns\n",
    "data['year'] = data['local_date'].dt.year\n",
    "data['quarter'] = data['local_date'].dt.quarter\n",
    "data['month'] = data['local_date'].dt.month\n",
    "data['day'] = data['local_date'].dt.day\n",
    "data['day_of_week'] = data['local_date'].dt.day_name()  \n",
    "data['day_of_year'] = data['local_date'].dt.dayofyear  # Day of the year\n",
    "data['week_number'] = data['local_date'].dt.isocalendar().week  # ISO week number\n",
    "\n",
    "# Assuming 'local_date' is in datetime format, otherwise you can parse it using pd.to_datetime\n",
    "def week_of_month(dt):\n",
    "    first_day = dt.replace(day=1)\n",
    "    # Calculate the week of the month by comparing the current date to the first day of the month\n",
    "    return (dt.day + first_day.weekday()) // 7 + 1\n",
    "\n",
    "# Apply this function to your 'local_date' column\n",
    "data['week_of_month'] = data['local_date'].apply(week_of_month)\n",
    "\n",
    "\n",
    "\n",
    "data['day_of_week_nr'] = data['local_date'].dt.weekday  # Monday=0, Sunday=6\n",
    "data['is_weekend'] = data['day_of_week_nr'].apply(lambda x: 1 if x >= 5 else 0)  # 1 for weekend, 0 for weekday\n",
    "data.drop(['day_of_week_nr'],axis=1) # already symbolic. not needed\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# device columns\n",
    "\n",
    "- for many cases we assumed devices, brands and os versions of 2021 as top devices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device mobile brand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fill 'device_mobile_brand_name' with 'PC' where the conditions are met\n",
    "data.loc[(data['device_operating_system'] == 'Windows') & (data['device_category'] == 'desktop'), 'device_mobile_brand_name'] = 'PC'\n",
    "data.loc[(data['device_operating_system'] == 'Web') & (data['device_category'] == 'desktop') & ((data['device_mobile_model_name'].isin(['Chrome','Edge','Firefox']))), 'device_mobile_brand_name'] = 'PC'\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Microsoft') & (data['device_category'] == 'desktop'), 'device_mobile_brand_name'] = 'PC'\n",
    "\n",
    "\n",
    "# data['device_mobile_brand_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_mobile_model_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Samsung'), 'device_mobile_model_name'] = 'Galaxy S21'\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Xiaomi'), 'device_mobile_model_name'] = 'Mi 11'\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Huawei') & (data['device_category'] == 'mobile'), 'device_mobile_model_name'] = 'P50'\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Apple') & (data['device_category'] == 'desktop'), 'device_mobile_model_name'] = 'Macintosh'\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Apple') & (data['device_category'] == 'mobile'), 'device_mobile_model_name'] = 'iPhone'\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Apple') & (data['device_category'] == 'tablet'), 'device_mobile_model_name'] = 'iPad'\n",
    "\n",
    "\n",
    "data.loc[(data['device_mobile_brand_name'] == 'PC')& (data['device_mobile_model_name'] == 'Chrome'), 'device_mobile_model_name'] = 'PC'\n",
    "\n",
    "\n",
    "\n",
    "data['device_mobile_model_name'] = (\n",
    "    data['device_mobile_brand_name'] + \" - \" +\n",
    "    data['device_mobile_model_name'].astype(str).where(pd.notna(data['device_mobile_model_name']), '')\n",
    ")\n",
    "\n",
    "\n",
    "if test_data==True:\n",
    "    data['device_mobile_model_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_operating_system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Chrome OS - ensure 'desktop' is correctly spelled\n",
    "data.loc[(data['device_mobile_model_name'] == 'ChromeBook') & (data['device_category'] == 'desktop'), 'device_operating_system'] = 'ChromeOS'\n",
    "\n",
    "# iOS - for iPhone and iPad\n",
    "data.loc[data['device_mobile_model_name'].isin(['iPhone', 'iPad']) | (data['device_mobile_brand_name'] == 'Apple') | ((data['device_mobile_model_name'] == 'Apple') & (data['device_category'].isin(['mobile','tablet']))), 'device_operating_system'] = 'iOS'\n",
    "\n",
    "# Android - for specified brands\n",
    "android_brands = ['Xiaomi', 'Huawei', 'Samsung']\n",
    "data.loc[data['device_mobile_brand_name'].isin(android_brands), 'device_operating_system'] = 'Android'\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Google') & (data['device_category'].isin(['mobile','tablet'])), 'device_operating_system'] = 'Android'\n",
    "\n",
    "# macOS\n",
    "data.loc[(data['device_mobile_brand_name'] == 'Apple') & (data['device_category'] == 'desktop'), 'device_operating_system'] = 'MacOS'\n",
    "\n",
    "# Windows\n",
    "data.loc[(data['device_operating_system'] == 'Web') & (data['device_category'] == 'desktop') & ((data['device_mobile_brand_name'] == 'PC')), 'device_operating_system'] = 'Windows'\n",
    "data.loc[(data['device_operating_system'] == 'Web') & (data['device_category'] == 'desktop') & ((data['device_mobile_brand_name'] == 'Mozilla')), 'device_operating_system'] = 'Windows'\n",
    "data.loc[(data['device_category'] == 'desktop') & ((data['device_mobile_brand_name'] == 'Microsoft')), 'device_operating_system'] = 'Windows'\n",
    "\n",
    "if test_data==True:\n",
    "    data.groupby(['device_category','device_operating_system', 'device_mobile_brand_name']).agg(\n",
    "        unique_event_count=('event_timestamp', 'nunique')\n",
    "    ).sort_values('unique_event_count',ascending=False).reset_index()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_operating_system_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace all string characters and keep float values\n",
    "data['device_operating_system_version'] = data['device_operating_system_version'].str.extract(r'(\\d+\\.\\d+|\\d+)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for chrome os consider same browser version\n",
    "# https://chromereleases.googleblog.com/2021/\n",
    "data.loc[(data['device_operating_system'] == 'ChromeOS') & (data['device_operating_system_version'].isnull()), 'device_operating_system_version'] = data['device_web_info_browser_version']\n",
    "\n",
    "\n",
    "data['device_operating_system_version'] = (\n",
    "    data['device_operating_system'] + \" - \" +\n",
    "    data['device_operating_system_version'].astype(str).where(pd.notna(data['device_operating_system_version']), '')\n",
    ")\n",
    "\n",
    "\n",
    "if test_data==True:\n",
    "    data['device_operating_system_version'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data==True:\n",
    "    data['device_operating_system_version'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if test_data==True:\n",
    "    data.groupby(['geo_country','device_language']).agg(\n",
    "        unique_event_count=('event_timestamp', 'nunique')\n",
    "    ).sort_values('unique_event_count',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data==True:\n",
    "    data.groupby(['geo_country']).agg(\n",
    "        null_device_language_count=('device_language', lambda x: x.isna().sum())\n",
    "    ).sort_values('null_device_language_count',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data==True:\n",
    "    data['device_language'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_web_info_browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[(data['device_web_info_browser'] == 'Android Webview'), 'device_web_info_browser'] = \"Chrome\"\n",
    "\n",
    "if test_data==True:\n",
    "    data['device_web_info_browser'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## device_web_info_browser_version\n",
    "\n",
    "let us concatenate so that the values make sense and not mixed between browser versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data['device_web_info_browser_version'] = (\n",
    "    data['device_web_info_browser'] + ' - ' +\n",
    "    data['device_web_info_browser_version'].astype(str).where(pd.notna(data['device_web_info_browser_version']), '')\n",
    ")\n",
    "\n",
    "\n",
    "if test_data==True:\n",
    "    data['device_operating_system_version'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traffic Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## session counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_data==True:\n",
    "    data.groupby(['traffic_source_medium','traffic_source_source']).agg(\n",
    "    unique_session_count=('session_id', 'nunique')\n",
    "    ).sort_values('unique_session_count',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove parenthesis ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['traffic_source_source'] = data['traffic_source_source'].str.replace(r'\\(|\\)', '', regex=True).str.strip()\n",
    "data['traffic_source_medium'] = data['traffic_source_medium'].str.replace(r'\\(|\\)', '', regex=True).str.strip()\n",
    "\n",
    "if test_data==True:\n",
    "    data.groupby(['traffic_source_medium','traffic_source_source']).agg(\n",
    "        unique_session_count=('session_id', 'nunique')\n",
    "    ).sort_values('unique_session_count',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace values\n",
    "\n",
    "- bad referral naming from shop.googlemerchandisestore.com means badly tracked and we should consider direct traffic instead\n",
    "- medium =none is referring to direct traffic and we will use the same name to not confuse with null values\n",
    "- data deleted is most likely paid campaign by google to avoid confidential data exposure so we will replace with cpc / google as well\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source is merchandisestore.com then direct (bad parameter)\n",
    "data.loc[(data['traffic_source_source'] == 'shop.googlemerchandisestore.com') & (data['traffic_source_medium'] == 'referral'), ['traffic_source_medium', 'traffic_source_source']] = ['direct','direct']\n",
    "\n",
    "\n",
    "# referral traffic\n",
    "data.loc[(data['traffic_source_medium'] == 'referral'), ['traffic_source_medium', 'traffic_source_source']] = ['referral','referral_link']\n",
    "\n",
    "# google organic\n",
    "data.loc[(data['traffic_source_source'].isnull()) & (data['traffic_source_medium']== 'organic'),  ['traffic_source_medium', 'traffic_source_source']] = ['organic','google']\n",
    "\n",
    "\n",
    "# when we have direct traffic it is direct traffic\n",
    "data.loc[(data['traffic_source_source'] == 'direct'), 'traffic_source_medium'] = 'direct'\n",
    "\n",
    "# data deleted is paid campaign cpc by google\n",
    "data.loc[(data['traffic_source_source'] == 'data deleted') | (data['traffic_source_medium'] == 'data deleted'), ['traffic_source_medium', 'traffic_source_source']] = ['cpc', 'google']\n",
    "\n",
    "\n",
    "# full null values are direct\n",
    "data.loc[(data['traffic_source_source'].isnull()) & (data['traffic_source_medium'].isnull()),  ['traffic_source_medium', 'traffic_source_source']] = ['direct','direct']\n",
    "# data.loc[(data['traffic_source_source'].isnull()) & (data['traffic_source_medium'].isnull()), 'traffic_source_medium'] = 'direct'\n",
    "\n",
    "if test_data == True:\n",
    "    data.groupby(['traffic_source_source','traffic_source_medium']).agg(\n",
    "        unique_session_count=('session_id', 'nunique')\n",
    "    ).sort_values('unique_session_count',ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Page columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pagelocation string work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# remove page unavailable\n",
    "data = data.loc[data['page_title'] != 'Page Unavailable']\n",
    "\n",
    "\n",
    "# lowercase everything to guarantee string match\n",
    "data['page_location'] = data['page_location'].str.lower()\n",
    "data['page_title'] = data['page_title'].str.lower()\n",
    "\n",
    "\n",
    "# domain readability\n",
    "data['page_location'] = data['page_location'].str.replace(r'shop.googlemerchandisestore.com/store.html', 'shop.googlemerchandisestore.com/').str.strip()\n",
    "data['page_location'] = data['page_location'].str.replace(r'+', ' ').str.strip()\n",
    "data['page_location'] = data['page_location'].str.replace(r'https://', '').str.strip()\n",
    "data['page_location'] = data['page_location'].str.replace(r'http://', '').str.strip()\n",
    "data['page_location'] = data['page_location'].str.replace(r'www.', '').str.strip()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove low percentile page record count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low percentile of event count: 7.0\n"
     ]
    }
   ],
   "source": [
    "df_pages_agg=data.groupby(['page_location']).agg(\n",
    "    total_events=('event_timestamp', 'count'),\n",
    ").sort_values('total_events',ascending=False).reset_index()\n",
    "\n",
    "\n",
    "# Calculate the percentile of event counts\n",
    "event_count_percentile = df_pages_agg['total_events'].quantile(0.30)\n",
    "\n",
    "print(f\"low percentile of event count: {event_count_percentile}\")\n",
    "\n",
    "# # Filter out page paths below the 10th percentile\n",
    "df_pages_agg_filtered = df_pages_agg[df_pages_agg['total_events'] >= event_count_percentile]\n",
    "\n",
    "\n",
    "\n",
    "data=data.merge(df_pages_agg_filtered[['page_location']],\n",
    "                on=['page_location'],\n",
    "                how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split categories in page paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split 'page_location' into 4 parts (max)\n",
    "split_columns = data['page_location'].str.split('/', n=4, expand=True)\n",
    "\n",
    "# Step 2: Assign the first three parts to new columns (ignore the first empty part if there is a leading '/')\n",
    "data['domain'] = split_columns[0] # url_domain \n",
    "data['page_path_level_1'] = split_columns[1].replace('', pd.NA)\n",
    "data['page_path_level_2'] = split_columns[2].replace('', pd.NA)\n",
    "data['page_path_level_3'] = split_columns[3].replace('', pd.NA)\n",
    "\n",
    "# Count the number of non-null levels in the path\n",
    "data['path_length'] = data[['page_path_level_1', 'page_path_level_2', 'page_path_level_3']].notna().sum(axis=1)\n",
    "\n",
    "\n",
    "\n",
    "df_pages_total=data[['page_title','page_location','page_path_level_1','page_path_level_2','page_path_level_3','path_length']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill length page path with page title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['page_path_level_1'] = data.apply(\n",
    "    lambda row: row['page_title'] if pd.isna(row['page_path_level_2']) else row['page_path_level_1'], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df_pages_total=data[['page_title','page_location','page_path_level_1','page_path_level_2','page_path_level_3']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## page location long tail conversion to \"other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Aggregate events by 'page_location' and 'page_path_level_3'\n",
    "df_pages_agg = data.groupby(['page_location', 'page_path_level_3']).agg(\n",
    "    total_events=('event_timestamp', 'count'),\n",
    ").sort_values('total_events', ascending=False).reset_index()\n",
    "\n",
    "# Step 2: Calculate the desired percentile of total events to consider long tail\n",
    "event_count_percentile = df_pages_agg['total_events'].quantile(0.75)\n",
    "\n",
    "# Step 3: Create a mask for long tail pages\n",
    "long_tail_mask = df_pages_agg['total_events'] < event_count_percentile\n",
    "\n",
    "# Step 4: Create a dictionary to map original page_path_level_3 to \"other\"\n",
    "long_tail_mapping = dict(zip(df_pages_agg['page_path_level_3'], \n",
    "                             df_pages_agg['page_path_level_3'].where(~long_tail_mask, \"other\")))\n",
    "\n",
    "\n",
    "# Step 5: Replace the values directly in the original DataFrame\n",
    "# Only apply the mapping if path_length is 3\n",
    "data['page_path_level_3'] = data.apply(\n",
    "    lambda row: long_tail_mapping.get(row['page_path_level_3'], row['page_path_level_3']) if row['path_length'] == 3 else row['page_path_level_3'],\n",
    "    axis=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fill the other page path levels with previous page path column\n",
    "\n",
    "This will allow for hierarchical encoding without sacrificing columns or rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill 'page_path_level_2' by concatenating 'page_path_1' and 'page_path_2' (if 'page_path_2' is null)\n",
    "data['page_path_level_2'] = data.apply(\n",
    "    lambda row: row['page_path_level_1'] if pd.isna(row['page_path_level_2']) \n",
    "    else f\"{row['page_path_level_1']}/{row['page_path_level_2']}\", axis=1\n",
    ")\n",
    "\n",
    "# Fill 'page_path_level_3' by concatenating 'page_path_level_2' and 'page_path_3' (if 'page_path_3' is null)\n",
    "data['page_path_level_3'] = data.apply(\n",
    "    lambda row: row['page_path_level_2'] if pd.isna(row['page_path_level_3']) \n",
    "    else f\"{row['page_path_level_2']}/{row['page_path_level_3']}\", axis=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final df without relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final=data.drop(['page_title','page_location','session_id','user_pseudo_id','event_timestamp','local_date','event_date'],axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307986, 42)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final.to_csv('data/df_merch_pre_proc.csv',index=False)\n",
    "\n",
    "# df_merch_pre_proc.csv\n",
    "# df_merch_profile.csv\n",
    "# df_merch_data_prep.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Excel File for encoding mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: property 'book' of 'OpenpyxlWriter' object has no setter\n",
      "The file might be corrupt or invalid. Creating a new file.\n",
      "Excel file has been updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "\n",
    "# Function to append distinct combinations of selected columns into sheets in an Excel file\n",
    "def append_columns_to_excel(df, columns_dict, output_file):\n",
    "    \"\"\"\n",
    "    Append distinct combinations of selected columns into separate sheets in an existing Excel file,\n",
    "    with the columns ordered by their names for easier hierarchical encoding and add an empty encoding column.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): The DataFrame containing the columns to save.\n",
    "    columns_dict (dict): Dictionary where keys are sheet names, and values are lists of column names to include.\n",
    "    output_file (str): The path of the Excel file to save the sheets.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Check if the file exists and is a valid Excel file\n",
    "    if os.path.exists(output_file):\n",
    "        try:\n",
    "            # Try to load the existing workbook\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl', mode='a', if_sheet_exists='replace') as writer:\n",
    "                writer.book = load_workbook(output_file)\n",
    "                \n",
    "                # Loop over each sheet name and corresponding list of columns\n",
    "                for sheet_name, columns in columns_dict.items():\n",
    "                    # Check if all the specified columns exist in the DataFrame\n",
    "                    missing_columns = [col for col in columns if col not in df.columns]\n",
    "                    if missing_columns:\n",
    "                        print(f\"Warning: The following columns are not found in the DataFrame for sheet '{sheet_name}': {missing_columns}\")\n",
    "                        continue\n",
    "\n",
    "                    # Get distinct combinations of the selected columns\n",
    "                    distinct_values = df[columns].drop_duplicates().dropna(how='all')\n",
    "\n",
    "                    # Sort distinct values by the specified columns for hierarchical grouping\n",
    "                    distinct_values.sort_values(by=columns, inplace=True)\n",
    "\n",
    "                    # Add an empty encoding column for each column in the DataFrame\n",
    "                    for col in columns:\n",
    "                        distinct_values[f'{col}_enc'] = pd.NA\n",
    "\n",
    "                    # Write distinct values to a new sheet named after the sheet_name\n",
    "                    distinct_values.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"The file might be corrupt or invalid. Creating a new file.\")\n",
    "            # Create a new file if loading fails\n",
    "            with pd.ExcelWriter(output_file, engine='openpyxl', mode='w') as writer:\n",
    "                for sheet_name, columns in columns_dict.items():\n",
    "                    missing_columns = [col for col in columns if col not in df.columns]\n",
    "                    if missing_columns:\n",
    "                        print(f\"Warning: The following columns are not found in the DataFrame for sheet '{sheet_name}': {missing_columns}\")\n",
    "                        continue\n",
    "                    distinct_values = df[columns].drop_duplicates().dropna(how='all')\n",
    "                    distinct_values.sort_values(by=columns, inplace=True)  # Sort distinct values\n",
    "\n",
    "                    # Add an empty encoding column for each column in the DataFrame\n",
    "                    for col in columns:\n",
    "                        distinct_values[f'{col}_enc'] = pd.NA\n",
    "\n",
    "                    distinct_values.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "    else:\n",
    "        # If the file does not exist, create a new one\n",
    "        with pd.ExcelWriter(output_file, engine='openpyxl', mode='w') as writer:\n",
    "            for sheet_name, columns in columns_dict.items():\n",
    "                missing_columns = [col for col in columns if col not in df.columns]\n",
    "                if missing_columns:\n",
    "                    print(f\"Warning: The following columns are not found in the DataFrame for sheet '{sheet_name}': {missing_columns}\")\n",
    "                    continue\n",
    "                distinct_values = df[columns].drop_duplicates().dropna(how='all')\n",
    "                distinct_values.sort_values(by=columns, inplace=True)  # Sort distinct values\n",
    "\n",
    "                # Add an empty encoding column for each column in the DataFrame\n",
    "                for col in columns:\n",
    "                    distinct_values[f'{col}_enc'] = pd.NA\n",
    "\n",
    "                distinct_values.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "                \n",
    "\n",
    "# Define the groups of columns for hierarchical encoding, grouped by sheet name\n",
    "columns_to_save = {\n",
    "    'event_name' : ['event_name'],  \n",
    "    'device_category': ['device_category'],   \n",
    "    'device_mobile_brand_name': ['device_mobile_brand_name'],\n",
    "    'device_mobile_model_name': ['device_mobile_model_name'],\n",
    "    'device_language': ['device_language'],\n",
    "    'device_category': ['device_category'],      \n",
    "    'device_operating_system_version': ['device_operating_system_version'],   \n",
    "    'device_operating_system': ['device_operating_system'],   \n",
    "    'device_web_info_browser': ['device_web_info_browser'],   \n",
    "    'device_web_info_browser_version': ['device_web_info_browser_version'],\n",
    "    'geo_country': ['geo_country'],   \n",
    "    'traffic_source_medium':['traffic_source_medium'],\n",
    "    'traffic_source_source':['traffic_source_source'],\n",
    "    'page_path_level_1':['page_path_level_1'],\n",
    "    'page_path_level_2':['page_path_level_2'],\n",
    "    'page_path_level_3':['page_path_level_3'],\n",
    "    \n",
    "}\n",
    "\n",
    "# Save the distinct values combinations of each column group into corresponding sheets\n",
    "append_columns_to_excel(data_final, columns_to_save, f'data/df_merch_values_pre_encoding.xlsx')\n",
    "\n",
    "print(\"Excel file has been updated successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
