{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Library Import and files\n",
    "\n",
    "- Files available here: https://amazon-reviews-2023.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nvidia rapids verification to run using rapids ubuntu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GPU detected.\n",
      "Loading GPU-accelerated `cudf.pandas`...\n",
      "GPU acceleration enabled!\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import pandas as pd\n",
    "\n",
    "def check_nvidia_gpu():\n",
    "    try:\n",
    "        # Check if NVIDIA GPU is available using nvidia-smi\n",
    "        result = subprocess.run(['nvidia-smi'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(\"NVIDIA GPU detected.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(\"No NVIDIA GPU detected. Using CPU-based `pandas`.\")\n",
    "            return False\n",
    "    except FileNotFoundError:\n",
    "        print(\"`nvidia-smi` command not found. Using CPU-based `pandas`.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while checking for the GPU: {str(e)}. Using CPU-based `pandas`.\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Check if we have a GPU\n",
    "    gpu_available = check_nvidia_gpu()\n",
    "\n",
    "    if gpu_available:\n",
    "        try:\n",
    "            # Load the cuDF extension to accelerate pandas with the GPU\n",
    "            print(\"Loading GPU-accelerated `cudf.pandas`...\")\n",
    "            # Ensure this works inside Jupyter using the `%load_ext` magic command\n",
    "            get_ipython().run_line_magic('load_ext', 'cudf.pandas')\n",
    "            print(\"GPU acceleration enabled!\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load `cudf.pandas`. Error: {e}\")\n",
    "            print(\"Falling back to CPU-based `pandas`.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# meta_ds = load_dataset(\"McAuley-Lab/Amazon-Reviews-2023\", \"raw_meta_Automotive\", split=\"full\", trust_remote_code=True)\n",
    "\n",
    "# meta_df = meta_ds.to_pandas() \n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path='meta_Automotive.jsonl'\n",
    "\n",
    "wsl_filepath='/mnt/c/Users/nosta/CODING/ist_dash_2024_rec/notebooks/amazon_reviews/meta_Automotive.jsonl'\n",
    "\n",
    "try:\n",
    "\n",
    "    meta_df = pd.read_json(file_path, lines=True)\n",
    "    print('wsl file path loaded')\n",
    "\n",
    "except:\n",
    "\n",
    "    meta_df = pd.read_json(file_path, lines=True)\n",
    "    print('regular folder file path loaded')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "display(meta_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_df['parent_asin'].nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How many categories?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['path_length'] = meta_df['categories'].apply(len)\n",
    "\n",
    "max_path_length = meta_df['path_length'].max()\n",
    "\n",
    "print(f\"Maximum path length: {max_path_length}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split df categories to then join with original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Split the 'categories' column into 9 columns\n",
    "df_split = meta_df['categories'].apply(lambda x: ','.join(x)).str.split(',', expand=True)\n",
    "\n",
    "# Step 2: Rename columns (e.g., category_1, category_2, ..., category_9)\n",
    "df_split.columns = [f'category_{i+1}' for i in range(df_split.shape[1])]\n",
    "\n",
    "\n",
    "# Encode missing values as a new category, with page path number\n",
    "for col in df_split.columns:\n",
    "    df_split[col] = df_split[col].fillna(meta_df['path_length'])\n",
    "\n",
    "\n",
    "# Step 3: Concatenate the new split columns back to the original DataFrame (meta_df)\n",
    "meta_df = pd.concat([meta_df, df_split], axis=1)\n",
    "\n",
    "\n",
    "meta_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(meta_df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image and video count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_df['num_images'] = meta_df['images'].apply(len)\n",
    "\n",
    "meta_df['num_videos'] = meta_df['videos'].apply(len)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature string parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## feature df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate random row sample to get values from dict\n",
    "print(meta_df['details'].sample(n=1).iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_feature = meta_df[['title','details','parent_asin']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_feature.sample(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_keys = set()\n",
    "# for details_dict in df_feature['details']:\n",
    "#     if isinstance(details_dict, dict):  # Ensure it's a dictionary\n",
    "#         unique_keys.update(details_dict.keys())\n",
    "\n",
    "# # Display the unique keys\n",
    "# print(unique_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_extract = [\n",
    "    'Brand',\n",
    "    'Color',\n",
    "    'Item Weight',\n",
    "    'Package Dimensions',\n",
    "    'Is Discontinued By Manufacturer',\n",
    "    'Manufacturer Part Number',\n",
    "    'OEM Part Number',\n",
    "    'Date First Available'\n",
    "    'Number of Pieces',\n",
    "    'Exterior Finish',\n",
    "    'Handle Type',\n",
    "    'Special Feature',\n",
    "    'Included Components',\n",
    "]\n",
    "\n",
    "\n",
    "# Loop over the list of keys and create new columns in the DataFrame\n",
    "for key in keys_to_extract:\n",
    "    column_name = key.replace(' ', '_').replace('By', '_By')  # Replace spaces and handle \"By\" for column names\n",
    "    df_feature[column_name] = df_feature['details'].apply(lambda x: x.get(key, None))\n",
    "\n",
    "\n",
    "# df_feature['Brand'] = df_feature['details'].apply(lambda x: x.get('Brand', None))\n",
    "# df_feature['Color'] = df_feature['details'].apply(lambda x: x.get('Color', None))\n",
    "# df_feature['Item_Weight'] = df_feature['details'].apply(lambda x: x.get('Item Weight', None))\n",
    "# df_feature['Package_Dimensions'] = df_feature['details'].apply(lambda x: x.get('Package Dimensions', None))\n",
    "# df_feature['Is_Discontinued'] = df_feature['details'].apply(lambda x: x.get('Is Discontinued By Manufacturer', None))\n",
    "# df_feature['Manufacturer_Part_Number'] = df_feature['details'].apply(lambda x: x.get('Manufacturer Part Number', None))\n",
    "# df_feature['OEM_Part_Number'] = df_feature['details'].apply(lambda x: x.get('OEM Part Number', None))\n",
    "# df_feature['Date_First_Available'] = df_feature['details'].apply(lambda x: x.get('Date First Available', None))\n",
    "\n",
    "# Number of Pieces\n",
    "# Exterior Finish\n",
    "# Handle Type\n",
    "# Special Feature\n",
    "# Included Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_feature.sample(n=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_feature.isnull().sum())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
