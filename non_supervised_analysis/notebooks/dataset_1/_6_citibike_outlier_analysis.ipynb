{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DSLabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dslabs_functions lodaded\n"
     ]
    }
   ],
   "source": [
    "%run \"scripts/dslabs_functions.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_functions lodaded\n"
     ]
    }
   ],
   "source": [
    "%run \"scripts/data_functions.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "#get file from data pre processing notebook. This file has all the column we need to start the data profiling phase\n",
    "filepath = r'data/citi_bike_pre_proc.csv'\n",
    "\n",
    "excel_mapping_filepath=r'data/citi_bike_values_encoded.xlsx'\n",
    "\n",
    "file_tag = 'Citi Bike'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1277439 entries, 456401 to 2116284\n",
      "Data columns (total 17 columns):\n",
      " #   Column               Non-Null Count    Dtype  \n",
      "---  ------               --------------    -----  \n",
      " 0   bike_type            1277439 non-null  object \n",
      " 1   user_type            1277439 non-null  object \n",
      " 2   start_borough        1277439 non-null  object \n",
      " 3   end_borough          1277439 non-null  object \n",
      " 4   day_of_month         1277439 non-null  int64  \n",
      " 5   hour                 1277439 non-null  int64  \n",
      " 6   day_of_week          1277439 non-null  object \n",
      " 7   is_weekend           1277439 non-null  int64  \n",
      " 8   time_of_day          1277439 non-null  object \n",
      " 9   ride_duration_min    1277439 non-null  float64\n",
      " 10  temperature_2m       1277439 non-null  float64\n",
      " 11  rain_mm              1277439 non-null  float64\n",
      " 12  cloud_cover_low_pct  1277439 non-null  int64  \n",
      " 13  wind_speed_10m       1277439 non-null  float64\n",
      " 14  wmo_weather_desc     1277439 non-null  object \n",
      " 15  ride_distance_km     1277439 non-null  float64\n",
      " 16  ride_avg_speed       1277439 non-null  float64\n",
      "dtypes: float64(6), int64(4), object(7)\n",
      "memory usage: 175.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(1277439, 17)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = True\n",
    "# test_data = False\n",
    "\n",
    "# Load the data\n",
    "if test_data:\n",
    "    df = pd.read_csv(filepath, na_values=\"\")\n",
    "\n",
    "    df: DataFrame = df.sample(frac=0.4, replace = False)\n",
    "\n",
    "else:\n",
    "    # If not test_data, load the entire dataset\n",
    "    df = pd.read_csv(filepath)\n",
    "\n",
    "id_cols=['ride_id',\n",
    "         'start_time', \n",
    "         'end_time',\n",
    "         'start_station',\n",
    "         'end_station',\n",
    "         'start_station_id',\n",
    "         'end_station_id',\n",
    "         'start_lat',\n",
    "         'start_lng', \n",
    "         #'hour',\n",
    "         'end_lat', \n",
    "         'end_lng',\n",
    "         'wmo_weather_code',\n",
    "         'snowfall_cm',\n",
    "         'ride_duration_sec']\n",
    "#remove also col = 'snowfall_cm' because it has only 0 values in our dataset\n",
    "#remove also start_station and end_Station = they have too many unique values(around 2k) which will make the encoding process very slow\n",
    "\n",
    "# Remove unnecessary columns\n",
    "df = df.drop(columns=id_cols, axis=1)\n",
    "\n",
    "df_enc = df.copy()  # DataFrame com as variáveis codificadas\n",
    "\n",
    "display(df_enc.info())\n",
    "display(df_enc.shape)\n",
    "\n",
    "\n",
    "#alterar consoante a necessidade:\n",
    "#- se quiser considerar a remoçao de outliers, alterar para \"df_prep_outl\"\n",
    "#- se nao quiser considerar a remoçao de outliers, alterar para \"df_enc\"\n",
    "use_df = \"df_enc\"  # Pode ser \"df_enc\" ou \"df_prep_outl\"\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
