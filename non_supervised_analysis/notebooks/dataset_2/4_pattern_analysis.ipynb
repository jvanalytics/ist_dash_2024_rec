{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- apriori method - is used to first identify the most frequent itemsets in the data.\n",
    "    - How does it work?\n",
    "        - It takes a dataset of transactions (or a binary dataframe where each column represents an item and each row represents a transaction).\n",
    "        - It finds frequent itemsets, meaning items that appear together in at least a minimum number of transactions (defined by min_support).\n",
    "\n",
    "- association_rules - After obtaining frequent itemsets using apriori, you can extract association rules using association_rules.\n",
    "\n",
    "    - How does it work?\n",
    "        - It uses the frequent itemsets to generate rules like:\n",
    "            {A} → {B} (if a customer buys A, there is a high probability they will buy B).\n",
    "        - It evaluates rule strength using metrics such as:\n",
    "        -   support → how often the rule appears in the dataset.\n",
    "        - confidence → how often B is bought when A is bought.\n",
    "        - lift → whether A and B occur together more often than expected by chance.\n",
    "\n",
    "***Summary***\n",
    "- Use Apriori (apriori) to find groups of items frequently bought together.\n",
    "- Use Association Rules (association_rules) to generate rules like “if customers buy A, they are likely to buy B.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "filepath=r'/Users/cozmaeug/Private/IST PG - DS/DaSH ENG/ist_dash_2024_rec/non_supervised_analysis/notebooks/dataset_2/df_bakery_encoded.csv'\n",
    "\n",
    "file_tag = \"Bakery Clustering\"\n",
    "\n",
    "data = pd.read_csv(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dslabs_functions lodaded\n"
     ]
    }
   ],
   "source": [
    "%run \"scripts/dslabs_functions.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_functions lodaded\n"
     ]
    }
   ],
   "source": [
    "%run \"scripts/data_functions.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling MV with median\n",
    "For K-means clustering, it's particularly important to avoid outliers or large deviations caused by extreme values, so median might be a safer choice than the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['total'] = data['total'].fillna(data['total'].median())\n",
    "data['Purchase value'] = data['Purchase value'].fillna(data['Purchase value'].median())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop MV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.copy()\n",
    "data = data.dropna(axis=0, how=\"any\") #axis=0 tells dropna to remove rows that have at least one NaN value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discretization if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern mining application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterizable pattern discovery\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import binom\n",
    "N = len(data)\n",
    "probs = {col : data[[col]].eq(1).sum()[col]/N for col in data.columns}\n",
    "\n",
    "def add_significance(patterns):\n",
    "    patterns['significance'] = 0.0\n",
    "    for i, pattern in patterns.iterrows():\n",
    "        prob = 1\n",
    "        for item in pattern['itemsets']: prob = prob * probs[item]\n",
    "        patterns.at[i,'significance'] = 1-binom.cdf(pattern['support']*N-1, N, prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_patterns(mine_rules=True, min_patterns=10, min_length=4, max_pvalue=0.1, \n",
    "                  min_support=0.6, min_confidence=0.8, min_lift=1.4):\n",
    "    patterns = {}\n",
    "    min_support = 1\n",
    "    while min_support>0:\n",
    "    \n",
    "        min_support = min_support*0.9\n",
    "        print(\"Finding patterns with min sup %f\"%min_support)\n",
    "        patterns = apriori(data, min_support=min_support, use_colnames=True)\n",
    "    \n",
    "        if mine_rules and len(patterns)>0:\n",
    "            patterns = association_rules(patterns, metric=\"lift\", min_threshold=min_lift)\n",
    "            patterns = patterns[['antecedents','consequents','support','confidence','lift']]\n",
    "            patterns = patterns[(patterns['confidence'] >= min_confidence)]\n",
    "            patterns['itemsets'] = [x | y for x, y in zip(patterns['antecedents'], patterns['consequents'])]\n",
    "        \n",
    "        patterns['length'] = patterns['itemsets'].apply(lambda x: len(x))\n",
    "        patterns = patterns[(patterns['length'] >= min_length)]\n",
    "        add_significance(patterns)\n",
    "        patterns = patterns[(patterns['significance'] <= max_pvalue)]\n",
    "            \n",
    "        if len(patterns) >= min_patterns: break\n",
    "    \n",
    "    print(\"Number of found patterns:\",len(patterns))\n",
    "    return patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find patterns inesperadamente frequentes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- using max of p_value=0.05 (5%)\n",
    "    - This way we make sure we only consider patterns that have relevent statistic significance (most likely they are not random) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_freq_patterns = find_patterns(\n",
    "    mine_rules = False,\n",
    "    min_patterns = 60, \n",
    "    min_length = 4, \n",
    "    max_pvalue = 0.05, \n",
    "    min_support=0.30\n",
    ")\n",
    "# Exportando para CSV novamente\n",
    "df_freq_patterns.to_csv('dataset_2/bakery_freq_patterns', sep=';',index=False)\n",
    "\n",
    "df_freq_patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find patterns inesperadamente discriminativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supondo que você tenha um DataFrame de padrões\n",
    "df_patterns = find_patterns(min_support=0.3, max_pvalue=0.05, min_patterns=15, \n",
    "                            min_confidence=0.9, min_lift=1.5)\n",
    "\n",
    "# Convertendo o frozenset em uma string para melhor exportação\n",
    "df_patterns['antecedents'] = df_patterns['antecedents'].apply(lambda x: ', '.join(map(str, list(x))) if isinstance(x, frozenset) else str(x))\n",
    "df_patterns['consequents'] = df_patterns['consequents'].apply(lambda x: ', '.join(map(str, list(x))) if isinstance(x, frozenset) else str(x))\n",
    "df_patterns['itemsets'] = df_patterns['itemsets'].apply(lambda x: ', '.join(map(str, list(x))) if isinstance(x, frozenset) else str(x))\n",
    "\n",
    "# Exportando para CSV novamente\n",
    "df_patterns.to_csv('data/citi_bike_discr_patterns.csv', sep=';',index=False)\n",
    "\n",
    "df_patterns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameterizable pattern discovery func vers 2 "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
